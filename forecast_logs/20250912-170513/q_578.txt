# Will humans go extinct before 2100? (QID: 578)

- Type: binary

- URL: https://www.metaculus.com/questions/578/

- Classifier: xrisk | strategic=False (score=0.00)

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
- **Historical Extinction Events**: Major extinction events in Earth's history (e.g., dinosaurs) have a base rate of 1 in 100 million years. Limitations: Not directly applicable to human extinction.
- **Species Survival**: Average mammalian species survival is around 1 million years. Limitations: Humans have unique technological and societal factors.
- **Expert Polls**: The 2008 Global Catastrophic Risk Conference estimated a 19% chance by 2100. Limitations: Potential selection bias.

### Recent developments (timeline bullets)
- [2023-06-15] AI alignment concerns ↑ — Increased AI capabilities raise existential risk concerns.
- [2023-03-10] Climate change mitigation efforts ↓ — Global agreements on emissions reduction show progress.
- [2022-11-05] Nuclear disarmament talks ↓ — Renewed international efforts to reduce nuclear arsenals.

### Mechanisms & drivers (causal levers)
- **Technological Risks**: AI and biotechnology advances (large) — Potential for catastrophic misuse or accidents.
- **Environmental Change**: Climate change and biodiversity loss (moderate) — Long-term impacts on human habitability.
- **Geopolitical Stability**: Nuclear war and global conflicts (large) — Direct existential threats.
- **Pandemics**: Emerging infectious diseases (moderate) — Potential for global health crises.
- **Space Colonization**: Human expansion beyond Earth (small) — Reduces extinction risk by diversification.

### Differences vs. the base rate (what’s unusual now)
- **Technological Acceleration**: Rapid advancements in AI and biotech increase potential risks.
- **Global Connectivity**: Increased interdependence can both mitigate and amplify risks.
- **Policy and Governance**: International cooperation on existential risks is more feasible now.
- **Public Awareness**: Growing awareness and discourse on existential risks.

### Bayesian update sketch (for the statistician)
- **Prior**: A plausible prior might be 1% based on historical extinction rates, with an "equivalent n" of 10.
- **Evidence mapping**:
  - AI risks ↑ (moderate)
  - Climate mitigation ↓ (small)
  - Nuclear disarmament ↓ (small)
  - Geopolitical tensions ↑ (moderate)
- **Net effect**: The posterior should move slightly up due to increased AI and geopolitical risks.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without alignment solutions.
  - Increased geopolitical tensions or conflicts.
  - Significant biodiversity loss or climate events.
- **DOWN indicators**:
  - Successful international AI safety agreements.
  - Progress in nuclear disarmament.
  - Advances in climate change mitigation technologies.

### Caveats & pitfalls
- **Uncertainty in AI development**: Unpredictable trajectory and impact.
- **Data gaps**: Limited historical data on human extinction scenarios.
- **Deception risks**: Potential misinformation on existential threats.
- **Regime changes**: Political shifts can alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human" and "extinction."

Final Research Summary: While historical extinction rates suggest low probabilities, recent technological and geopolitical developments warrant a slight upward revision in extinction risk estimates by 2100.

### Sources (autofetched)
- Deep future: Why we'll still be here (newscientist.com) — https://www.newscientist.com/article/mg21328541-800-deep-future-why-well-still-be-here/

### Provider Candidates (pre-filter)
- Deep future: Why we'll still be here (newscientist.com) — https://www.newscientist.com/article/mg21328541-800-deep-future-why-well-still-be-here/
- In Order to Ensure Our Survival, We Must Become a Multi-Planetary Species (futurism.com) — https://futurism.com/in-order-to-ensure-human-survival-we-must-become-a-multi-planetary-species

## Per-model

- **OpenRouter-Default** ok=True time=16850ms cost=$0.0000 parsed=0.02

  - Raw output (excerpt):

    To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach:
    
    1. **Base Rate (Prior) Selection**
       - **Base Rate**: A reasonable prior for human extinction by 2100 could be around 1%, based on historical extinction rates of species and the unique factors affecting humans. This is a conservative estimate, acknowledging the rarity of extinction events over short timescales.
       - **Rationale**: Historical extinction events are extremely rare on a century scale, and humans have unique technological and societal capabilities that may mitigate some risks.
    
    2. **Comparison to Base Case**
       - **Similarities**: The base case assumes low extinction probability due to historical rarity.
       - **Differences**: Current technological and geopolitical factors, such as AI risks and nuclear tensions, are more pronounced than in historical contexts, potentially increasing risk.
    
    3. **Evidence Evaluation (Likelihoods)**
       - **AI Risks**: Increased AI capabilities without alignment solutions raise existential risks. Likelihood ratio > 1, increasing probability.
       - **Climate Mitigation**: Progress in climate agreements slightly reduces risk. 

- **Claude-3.7-Sonnet (OR)** ok=True time=25346ms cost=$0.0000 parsed=0.03

  - Raw output (excerpt):

    # Will humans go extinct before 2100?
    
    ## 1. Base Rate (Prior) Selection
    
    For human extinction by 2100, I need to select an appropriate base rate. This is challenging because human extinction has never occurred, so I'll consider several reference classes:
    
    - **Anthropic reasoning (Doomsday argument)**: Suggests roughly 5-20% chance of extinction this century
    - **Longevity of mammalian species**: Average mammal species survives ~1-2 million years, suggesting <0.01% chance per century
    - **Expert surveys**: The Oxford Global Catastrophic Risk Conference yielded 19%, but with acknowledged selection bias
    - **Toby Ord's estimate**: In "The Precipice," Ord estimates ~1/6 (16.7%) chance of existential catastrophe this century
    - **Metaculus community**: Previous predictions on similar questions typically range from 1-10%
    
    Given these considerations, I'll select a base rate of **5%** as my prior probability. This acknowledges both the historical resilience of humanity while recognizing the unique technological risks of our era. This is higher than the pure biological extinction rate but lower than potentially biased expert surveys.
    
    ## 2. Comparison to Base Case
    
    **Similarities to base case:

- **Gemini** ok=False time=13658ms cost=$0.0000 | error=Gemini HTTP 503: The model is overloaded. Please try again later. parsed=0.0

- **Grok** ok=True time=41599ms cost=$0.0000 parsed=0.03

  - Raw output (excerpt):

    ### 1. Base Rate (Prior) Selection
    An appropriate base rate for human extinction before 2100 should draw from historical extinction events and species survival data, adjusted for humanity's unique position. Historical major extinction events (e.g., mass extinctions like the Permian-Triassic) occur roughly once every 100 million years, implying a ~0.000075% chance per year (or ~0.0056% over 75 years from 2025 to 2100). Average mammalian species lifespan is about 1 million years, suggesting a ~0.0075% annual extinction risk (or ~0.56% over 75 years). These are very low, but humans differ due to technology, intelligence, and global interconnectedness, which could both amplify risks (e.g., self-inflicted catastrophes) and mitigate them (e.g., space colonization). Expert polls, like the 2008 Oxford conference's 19% median, are higher but likely biased toward pessimists. A more balanced prior, blending these, is around 1% (as suggested in the research report's Bayesian sketch), reflecting low historical rates but acknowledging human-specific risks. In odds form: 1:99 (prior probability P(H) = 1%).
    
    ### 2. Comparison to Base Case
    The base case (e.g., historical mammalian extinctions) assu

## Ensembles (with research)

- main=0.0567 | no_gtmc1=0.0567 | uniform=0.0567 | simple=0.0267

## Ablation (no research)

- main=0.0922 | uniform=0.0922 | simple=0.0650