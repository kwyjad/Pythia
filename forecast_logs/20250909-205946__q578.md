üìä Forecast Run 20250909-205946
Timestamp: 2025-09-09 21:00:37 +0300
============================================================

------------------------------------------------------------

‚ùì **Question:** Will humans go extinct before 2100?
üîó **URL:** https://www.metaculus.com/questions/578/

üìù **Summary**

- Final Prediction: **5.53% YES**

------------------------------------------------------------

## RESEARCH
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ‚Üë ‚Äî AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] ‚Üë ‚Äî Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] ‚Üë ‚Äî Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large ‚Äî Potential for AI to surpass human control.
- **Nuclear Conflict**: Large ‚Äî Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate ‚Äî Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate ‚Äî New diseases could emerge with high mortality rates.
- **Technological Advances**: Small ‚Äî Could mitigate or exacerbate risks.

### Differences vs. the base rate (what‚Äôs unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - ‚Üë AI risk (large)
  - ‚Üë Climate change impacts (moderate)
  - ‚Üë Nuclear tensions (large)
  - ‚Üì Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 20:59:46 +0300 (Similarity threshold ‚â•0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? ‚Äî binary ‚Äî 1.0% YES ‚Äî [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] ‚Äî binary ‚Äî 4.5% YES ‚Äî [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

## GTMC1 DETAILS
_GTMC1 not used for this question._

------------------------------------------------------------

## BAYES-MC DETAILS
```json
{
  "method": "Beta weak-prior pool",
  "alpha": 0.245,
  "beta": 3.955,
  "p10": 1.3869119031594926e-05,
  "p50": 0.010543667804114646,
  "p90": 0.17874029955390297,
  "n_members": 4
}
```

------------------------------------------------------------

## MODEL REASONING (RAW)
### OpenRouter-Default [openai/gpt-4o]
```
To forecast the probability of human extinction before 2100, we will follow a structured Bayesian approach:

1. **Base Rate (Prior) Selection**
   - The historical base rate for human extinction is effectively zero, as there have been no known extinction events in human history. However, expert estimates, albeit with selection bias, suggest a 19% chance by 2100. Given the extreme nature of the event and the lack of historical precedent, a more conservative prior of 1% is reasonable, reflecting the rarity of such an event.

2. **Comparison to Base Case**
   - Similarities: The current situation shares the baseline of no historical human extinction events.
   - Differences: Recent developments in AI, climate change, and geopolitical tensions introduce new risks not present in historical contexts. These differences suggest a higher risk than the historical base rate alone would indicate.

3. **Evidence Evaluation (Likelihoods)**
   - **AI Risk**: AI experts warn of a 20% chance of AI-induced extinction within 30 years. This significantly increases the likelihood of extinction compared to the base rate.
   - **Climate Change**: A 70% chance of Gulf Stream collapse by 2035 could lead to severe climate shifts, moderately increasing the risk.
   - **Nuclear Conflict**: Increased geopolitical tensions and potential for nuclear war also raise the likelihood of extinction.
   - **Historical Extinction Events**: The absence of past extinction events slightly decreases the likelihood.
   - Overall, the evidence suggests a moderate increase in the probability of extinction.

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds: 1% probability = 0.01 odds.
   - Likelihood ratio: Considering the significant evidence from AI, climate change, and nuclear risks, let's assume a combined likelihood ratio of 3 (a moderate increase).
   - Posterior odds = Prior odds √ó Likelihood ratio = 0.01 √ó 3 = 0.03.
   - Posterior probability = 0.03 / (1 + 0.03) ‚âà 2.91%.

5. **Red Team Thinking**
   - Overconfidence risk: The forecast might underestimate the potential for mitigating actions or overestimate the impact of speculative risks.
   - Alternative scenarios: Successful global cooperation on AI governance, climate mitigation, and nuclear disarmament could significantly reduce risks.
   - Adjustments: Given the uncertainties and potential for mitigation, a slight downward adjustment is warranted.

6. **Final Forecast**
   - Considering the base rate, evidence, and red team thinking, the final forecast is a calibrated probability of human extinction before 2100.

Final: 2.5%
```

### Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. This is challenging since human extinction has never occurred, so I'll consider several reference points:

- Historical data: Humans have existed for roughly 300,000 years with no extinction events (0% historical rate)
- Metaculus community forecast: 1.0%
- Manifold market: 4.5%
- Oxford Global Catastrophic Risk Conference: 19% (with noted selection bias)
- Species extinction rates: The background extinction rate for mammals is roughly 1 extinction per million species-years

Given these reference points, I'll select a conservative prior of 2%. This is higher than the historical rate (which would be effectively 0%) but lower than the potentially biased Oxford conference estimate. It aligns more closely with prediction markets while acknowledging the unprecedented nature of the question.

Prior probability: 2% (odds of 1:49)

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans have demonstrated remarkable resilience and adaptability throughout history
- We have survived previous existential threats (Cold War nuclear tensions, pandemics)
- Global cooperation mechanisms exist for addressing shared threats

**Differences from base case:**
- Unprecedented technological capabilities with AI development accelerating rapidly
- Climate change occurring at rates not seen in human history
- Nuclear arsenals remain substantial despite reductions
- Global connectivity increases pandemic spread potential
- Multiple simultaneous existential risks rather than isolated threats

These differences suggest the need for some adjustment to the base rate, as we face novel combinations of threats.

## 3. Evidence Evaluation (Likelihoods)

**Evidence 1: AI risk warnings**
- P(E|H): If humans go extinct, there's a high probability (~80%) we would see expert warnings about AI risks
- P(E|~H): If humans don't go extinct, there's still a moderate probability (~30%) we would see such warnings due to caution
- Likelihood ratio: 80%/30% ‚âà 2.67
- This evidence increases the probability of extinction

**Evidence 2: Climate change acceleration**
- P(E|H): If humans go extinct, there's a moderate probability (~60%) we would see severe climate warnings
- P(E|~H): If humans don't go extinct, there's also a moderate probability (~40%) we would see such warnings
- Likelihood ratio: 60%/40% = 1.5
- This evidence slightly increases the probability of extinction

**Evidence 3: Nuclear conflict risks**
- P(E|H): If humans go extinct, there's a high probability (~70%) we would see nuclear tensions
- P(E|~H): If humans don't go extinct, there's a moderate probability (~35%) we would see such tensions
- Likelihood ratio: 70%/35% = 2
- This evidence increases the probability of extinction

**Evidence 4: Historical resilience**
- P(E|H): If humans go extinct, there's a low probability (~20%) we would have observed historical resilience
- P(E|~H): If humans don't go extinct, there's a high probability (~80%) we would observe such resilience
- Likelihood ratio: 20%/80% = 0.25
- This evidence decreases the probability of extinction

## 4. Bayesian Updating (Posterior Probability)

Starting with prior odds of 1:49 (2%), I'll update based on the evidence:

**Update on AI risk warnings:**
- Prior odds: 1:49
- Likelihood ratio: 2.67
- Posterior odds: 1:49 √ó 2.67 = 2.67:49 ‚âà 1:18.35
- Posterior probability: 1/(1+18.35) ‚âà 5.2%

**Update on climate change acceleration:**
- Prior odds: 1:18.35
- Likelihood ratio: 1.5
- Posterior odds: 1:18.35 √ó 1.5 = 1.5:18.35 ‚âà 1:12.23
- Posterior probability: 1/(1+12.23) ‚âà 7.6%

**Update on nuclear conflict risks:**
- Prior odds: 1:12.23
- Likelihood ratio: 2
- Posterior odds: 1:12.23 √ó 2 = 2:12.23 ‚âà 1:6.12
- Posterior probability: 1/(1+6.12) ‚âà 14.1%

**Update on historical resilience:**
- Prior odds: 1:6.12
- Likelihood ratio: 0.25
- Posterior odds: 1:6.12 √ó 0.25 = 0.25:6.12 ‚âà 1:24.48
- Posterior probability: 1/(1+24.48) ‚âà 3.9%

After considering all evidence, the posterior probability is approximately 3.9%.

## 5. Red Team Thinking

**Potential overconfidence:**
- I may be underestimating unprecedented technological risks, particularly from AI
- The combination of multiple simultaneous risks could create unforeseen compounding effects
- Historical resilience may not apply to novel threats

**Alternative scenarios:**
- A more pessimistic view might place greater weight on expert warnings about AI risks
- A more optimistic view would emphasize human adaptability and technological solutions

**Blind spots:**
- Unknown unknowns: There may be existential threats we haven't yet identified
- Technological solutions: I may be underestimating humanity's capacity to develop protective technologies
- Definitional issues: The resolution criteria specify biological humans, but humanity might transition to non-biological forms

Given these considerations, I should adjust my estimate slightly upward to account for unknown risks and the unprecedented nature of current technological development.

## 6. Final Forecast

After considering the base rate, evidence, and potential blind spots, I believe the probability of human extinction before 2100 is approximately 5%. This reflects both the historical resilience of humanity and the unprecedented nature of current existential risks.

Final: 5%
```

### Gemini [gemini-2.5-pro]
```
### 1. Base Rate (Prior) Selection

The event in question is the extinction of the human species within the next ~75 years. A purely historical base rate is not useful, as there are zero instances of human extinction in our history. This would lead to a prior of 0%, which is unhelpful for Bayesian analysis as it cannot be updated by any evidence.

Instead, I will select a prior based on the aggregated judgment of a group of forecasters who are incentivized for accuracy. The Metaculus community prediction of 1.0% is a strong candidate for a prior. This community has a track record of careful, calibrated forecasting and the 1% figure implicitly accounts for the various known risks (nuclear, biological, etc.) as of the time of the forecast. It is a more conservative and likely less biased starting point than the 19% figure from the self-selected group at the Global Catastrophic Risk Conference.

Therefore, I will set my prior probability P(H) at 1%, which corresponds to prior odds of 1:99.

### 2. Comparison to Base Case

The base case, represented by the 1% prior, assumes a general understanding of the existential risk landscape. The current situation differs from this baseline in several key ways highlighted by the research report:

*   **Similarities:** The major categories of risk (nuclear, climate, pandemics) are the same ones that have been considered for years and are likely factored into the 1% prior. Geopolitical tensions have been a constant feature of the post-WWII era.
*   **Differences:**
    *   **Pace of AI Development:** The capabilities of AI systems are advancing at a rate that appears to be faster than many baseline models predicted. This introduces a novel and rapidly evolving existential risk that may not be fully priced into a slow-moving aggregate forecast.
    *   **Climate Tipping Points:** The specific warning about a potential Gulf Stream (AMOC) collapse by 2035 with 70% probability is a concrete, near-term piece of evidence suggesting climate models might be underestimating the speed and severity of state shifts in the climate system.
    *   **Geopolitical Instability:** While tensions are not new, the current multipolar world with several nuclear powers in complex relationships may present a higher risk of miscalculation and escalation than the bipolar Cold War era.

These differences suggest that the risk level today is likely higher than the long-term average reflected in the 1% prior.

### 3. Evidence Evaluation (Likelihoods)

I will evaluate the new pieces of evidence from the research report against the prior.

*   **Evidence 1: AI experts warn of a 20% chance of AI-induced extinction within 30 years.**
    *   P(E | H): If humanity is going to go extinct by 2100 (H), it is highly likely that in the decades prior, credible experts in a leading risk area like AI would be issuing dire warnings with high probabilities. This evidence is very consistent with H.
    *   P(E | ~H): If humanity is *not* going to go extinct (~H), it is still possible that experts would be alarmist or that the risks would be successfully mitigated. However, such a specific and high-probability warning from a group of experts is less likely in a world where the true risk is negligible.
    *   **Likelihood Ratio:** P(E|H) / P(E|~H) is significantly greater than 1. This evidence strongly increases the probability of extinction.

*   **Evidence 2: Study shows a 70% chance of Gulf Stream collapse by 2035.**
    *   P(E | H): If humanity goes extinct, it could well be due to a cascade of events triggered by a major climate catastrophe like an AMOC collapse. This evidence is consistent with H.
    *   P(E | ~H): If humanity does not go extinct, an AMOC collapse is still a plausible event. It would cause immense devastation, famine, and conflict, but it is unlikely to directly cause the extinction of every human on the planet. Humans are adaptable and live in diverse global regions, many of which would be less directly affected. Therefore, this evidence is also quite consistent with ~H.
    *   **Likelihood Ratio:** P(E|H) / P(E|~H) is slightly greater than 1. This evidence increases the probability, but only moderately, as the causal link between AMOC collapse and *total extinction* is weaker than the link to civilizational collapse.

*   **Evidence 3: Manifold market at 4.5%.**
    *   This is another form of aggregated forecast. While play-money markets can be noisy, a 4.5% probability is a significant deviation from the 1% Metaculus prior. It serves as a piece of evidence that a segment of the forecasting community sees the risk as substantially higher.
    *   **Likelihood Ratio:** This evidence has a likelihood ratio greater than 1, pushing the probability up from the 1% baseline.

### 4. Bayesian Updating (Posterior Probability)

Let's start with the prior odds of 1:99.

The strongest piece of evidence is the AI expert warning. Let's estimate a conservative likelihood ratio for it. If such a warning makes the "extinction" world 2.5 times more likely than the "no extinction" world, our likelihood ratio is 2.5.

*   **Posterior Odds = Prior Odds √ó Likelihood Ratio**
*   Posterior Odds = (1/99) √ó 2.5 = 2.5 / 99

Now, let's convert this back to a probability:

*   Posterior Probability = (2.5/99) / (1 + 2.5/99) = (2.5/99) / (101.5/99) = 2.5 / 101.5 ‚âà 2.46%

The other evidence (AMOC collapse risk, Manifold market) also pushes the probability up, though more weakly. Cumulatively, they might add another half a percentage point or so. This brings the estimate into the ~3% range.

### 5. Red Team Thinking

*   **Argument for a Lower Forecast (Overconfidence):** The definition of extinction is "no known humans alive." This is an extremely high bar. Even in the most catastrophic nuclear war or pandemic, small, isolated groups of humans in remote locations (e.g., Antarctica, submarines, deep bunkers, the Southern Hemisphere) are likely to survive. The resilience and geographic distribution of humanity are immense. I may be conflating civilizational collapse (a much higher probability event) with total extinction. The 1% Metaculus prior may already account for the AI risk hype, and I might be double-counting it.

*   **Argument for a Higher Forecast (Blind Spots):** The risks are not independent. Climate change could cause resource scarcity and state failure, leading to geopolitical instability and nuclear conflict. An AI could be used as a weapon in such a conflict, or a crisis could distract from AI safety efforts, leading to an accidental AI catastrophe. These correlated risks could create a cascade where the total risk is much greater than the sum of its parts. My update may be too conservative and fail to capture these interaction effects. The 4.5% on Manifold might be closer to the true mark.

*   **Rebuttal and Adjustment:** The "high bar for extinction" argument is very compelling and serves as a strong cap on the forecast. However, the novel and rapidly escalating nature of AI risk, combined with the potential for risk cascades, justifies a significant update from the 1% prior. A 3% forecast represents a tripling of the prior odds, which is a substantial adjustment, while still acknowledging that total extinction remains an unlikely event. It strikes a balance between the alarming new evidence and the inherent resilience of the human species.

### 6. Final Forecast

My initial prior was 1%. The recent, specific, and high-probability warnings about AI risk and climate tipping points constitute significant new evidence that the risk is higher than this baseline. A Bayesian update focusing on the strongest evidence moves the probability to ~2.5%. Factoring in weaker evidence and the higher Manifold market price pushes it slightly higher. After red-teaming the forecast, I find that while the bar for total extinction is very high, the potential for cascading, novel risks justifies a notable increase. I will settle on 3%.

Final: 3%
```

### Grok [grok-4]
```
### 1. Base Rate (Prior) Selection
For human extinction before 2100, an appropriate base rate is challenging due to the lack of historical precedents‚Äîhumanity has never gone extinct, so the empirical frequency is 0%. However, this is uninformative for future risks, as it ignores emerging threats like AI, nuclear war, and climate change that have no direct analogs in history. Instead, I'll draw from expert elicitations and forecasting platforms as a reference class. The 2008 Oxford poll gave a median of 19%, but as noted, it's biased toward high-risk views. More balanced aggregates like Metaculus (1%) and Manifold (4.5%) suggest a lower consensus. I'll select a prior of 1% (or odds of 1:99), aligning with the research report's suggestion and Metaculus as a starting point for low-probability existential risks, reflecting the rarity of such events while acknowledging expert concerns.

### 2. Comparison to Base Case
The base case is the historical record of human survival through millennia, including past existential threats like pandemics (e.g., Black Death killed ~30-50% of Europe but not globally extinctive) and geopolitical tensions (e.g., Cold War nuclear standoffs without escalation). Similarities include ongoing human resilience, technological progress mitigating risks (e.g., vaccines for pandemics), and no extinction events. Differences are significant: today's situation involves novel, accelerating risks like rapid AI development (potentially uncontrollable superintelligence), intensifying climate instability (e.g., possible Gulf Stream collapse), and heightened nuclear tensions in a multipolar world. These differ from historical cases by their global scale, speed, and interconnectedness (e.g., global connectivity amplifying pandemics), which could make extinction more plausible, warranting an upward adjustment from the near-zero historical base.

### 3. Evidence Evaluation (Likelihoods)
Key evidence from the research report and background:

- **AI risks (‚Üë large)**: Warnings of 20% AI-induced extinction risk within 30 years. P(E | H) is high (~80%) because uncontrolled AI could directly cause extinction (e.g., misaligned goals). P(E | ~H) is moderate (~40%) as such warnings could arise from hype or caution without true risk. Likelihood ratio: ~2 (80%/40%), increasing probability.
  
- **Climate change (‚Üë moderate)**: 70% chance of Gulf Stream collapse by 2035, potentially causing severe shifts. P(E | H) high (~70%) as this could cascade to ecosystem collapse. P(E | ~H) lower (~30%) since models are uncertain and mitigation possible. Likelihood ratio: ~2.3 (70%/30%), increasing probability moderately.

- **Nuclear war risks (‚Üë large)**: Recent articles on geopolitical tensions. P(E | H) high (~60%) as escalation could lead to nuclear winter. P(E | ~H) moderate (~30%) due to deterrence and diplomacy. Likelihood ratio: ~2 (60%/30%), increasing probability.

- **Pandemics and other drivers (‚Üë moderate)**: Increased potential from global connectivity. P(E | H) ~50% (engineered pandemics could wipe out humanity); P(E | ~H) ~20% (outbreaks happen but are contained). Likelihood ratio: ~2.5, increasing probability.

- **Historical lack of extinction (‚Üì small)**: No past events. P(E | H) low (~10%‚Äîextinction would require unprecedented failure); P(E | ~H) high (~90%‚Äîsurvival is the norm). Likelihood ratio: ~0.11 (10%/90%), decreasing probability slightly.

Net: Most evidence has positive likelihood ratios, pushing toward higher probability, but historical survival pulls back modestly.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior odds of 1:99 (P(H) = 1%).

Combine likelihood ratios qualitatively into a net LR of ~4 (product of ~2 for AI, ~2.3 for climate, ~2 for nuclear, ~2.5 for pandemics, tempered by ~0.11 for history: rough net ~4 after multiplication and adjustment for correlation/overcounting).

Posterior odds = 1:99 √ó 4 = 4:396 ‚âà 1:99 (simplified, but actually ~1:99, wait‚Äî1:99 √ó 4 = 4:396 = 1:99, no: 1/99 √ó 4 = 4/396 = 1/99, that's unchanged‚Äîmiscalc.

Proper calc: Prior odds = P(H)/P(~H) = 0.01 / 0.99 ‚âà 0.0101.

Net LR ‚âà (2 √ó 2.3 √ó 2 √ó 2.5) / 0.11 ‚âà (23) / 0.11 ‚âà 209 (but this is overestimate; evidence is correlated, so dampen to ~5-10 for conservatism).

Using damped net LR=6: Posterior odds ‚âà 0.0101 √ó 6 ‚âà 0.0606.

Posterior P(H) = 0.0606 / (1 + 0.0606) ‚âà 5.7%.

This remains uncertain due to wide variance in expert views (1-19%) and model uncertainties, with confidence around 3-8%.

### 5. Red Team Thinking
My forecast might be overconfident in upward adjustments‚Äîexpert polls like the biased 19% could inflate perceived risks, and Metaculus at 1% suggests I'm overupdating on recent "‚Üë" developments, which are speculative (e.g., AI timelines vary wildly, Gulf Stream collapse isn't guaranteed extinctive). Blind spots: Overlooking mitigation (e.g., AI governance progress, climate tech breakthroughs) or tail risks like unknown unknowns (e.g., asteroid impacts, low but non-zero). Alternative forecast: A skeptic might say 0.5%, arguing all risks are overhyped and human adaptability prevails (plausible given history); rebuttal: Ignoring accelerating tech/climate drivers underestimates novelty. Conversely, a pessimist might say 10-20% based on AI alone; rebuttal: Overreliance on unproven models. To account, I widen my implicit interval to 2-10%, nudging final down slightly for overupdate caution per calibration advice (small nudges).

### Final Forecast
Final: 4%
```

