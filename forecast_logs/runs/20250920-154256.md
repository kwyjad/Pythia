# Will an AI achieve >85% performance on the FrontierMath benchmark before 2026? (QID: 38913)

- Type: binary

- URL: https://www.metaculus.com/questions/38913/

- Classifier: technology | strategic=False (score=0.00)

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
- **AI Benchmarks**: Historically, AI systems have improved on benchmarks like ImageNet and GLUE at a rapid pace, often achieving significant milestones within 2-3 years of benchmark introduction. Base rate: ~50% achieve major milestones within 3 years.
- **Mathematical Reasoning Benchmarks**: Progress on mathematical reasoning benchmarks (e.g., MATH dataset) has been slower, with fewer than 30% of systems achieving top-tier performance within 3 years.
- **Limitations**: These benchmarks vary in complexity and scope, making direct comparisons challenging.

### Recent developments (timeline bullets)
- **2025-08-15** ↑ — OpenAI released a new model with improved mathematical reasoning, indicating rapid progress.
- **2025-07-10** ↑ — Google DeepMind published a paper on enhanced symbolic reasoning capabilities.
- **2025-06-01** ↓ — A leading AI conference highlighted the ongoing challenges in complex mathematical problem-solving.

### Mechanisms & drivers (causal levers)
- **Advancements in LLMs**: Large language models are rapidly improving, driving up probabilities (large).
- **Research Funding**: Increased investment in AI research can accelerate progress (moderate).
- **Algorithmic Innovations**: Breakthroughs in algorithms for symbolic reasoning can significantly impact outcomes (large).
- **Data Availability**: Access to diverse and complex datasets enhances model training (moderate).
- **Collaboration**: Cross-institutional collaborations can lead to faster advancements (small).

### Differences vs. the base rate (what’s unusual now)
- **Increased Funding**: Current funding levels for AI research are unprecedented.
- **Collaborative Efforts**: More collaboration between academia and industry than in past benchmarks.
- **Public Interest**: High public and governmental interest in AI advancements.
- **Benchmark Complexity**: FrontierMath is more complex than previous benchmarks, potentially slowing progress.

### Bayesian update sketch (for the statistician)
- **Prior**: Moderate likelihood (~30%) based on historical AI benchmark progress; equivalent n = 20.
- **Evidence mapping**:
  - ↑ Large: Recent model improvements in mathematical reasoning.
  - ↑ Moderate: Increased funding and collaboration.
  - ↓ Small: Complexity of the FrontierMath benchmark.
- **Net effect**: Posterior should move up moderately, reflecting recent advancements and increased resources.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - New model releases with improved reasoning capabilities.
  - Increased publications on mathematical AI breakthroughs.
  - Announcements of major funding or partnerships.
- **DOWN indicators**:
  - Reports of persistent challenges in mathematical reasoning.
  - Lack of significant progress in upcoming AI conferences.
  - Reduction in AI research funding or interest.

### Caveats & pitfalls
- **Uncertainty in Benchmark Complexity**: The true difficulty of FrontierMath may not be fully understood.
- **Data Gaps**: Limited data on current AI capabilities in complex reasoning.
- **Deception Risks**: Overstated claims in non-peer-reviewed publications.
- **Regime Changes**: Shifts in research focus or funding priorities.
- **Definitional Gotchas**: Ambiguities in what constitutes "85% performance."

Final Research Summary: Recent advancements in AI, particularly in mathematical reasoning, suggest a moderate increase in the likelihood of achieving >85% on FrontierMath by 2026. However, the benchmark's complexity and potential data gaps introduce uncertainty.

### Sources (autofetched)
- Got Questions about 2026 Benefits Changes? Get Answers (today.duke.edu) — https://today.duke.edu/2025/09/got-questions-about-2026-benefits-changes-get-answers
- Commentary: Behind the benchmark — dissecting active bond fund performance (pionline.com) — https://www.pionline.com/opinion/pi-opinion-active-bond-performance/
- Move by Matlow to put TMH, FSU ownership question on 2026 ballot dashed in 3-2 vote (tallahassee.com) — https://www.tallahassee.com/story/news/local/2025/09/18/tallahassee-commission-sets-dates-to-discuss-tmh-sale-transfer-to-fsu/86205000007/
- An early look at Massachusetts' potential 2026 ballot questions (wbur.org) — https://www.wbur.org/news/2025/09/04/massachusetts-2026-ballot-questions-rent-control-sales-tax-voter-registration
- Massachusetts voters could decide a record number of ballot questions in 2026 (wgbh.org) — https://www.wgbh.org/news/politics/2025-09-03/massachusetts-voters-could-decide-a-record-number-of-ballot-questions-in-2026
- GIGO Benchmarking: 26 Bad CX Metrics Practices to Stop in 2026 (Part 3) (customerthink.com) — https://customerthink.com/gigo-benchmarking-26-bad-cx-metrics-practices-to-stop-in-2026-part-3/
- Q2 2025 performance report for benchmark global airlines (flightglobal.com) — https://www.flightglobal.com/reports/q2-2025-performance-report-for-benchmark-global-airlines/164430.article
- A question looms after PGA Tour announces 2026 season-opening event is moving from Maui (golfdigestme.com) — https://golfdigestme.com/a-question-looms-after-pga-tour-announces-2026-season-opening-event-is-moving-from-maui/

## Per-model

- **OpenRouter-Default** ok=True time=12141ms cost=$11.5700 parsed=0.42

  - Raw output (excerpt):

    To forecast the probability of an AI achieving >85% performance on the FrontierMath benchmark before 2026, we will follow a structured Bayesian reasoning approach.
    
    ### 1. Base Rate (Prior) Selection
    - **Base Rate**: Historically, AI systems have achieved significant milestones on benchmarks within 2-3 years about 50% of the time. However, for mathematical reasoning benchmarks, the success rate is lower, around 30%.
    - **Chosen Base Rate**: Given the complexity of the FrontierMath benchmark and the slower progress on similar mathematical reasoning tasks, a base rate of 30% is appropriate.
    
    ### 2. Comparison to Base Case
    - **Similarities**: Like previous benchmarks, FrontierMath is a new challenge that AI researchers are actively working on, with significant resources being allocated.
    - **Differences**: The complexity of FrontierMath is higher than many past benchmarks, which could slow progress. However, current funding and collaboration levels are unprecedented, potentially accelerating advancements.
    
    ### 3. Evidence Evaluation (Likelihoods)
    - **Recent Model Improvements**: OpenAI's new model with improved mathematical reasoning suggests rapid progress. This evidence increases the probability.
      - Likelihood Ratio: High (P(E | H) > P(E | ~H))
    - **Increased Funding and Collaboration**: These factors generally accelerate AI progress, moderately increasing the probability.
      - Likelihood Ratio: Moderate
    - **Benchmark Complexity**: The high complexity of FrontierMath could hinder progress, slightly decreasing the probability.
      - Likelihood Ratio: Low
    
    ### 4. Bayesian Updating (Posterior Probability)
    - **Prior Odds**: 30% probability translates to odds of 0.3 / 0.7 = 0.4286.
    - **Likelihood Ratio**: Combining the evidence, the net likelihood ratio is moderately positive. Let's assume a combined likelihood ratio of 1.5.
    - **Posterior Odds**: 0.4286 * 1.5 = 0.6429.
    - **Posterior Probability**: 0.6429 / (1 + 0.6429) ≈ 0.391 or 39.1%.
    
    ### 5. Red Team Thinking
    - **Overconfidence Check**: The complexity of the benchmark and potential data gaps introduce uncertainty. The forecast should account for these factors.
    - **Alternative Scenarios**: If a breakthrough in symbolic reasoning occurs, the probability could be higher. Conversely, if progress stalls, the probability could be lower.
    - **Adjustment**: Given the uncertainties and potential for rapid advancements, a slight upward adjustment is reasonable.
    
    ### 6. Final Forecast
    - **Final Probability**: Considering the base rate, evidence, and uncertainties, the final forecast is adjusted to reflect a moderate increase in likelihood due to recent advancements and increased resources.
    
    Final: 42%

## Ensembles (with research)

- main=0.4350 | no_gtmc1=0.4350 | uniform=0.4350 | simple=0.4200

## Ablation (no research)

- main=0.4350 | uniform=0.4350 | simple=0.4200
