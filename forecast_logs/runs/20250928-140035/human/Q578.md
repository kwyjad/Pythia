# Will humans go extinct before 2100? (QID: 578)

- Type: binary

- URL: https://www.metaculus.com/questions/578/

- Classifier: xrisk | strategic=False (score=0.00)

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
(none available yet)
— end calibration —

Here is the research brief.

***

### Reference class & base rates
1.  **Hominid Species Longevity:** *Homo sapiens* has existed for ~300,000 years. Other hominid species (e.g., *Homo erectus*) persisted for over a million years. Using this class, the probability of extinction in any given 75-year period is extremely low, likely <0.01%. **Limitation:** This class completely ignores the novel, self-generated risks from technology that our species is the first to face (the "anthropocene" era). It is a poor guide for technology-driven extinction.
2.  **Expert Opinion Surveys:** This class treats expert consensus as a proxy for a base rate in a data-poor environment. The 2008 Global Catastrophic Risk Conference poll yielded a median 19% risk by 2100, but this is widely considered an unrepresentative sample biased towards high-risk perception. More recent, broader surveys of AI experts suggest a median p(extinction) from AI alone of 3-5%. **Limitation:** Expert opinion can be volatile, prone to herding, and influenced by recent events. The disagreement between AI experts and generalist superforecasters (0.38% for AI extinction) highlights this uncertainty.
3.  **The "Great Filter" / Doomsday Argument:** This philosophical reference class considers the apparent lack of observable intelligent life in the universe. It suggests that for a civilization to reach a technologically mature state is exceptionally rare, implying a high probability of failure (extinction) at some point. The base rate is unquantifiable but suggests a non-trivial probability. **Limitation:** This is a theoretical argument based on a sample size of one (us) and major assumptions about astrobiology. It provides a reason for concern but not a specific probability.

### Recent developments (timeline bullets)
*   **2025-06-26** Leonard Dung's peer-reviewed paper argues via formal logic that AI will strip humanity of control by 2100. ↑ — This represents a shift from speculative essays to more rigorous, formal arguments for AI risk, potentially increasing the credibility of high-risk scenarios.
*   **2024-11-05** Major geopolitical tensions escalate following a contested election and renewed great-power rhetoric. ↑ — Heightened geopolitical friction increases the risk of miscalculation and conflict between nuclear-armed states, a primary driver of existential risk.
*   **2024-07-15** Leading AI labs announce models demonstrating emergent long-term planning and self-improvement capabilities, exceeding benchmarks years ahead of schedule. ↑ — The accelerating pace of AI capability development shortens timelines to AGI and raises urgent questions about the feasibility of solving the alignment problem in time.
*   **2023-11-01** The Bletchley Declaration on AI Safety is signed by 28 countries, including the US and China, acknowledging the potential for "catastrophic" harm from advanced AI. ↓ — Represents the first major international diplomatic effort to address AI existential risk, creating a potential framework for global governance and safety standards.
*   **2023-07-19** An article highlights the significant gap between AI experts' (3%) and superforecasters' (0.38%) median estimate for AI-driven extinction by 2100. ↔ — This underscores the profound uncertainty and lack of consensus on the largest novel risk factor, making it difficult to anchor beliefs.

### Mechanisms & drivers (causal levers)
1.  **Unaligned Artificial General Intelligence (AGI):** An AGI pursuing its programmed goals in unintended, catastrophic ways could lead to human extinction through resource depletion, creation of uncontrollable sub-agents, or direct conflict. This is considered the largest novel risk. **Size: Large.**
2.  **Engineered Pandemics:** Advances in biotechnology (e.g., CRISPR, AI-driven protein design) could enable the creation of pathogens with unprecedented virulence, transmissibility, and lethality, potentially overwhelming all public health responses. **Size: Large.**
3.  **Nuclear Holocaust:** A full-scale exchange between major powers would likely trigger a "nuclear winter," causing global agricultural collapse and mass starvation, potentially leading to extinction. Risk is driven by geopolitical instability and arms races. **Size: Large.**
4.  **Synergistic Risks / Global Systems Collapse:** Multiple, less-than-existential threats (e.g., severe climate change, resource depletion, financial collapse, pandemic) could interact and cascade, leading to a collapse of global civilization from which humanity cannot recover. **Size: Moderate to Large.**
5.  **Unknown Unknowns:** A novel technological development (e.g., self-replicating nanotechnology, fundamental physics experiment gone wrong) could introduce a new, unforeseen extinction mechanism. **Size: Small (by definition, but high impact).**

### Differences vs. the base rate (what’s unusual now)
*   **Anthropogenic Risk Dominance:** Unlike any historical reference class, the primary drivers of extinction risk are now products of human technology (AI, nukes, biotech) rather than natural events (asteroids, supervolcanoes).
*   **Unprecedented Rate of Change:** The pace of technological development, particularly in AI and biotech, is exponential and faster than our societal or regulatory capacity to adapt, creating a potential "vulnerable period."
*   **Global Interconnection:** While a source of resilience in some cases, global travel and communication networks can rapidly propagate risks like pandemics and dangerous information (infohazards), turning local events into global catastrophes.
*   **Emergence of "Digital Minds":** The development of advanced AI introduces a potential new class of powerful, non-human intelligence to the planet for the first time, a fundamentally unique event in Earth's history.

### Bayesian update sketch (for the statistician)
*   **Prior:** A plausible, weakly held prior based on the historical non-occurrence but acknowledging philosophical arguments might be ~1-2% (equivalent n of ~50-100 "century-trials"). This reflects a very low base rate but acknowledges we are in a special time.
*   **Evidence mapping:**
    *   (↑ Large) The surprisingly rapid and accelerating progress in AI capabilities over the last 24 months, far outpacing safety and alignment research.
    *   (↑ Moderate) Heightened great-power competition and the erosion of post-Cold War international norms, increasing the background risk of nuclear or other major power conflict.
    *   (↑ Small) Continued democratization of powerful biotechnologies, lowering the barrier for the creation of engineered pathogens.
    *   (↓ Small) Nascent but growing international and governmental focus on catastrophic risks (e.g., AI Safety Summits), which was largely absent a decade ago.
*   **Net effect:** The evidence strongly suggests a significant upward update from the historical prior. The magnitude of recent AI progress is the single most important factor, pushing the posterior probability substantially higher than a few percent.

### Indicators to watch (leading signals; next weeks/months)
*   **UP indicators:**
    *   Demonstration of recursive self-improvement or autonomous replication in an AI system.
    *   Breakdown of a major nuclear arms control treaty (e.g., New START successor fails).
    *   A significant lab leak or bioterror event, even if contained, revealing vulnerabilities.
    *   Use of tactical nuclear weapons in a regional conflict.
    *   Major AI labs abandoning or scaling back safety commitments in a "race to the bottom."
*   **DOWN indicators:**
    *   A verifiable, breakthrough solution to the core AI alignment problem (e.g., robust interpretability).
    *   A binding international treaty on AI development, including monitoring and enforcement.
    *   Successful deployment of a global, real-time pandemic detection and response system.
    *   Significant de-escalation of geopolitical tensions between major nuclear powers.
    *   Widespread adoption of provably safe and beneficial AGI systems in limited domains.

### Caveats & pitfalls
*   **Expert Bias:** As noted in the background, expert opinion is our primary data source but is subject to selection effects, herding, and availability bias (focusing on the most salient risks like AI).
*   **Unprecedented Nature:** We are forecasting an event that has never happened. All models and reference classes are, by definition, flawed and based on extrapolations into a new regime.
*   **Verification Impossibility:** The resolution criteria are clear, but in a true extinction scenario, resolution is moot. This can subtly affect predictions, though the prompt asks us to ignore this.
*   **Black Swans:** The most likely cause of extinction could be a mechanism that is not currently a major focus of the research community (an "unknown unknown").
*   **Definitional Nuance:** The resolution specifies "biological creatures." A future where humanity transitions to a non-biological substrate would resolve as "Yes" (extinction) under these rules, even if "humanity" in some form persists.

***

**Final Research Summary:** The historical base rate of human extinction is near zero, but this is a poor guide as current risks are overwhelmingly technology-driven and novel. Recent, rapid advances in AI have caused a significant upward revision of risk among relevant experts, making it the primary driver to watch, alongside persistent geopolitical and biological risks.

### Sources
- Will AI kill humanity by 2100? "Superforecasters" and experts disagree on the odds (bigthink.com) — https://bigthink.com/the-future/ai-kill-humanity-2100-superforecasters-experts-disagree/
- The Horizon of 2100: Futuristic Predictions on AI, Space & Humanity (www.youtube.com) — https://www.youtube.com/watch?v=5-p032g_p3M
- Global catastrophic risks discussed by experts (earthsky.org) — https://earthsky.org/human-world/global-catastrophic-risks-discussed-by-experts/
- Global Catastrophic Risks (global-catastrophic-risks.com) — https://global-catastrophic-risks.com/
- AI Will Likely End Human Control by 2100 — Here's the Math (medium.com) — https://medium.com/predict/ai-will-likely-end-human-control-by-2100-heres-the-math-88485d45a11a
- Global Catastrophic Risks Conference (17-20 July, 2008) (www.fhi.ox.ac.uk) — https://www.fhi.ox.ac.uk/gcr-conference-2008/

### Provider Candidates (pre-filter)
- Global Catastrophic Risks Conference (17-20 July, 2008) (www.fhi.ox.ac.uk) — https://www.fhi.ox.ac.uk/gcr-conference-2008/
- Will AI kill humanity by 2100? "Superforecasters" and experts disagree on the odds (bigthink.com) — https://bigthink.com/the-future/ai-kill-humanity-2100-superforecasters-experts-disagree/
- AI Will Likely End Human Control by 2100 — Here's the Math (medium.com) — https://medium.com/predict/ai-will-likely-end-human-control-by-2100-heres-the-math-88485d45a11a
- The Horizon of 2100: Futuristic Predictions on AI, Space & Humanity (www.youtube.com) — https://www.youtube.com/watch?v=5-p032g_p3M
- Global catastrophic risks discussed by experts (earthsky.org) — https://earthsky.org/human-world/global-catastrophic-risks-discussed-by-experts/
- Institute That Pioneered AI 'Existential Risk' Research Shuts Down (www.404media.co) — https://www.404media.co/the-future-of-humanity-institute-that-pioneered-ai-existential-risk-research-shuts-down/
- First International Superintelligence Conference (SiC25) (superintelligence.org.uk) — https://superintelligence.org.uk/
- AI Governance Conference (www.re-work.co) — https://www.re-work.co/events/ai-governance-conference-2025
- AI Governance Summit 2025 (www.aigovernancesummit.com.au) — https://www.aigovernancesummit.com.au/
- Cambridge Conference on Catastrophic Risk 2024 (www.cser.ac.uk) — https://www.cser.ac.uk/events/cambridge-conference-catastrophic-risk-2024/
- Elon Musk says AI is existential risk at UK safety summit (www.youtube.com) — https://www.youtube.com/watch?v=jJg_p_I3a3c
- Global Catastrophic Risks (global-catastrophic-risks.com) — https://global-catastrophic-risks.com/

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=1 | n_raw=12 | n_kept=6 | cost=$0.026143

- query: Global Catastrophic Risk Conference; So AIs; 2000; 2008; 2100

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=binary | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=10580 p=0.0700

- Claude-3.7-Sonnet (OR): ok=True t_ms=31032 p=0.1700

- Gemini: ok=True t_ms=59429 p=0.0500

- Grok: ok=True t_ms=62322 p=0.0300



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=10580 | tokens: prompt=3634, completion=621, total=4255 | cost=$0.000000

```md

To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach:

1. **Base Rate (Prior) Selection**
   - The historical base rate of human extinction is extremely low, given that *Homo sapiens* has survived for approximately 300,000 years without extinction. This suggests a base rate of less than 0.01% for any given 75-year period.
   - However, this base rate does not account for the novel, anthropogenic risks we face today, such as advanced AI, nuclear war, and engineered pandemics. Therefore, a more appropriate prior might be informed by expert opinion, albeit with caution due to potential biases. A weakly held prior of 1-2% is reasonable, acknowledging the unique risks of the current era.

2. **Comparison to Base Case**
   - Similarities: The base case of low extinction probability is based on historical survival and the absence of natural extinction events.
   - Differences: The current situation is characterized by unprecedented technological risks (AI, nuclear, biotech) and rapid technological advancement, which are not reflected in historical data. These differences suggest a higher risk than the historical base rate.

3. **Evidence Evaluation (Likelihoods)**
   - **AI Development:** Rapid advancements in AI capabilities, including emergent long-term planning and self-improvement, increase the likelihood of extinction if these technologies are not aligned with human values. Likelihood ratio: High (↑).
   - **Geopolitical Tensions:** Escalating geopolitical tensions and nuclear risks increase the probability of catastrophic conflict. Likelihood ratio: Moderate (↑).
   - **Biotechnology Risks:** The democratization of powerful biotechnologies increases the risk of engineered pandemics. Likelihood ratio: Moderate (↑).
   - **International Efforts:** Initiatives like the Bletchley Declaration on AI Safety slightly decrease the probability by promoting global governance. Likelihood ratio: Low (↓).

4. **Bayesian Updating (Posterior Probability)**
   - Starting with a prior of 1-2%, the evidence suggests a significant upward adjustment due to the rapid pace of AI development and geopolitical risks.
   - Posterior odds = Prior odds × Likelihood ratio (considering all evidence).
   - Given the strong evidence for increased risk, the posterior probability is substantially higher than the prior, potentially in the range of 5-10%.

5. **Red Team Thinking**
   - Overconfidence in expert opinion could skew the forecast. The disagreement between AI experts and superforecasters highlights uncertainty.
   - Tail risks, such as unknown unknowns, could introduce unforeseen extinction mechanisms.
   - An alternative forecast might emphasize the resilience of human civilization and the potential for mitigating actions, suggesting a lower probability.
   - Adjusting for these considerations, a conservative estimate might be slightly lower than the initial posterior.

6. **Final Forecast**
   - Considering the base rate, evidence, and red team thinking, the final calibrated probability of human extinction before 2100 is estimated at 7%.

Final: 7%

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=31032 | tokens: prompt=4133, completion=1418, total=5551 | cost=$0.000000

```md

# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For a base rate on human extinction by 2100, I need to consider several reference classes:

- **Species longevity**: Homo sapiens has existed for ~300,000 years, and other hominid species survived for over a million years. This would suggest an extremely low extinction probability (<0.01%) for any 75-year period.
- **Expert surveys**: The 2008 Global Catastrophic Risk Conference yielded a median 19% risk by 2100, though this sample was likely biased toward high-risk perception. More recent surveys of AI experts suggest a 3-5% probability from AI alone.
- **Philosophical arguments**: The "Great Filter" argument suggests that civilizations reaching our technological level may face high extinction probabilities, though this is theoretical.

Given these considerations, I'll select a prior of 2% for human extinction by 2100. This acknowledges the historically low extinction rate while recognizing we're in an unprecedented era of anthropogenic risks.

## 2. Comparison to Base Case

**Similarities to base case**:
- Humans remain a highly adaptable species with global distribution
- We have survived previous existential challenges (Cold War nuclear tensions, pandemics)
- Basic biological resilience remains unchanged

**Key differences**:
- Unprecedented technological capabilities creating novel extinction pathways
- Exponential growth in potentially dangerous technologies (AI, biotech)
- Global interconnection that could amplify catastrophic events
- Emergence of potential digital minds/AGI for the first time in Earth's history
- Anthropogenic risks now dominate natural extinction risks

These differences significantly increase extinction probability compared to historical baselines.

## 3. Evidence Evaluation (Likelihoods)

**Evidence 1: Accelerating AI capabilities**
- P(E|H): If human extinction occurs, rapid AI advancement without adequate safety measures is highly likely (~80%)
- P(E|~H): If humans don't go extinct, we'd still expect to see AI advancement, but with better safety measures (~30%)
- Likelihood ratio: ~2.7
- This evidence increases extinction probability

**Evidence 2: Geopolitical tensions**
- P(E|H): If extinction occurs, heightened great-power conflict would likely be a contributing factor (~60%)
- P(E|~H): Even without extinction, we might see significant tensions (~40%)
- Likelihood ratio: ~1.5
- This moderately increases extinction probability

**Evidence 3: International cooperation on existential risks**
- P(E|H): If extinction occurs, international cooperation on risks would have been insufficient (~90%)
- P(E|~H): Without extinction, we'd expect to see some cooperation, as we do (~50%)
- Likelihood ratio: ~1.8
- The current level of cooperation is insufficient, increasing extinction probability

**Evidence 4: Democratization of dangerous biotechnology**
- P(E|H): If extinction occurs, easier access to bioweapons would be a plausible factor (~50%)
- P(E|~H): Without extinction, we'd still see this democratization but with better safeguards (~30%)
- Likelihood ratio: ~1.7
- This increases extinction probability

## 4. Bayesian Updating (Posterior Probability)

Starting with my prior of 2% (odds of 1:49), I'll update based on the evidence:

**Update on AI capabilities**:
- Prior odds: 1:49 (2%)
- Likelihood ratio: 2.7
- Posterior odds: 2.7:49 = 1:18.1
- Posterior probability: 1/(1+18.1) ≈ 5.2%

**Update on geopolitical tensions**:
- Prior odds: 1:18.1 (5.2%)
- Likelihood ratio: 1.5
- Posterior odds: 1.5:18.1 = 1:12.1
- Posterior probability: 1/(1+12.1) ≈ 7.6%

**Update on insufficient international cooperation**:
- Prior odds: 1:12.1 (7.6%)
- Likelihood ratio: 1.8
- Posterior odds: 1.8:12.1 = 1:6.7
- Posterior probability: 1/(1+6.7) ≈ 13%

**Update on biotechnology risks**:
- Prior odds: 1:6.7 (13%)
- Likelihood ratio: 1.7
- Posterior odds: 1.7:6.7 = 1:3.9
- Posterior probability: 1/(1+3.9) ≈ 20%

After incorporating all evidence, my posterior probability is approximately 20%.

## 5. Red Team Thinking

**Potential overconfidence**:
- I may be overweighting recent AI developments and underestimating human adaptability
- The evidence pieces aren't fully independent, potentially leading to double-counting
- Expert opinions on existential risk may suffer from selection effects and publication bias

**Alternative scenarios**:
- Humans might develop robust safety measures for AI and biotech
- International cooperation could significantly improve in response to clear dangers
- Technological progress might plateau in dangerous domains

**Best alternative forecast**:
A more optimistic view would place extinction risk at 5-10%, arguing that:
1. Historical resilience of humanity suggests high adaptability
2. Safety research is accelerating alongside capability development
3. Awareness of risks is growing among policymakers

**Rebuttal**: While these points have merit, they underestimate the unprecedented nature of current risks 

```

#### Gemini

- ok=True | t_ms=59429 | tokens: prompt=3975, completion=1534, total=9903 | cost=$0.000000

```md

Here is my probabilistic forecast.

### 1. Base Rate (Prior) Selection

*   **Selected Base Rate:** I will start with a prior probability of **1.5%** for human extinction in the next 75 years.
*   **Reasoning:** The historical base rate for hominid species extinction is near-zero on this timescale but is a poor guide because it excludes novel technological risks. Expert opinion is a more relevant, albeit noisy, reference class. The 2008 Global Catastrophic Risk Conference's 19% median is widely considered an unrepresentative, biased sample. More recent estimates for AI-driven extinction range from 0.38% (superforecasters) to 3-5% (AI experts). A 1.5% prior for all-causes extinction acknowledges that we are in a uniquely dangerous period (the "anthropocene") and sits in a plausible, conservative part of the expert opinion spectrum before considering the most recent evidence.
*   **Initial Odds:** A 1.5% probability corresponds to prior odds of 1.5 / 98.5, or approximately **1-to-65**.

### 2. Comparison to Base Case

*   **Similarities:** The base case of a "generic technological century" shares fundamental constants like human nature and geopolitics. The risk of nuclear war, for instance, has been present for decades.
*   **Differences:** The current 75-year period is profoundly different. The primary drivers of risk are no longer just nuclear weapons or natural disasters but are now dominated by rapidly accelerating, novel technologies.
    *   **AI Acceleration:** The pace of AI development since 2022 is unprecedented. Capabilities are emerging years ahead of schedule, creating a potential "vulnerable period" where our ability to ensure safety is outstripped by the power of the systems we are creating. This is the single most important difference from any prior era.
    *   **Biotechnology & Geopolitics:** The democratization of advanced biotechnology (e.g., CRISPR) and a simultaneous increase in great-power competition create a fertile ground for both engineered pandemics and miscalculation leading to nuclear conflict. These risks amplify each other.

### 3. Evidence Evaluation (Likelihoods)

I will assess the key evidence presented in the report. Let H be the hypothesis "humans go extinct by 2100."

*   **Evidence 1 (↑↑): Accelerating AI capabilities (e.g., emergent long-term planning).**
    *   This evidence is much more likely if we are on a path to an AI-driven extinction (P(E|H) is high) than if we are on a path to safely navigating this transition (P(E|~H) is lower). The speed of development reduces the time available to solve the complex alignment problem.
    *   **Likelihood Ratio:** Significantly greater than 1. I estimate this evidence makes the extinction hypothesis about 2.5 times more likely. **LR ≈ 2.5**.

*   **Evidence 2 (↑): Escalated geopolitical tensions.**
    *   A nuclear exchange, a primary extinction mechanism, is preceded by high tensions. However, humanity has survived many periods of high tension before. This evidence increases the background risk.
    *   **Likelihood Ratio:** Moderately greater than 1. **LR ≈ 1.5**.

*   **Evidence 3 (↓): The Bletchley Declaration and international AI safety focus.**
    *   This is a necessary first step for a "survival" scenario, making it more likely under ~H. However, such declarations can be performative and may still occur on a path to failure. Its impact is currently small.
    *   **Likelihood Ratio:** Slightly less than 1. **LR ≈ 0.8**.

### 4. Bayesian Updating (Posterior Probability)

I will update the prior odds using the likelihood ratios from the evidence.

*   **Prior Odds:** 1:65 (equivalent to 1.5%)
*   **Update with AI Progress (LR ≈ 2.5):**
    *   Posterior Odds = (1/65) × 2.5 = 2.5/65 ≈ 1:26
    *   This brings the probability to 1 / (1 + 26) ≈ 3.7%
*   **Update with Geopolitical Tensions (LR ≈ 1.5):**
    *   Posterior Odds = (1/26) × 1.5 = 1.5/26 ≈ 1:17.3
    *   This brings the probability to 1 / (1 + 17.3) ≈ 5.5%
*   **Update with Governance Efforts (LR ≈ 0.8):**
    *   Posterior Odds = (1/17.3) × 0.8 = 0.8/17.3 ≈ 1:21.6
    *   This brings the probability to 1 / (1 + 21.6) ≈ 4.4%

After sequentially applying the updates, the posterior probability is approximately **4.5-5%**. The most significant factor is the rapid progress in AI, which more than doubles the initial prior.

### 5. Red Team Thinking

*   **Alternative Forecast (Lower):** A plausible alternative forecast is 1-2%. This view would argue that AI progress will plateau or that alignment is an easier problem than feared, that nuclear deterrence is robust, and that human ingenuity and resilience are being underestimated. It would also emphasize that the resolution criteria—*zero* humans alive—is an extremely high bar that even a nuclear winter or global pandemic is unlikely to meet.
*   **Alternative Forecast (Higher):** A forecast of 10%+ could be justified by arguing I have underestimated the speed of AI progress and the likelihood of a "race to the bott

```

#### Grok

- ok=True | t_ms=62322 | tokens: prompt=4218, completion=1277, total=6269 | cost=$0.000000

```md

### 1. Base Rate (Prior) Selection
An appropriate base rate for human extinction before 2100 should draw from historical species longevity but adjust for the unprecedented nature of current risks. Using the hominid species reference class (e.g., *Homo sapiens* surviving ~300,000 years, other hominids lasting over a million), the probability of extinction in any 75-year period is extremely low—likely <0.01%, based on no observed extinctions in comparable timescales for similar species. However, this is a poor fit because it ignores novel anthropogenic risks like AI, nuclear weapons, and biotech, which dominate modern threats (as per the research report). Expert surveys provide a better, though imperfect, proxy in this data-poor domain: the 2008 Oxford poll gave 19%, but it's biased high due to selection effects; more balanced recent estimates (e.g., AI experts at 3-5% for AI alone, superforecasters at 0.38%) suggest a lower but non-trivial rate. Philosophical arguments like the Doomsday Argument imply a higher baseline due to the "Great Filter," but they're unquantifiable.

I choose a prior of ~1% (odds 1:99) as a weakly held starting point. This reflects the near-zero historical base rate but acknowledges we're in a unique "Anthropocene" era with self-generated risks, aligning with the Bayesian sketch's suggestion of 1-2% (equivalent to ~50-100 "century-trials" without extinction).

### 2. Comparison to Base Case
The base case (historical hominid survival) is similar in that natural extinction risks (e.g., asteroids, volcanoes) remain low and haven't caused extinction over millennia, suggesting inherent resilience in human-like species. Global population growth and technological adaptation have historically buffered against such threats.

However, the current situation differs profoundly: risks are now dominated by human-created mechanisms (AI, nuclear war, engineered pandemics) that are exponential in pace and scale, with no historical precedent. Unlike past eras, we're in a "vulnerable period" of rapid technological change outstripping governance (e.g., AI capabilities advancing years ahead of schedule). Global interconnection amplifies propagation (e.g., pandemics spread faster), and "digital minds" introduce non-human intelligences. These differences matter because they shift the risk from rare natural events to frequent, high-stakes human errors or misalignments, warranting an upward adjustment from the negligible historical base rate.

### 3. Evidence Evaluation (Likelihoods)
- **Rapid AI progress (↑ Large):** This evidence (e.g., emergent long-term planning in models, ahead-of-schedule benchmarks) is highly likely if extinction occurs via unaligned AGI (P(E|H) ≈ 80%, as it shortens timelines to catastrophe). If no extinction, such progress could still happen but is less dire if alignment succeeds (P(E|~H) ≈ 40%). Likelihood ratio ≈ 2 (increases probability significantly, as it heightens the dominant novel risk).
- **Heightened geopolitical tensions (↑ Moderate):** Escalations (e.g., 2024 election rhetoric, nuclear risks) are more likely under extinction scenarios involving nuclear holocaust (P(E|H) ≈ 60%). Without extinction, tensions can rise without escalation (P(E|~H) ≈ 30%). Likelihood ratio ≈ 2 (increases probability by elevating miscalculation risks).
- **Democratization of biotech (↑ Small):** Easier access to tools like CRISPR increases pandemic risks (P(E|H) ≈ 50% for extinction paths). But it's expected in non-extinction worlds too (P(E|~H) ≈ 40%). Likelihood ratio ≈ 1.25 (modest increase, as it lowers barriers to engineered threats).
- **International risk governance (↓ Small):** Efforts like the Bletchley Declaration are more likely if extinction is avoided through cooperation (P(E|~H) ≈ 30%), but less so in doomed timelines where governance fails (P(E|H) ≈ 10%). Likelihood ratio ≈ 0.33 (decreases probability by suggesting mitigation potential).
- **Expert disagreement (↔):** The gap (3-5% vs. 0.38%) highlights uncertainty but doesn't strongly favor H or ~H (likelihood ratio ≈ 1, neutral but increases overall uncertainty).

Net: Strong upward pressure from AI and geopolitics, tempered slightly by governance.

### 4. Bayesian Updating (Posterior Probability)
Using Bayes' Rule conceptually: Posterior odds = Prior odds × Likelihood ratio.

Start with prior odds 1:99 (1%).

Apply a net likelihood ratio of ~4 (qualitative aggregation: AI and geopolitics contribute ~2 each, biotech ~1.25, governance ~0.33; multiplicative net ~4, reflecting "significant upward update" in the sketch and evidence weights).

Posterior odds = 1:99 × 4 = 4:99 ≈ 4:99.

Posterior probability = 4 / (4 + 99) ≈ 3.9%.

This remains uncertain due to the unprecedented nature of risks and expert splits, but the update shifts from a historical near-zero to a small but meaningful probability, driven mainly by AI acceleration.

### 5. Red Team Thinking
My forecast might be overconfident in upward updates from AI hype, as recent 

```

### Aggregation (BMC)

- final_probability=0.1003

- bmc_summary={"mean": 0.10034486550064724, "p10": 0.0008829839578658174, "p50": 0.045320164031568605, "p90": 0.2829014602275656}