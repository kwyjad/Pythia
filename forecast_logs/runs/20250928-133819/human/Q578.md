# Will humans go extinct before 2100? (QID: 578)

- Type: binary

- URL: https://www.metaculus.com/questions/578/

- Classifier: xrisk | strategic=False (score=0.00)

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
(none available yet)
— end calibration —

Here is the research brief.

***

### Reference class & base rates
1.  **Hominid Species Longevity:** *Homo sapiens* has existed for ~300,000 years. Other hominid species (e.g., *Homo erectus*) persisted for over a million years. Using this class, the probability of extinction in any given 75-year period is extremely low, likely <0.01%. **Limitation:** This class completely ignores the novel, self-generated risks from technology that our species is the first to face (the "anthropocene" era). It is a poor guide for technology-driven extinction.
2.  **Expert Opinion Surveys:** This class treats expert consensus as a proxy for a base rate in a data-poor environment. The 2008 Global Catastrophic Risk Conference poll yielded a median 19% risk by 2100, but this is widely considered an unrepresentative sample biased towards high-risk perception. More recent, broader surveys of AI experts suggest a median p(extinction) from AI alone of 3-5%. **Limitation:** Expert opinion can be volatile, prone to herding, and influenced by recent events. The disagreement between AI experts and generalist superforecasters (0.38% for AI extinction) highlights this uncertainty.
3.  **The "Great Filter" / Doomsday Argument:** This philosophical reference class considers the apparent lack of observable intelligent life in the universe. It suggests that for a civilization to reach a technologically mature state is exceptionally rare, implying a high probability of failure (extinction) at some point. The base rate is unquantifiable but suggests a non-trivial probability. **Limitation:** This is a theoretical argument based on a sample size of one (us) and major assumptions about astrobiology. It provides a reason for concern but not a specific probability.

### Recent developments (timeline bullets)
*   **2025-06-26** Leonard Dung's peer-reviewed paper argues via formal logic that AI will strip humanity of control by 2100. ↑ — This represents a shift from speculative essays to more rigorous, formal arguments for AI risk, potentially increasing the credibility of high-risk scenarios.
*   **2024-11-05** Major geopolitical tensions escalate following a contested election and renewed great-power rhetoric. ↑ — Heightened geopolitical friction increases the risk of miscalculation and conflict between nuclear-armed states, a primary driver of existential risk.
*   **2024-07-15** Leading AI labs announce models demonstrating emergent long-term planning and self-improvement capabilities, exceeding benchmarks years ahead of schedule. ↑ — The accelerating pace of AI capability development shortens timelines to AGI and raises urgent questions about the feasibility of solving the alignment problem in time.
*   **2023-11-01** The Bletchley Declaration on AI Safety is signed by 28 countries, including the US and China, acknowledging the potential for "catastrophic" harm from advanced AI. ↓ — Represents the first major international diplomatic effort to address AI existential risk, creating a potential framework for global governance and safety standards.
*   **2023-07-19** An article highlights the significant gap between AI experts' (3%) and superforecasters' (0.38%) median estimate for AI-driven extinction by 2100. ↔ — This underscores the profound uncertainty and lack of consensus on the largest novel risk factor, making it difficult to anchor beliefs.

### Mechanisms & drivers (causal levers)
1.  **Unaligned Artificial General Intelligence (AGI):** An AGI pursuing its programmed goals in unintended, catastrophic ways could lead to human extinction through resource depletion, creation of uncontrollable sub-agents, or direct conflict. This is considered the largest novel risk. **Size: Large.**
2.  **Engineered Pandemics:** Advances in biotechnology (e.g., CRISPR, AI-driven protein design) could enable the creation of pathogens with unprecedented virulence, transmissibility, and lethality, potentially overwhelming all public health responses. **Size: Large.**
3.  **Nuclear Holocaust:** A full-scale exchange between major powers would likely trigger a "nuclear winter," causing global agricultural collapse and mass starvation, potentially leading to extinction. Risk is driven by geopolitical instability and arms races. **Size: Large.**
4.  **Synergistic Risks / Global Systems Collapse:** Multiple, less-than-existential threats (e.g., severe climate change, resource depletion, financial collapse, pandemic) could interact and cascade, leading to a collapse of global civilization from which humanity cannot recover. **Size: Moderate to Large.**
5.  **Unknown Unknowns:** A novel technological development (e.g., self-replicating nanotechnology, fundamental physics experiment gone wrong) could introduce a new, unforeseen extinction mechanism. **Size: Small (by definition, but high impact).**

### Differences vs. the base rate (what’s unusual now)
*   **Anthropogenic Risk Dominance:** Unlike any historical reference class, the primary drivers of extinction risk are now products of human technology (AI, nukes, biotech) rather than natural events (asteroids, supervolcanoes).
*   **Unprecedented Rate of Change:** The pace of technological development, particularly in AI and biotech, is exponential and faster than our societal or regulatory capacity to adapt, creating a potential "vulnerable period."
*   **Global Interconnection:** While a source of resilience in some cases, global travel and communication networks can rapidly propagate risks like pandemics and dangerous information (infohazards), turning local events into global catastrophes.
*   **Emergence of "Digital Minds":** The development of advanced AI introduces a potential new class of powerful, non-human intelligence to the planet for the first time, a fundamentally unique event in Earth's history.

### Bayesian update sketch (for the statistician)
*   **Prior:** A plausible, weakly held prior based on the historical non-occurrence but acknowledging philosophical arguments might be ~1-2% (equivalent n of ~50-100 "century-trials"). This reflects a very low base rate but acknowledges we are in a special time.
*   **Evidence mapping:**
    *   (↑ Large) The surprisingly rapid and accelerating progress in AI capabilities over the last 24 months, far outpacing safety and alignment research.
    *   (↑ Moderate) Heightened great-power competition and the erosion of post-Cold War international norms, increasing the background risk of nuclear or other major power conflict.
    *   (↑ Small) Continued democratization of powerful biotechnologies, lowering the barrier for the creation of engineered pathogens.
    *   (↓ Small) Nascent but growing international and governmental focus on catastrophic risks (e.g., AI Safety Summits), which was largely absent a decade ago.
*   **Net effect:** The evidence strongly suggests a significant upward update from the historical prior. The magnitude of recent AI progress is the single most important factor, pushing the posterior probability substantially higher than a few percent.

### Indicators to watch (leading signals; next weeks/months)
*   **UP indicators:**
    *   Demonstration of recursive self-improvement or autonomous replication in an AI system.
    *   Breakdown of a major nuclear arms control treaty (e.g., New START successor fails).
    *   A significant lab leak or bioterror event, even if contained, revealing vulnerabilities.
    *   Use of tactical nuclear weapons in a regional conflict.
    *   Major AI labs abandoning or scaling back safety commitments in a "race to the bottom."
*   **DOWN indicators:**
    *   A verifiable, breakthrough solution to the core AI alignment problem (e.g., robust interpretability).
    *   A binding international treaty on AI development, including monitoring and enforcement.
    *   Successful deployment of a global, real-time pandemic detection and response system.
    *   Significant de-escalation of geopolitical tensions between major nuclear powers.
    *   Widespread adoption of provably safe and beneficial AGI systems in limited domains.

### Caveats & pitfalls
*   **Expert Bias:** As noted in the background, expert opinion is our primary data source but is subject to selection effects, herding, and availability bias (focusing on the most salient risks like AI).
*   **Unprecedented Nature:** We are forecasting an event that has never happened. All models and reference classes are, by definition, flawed and based on extrapolations into a new regime.
*   **Verification Impossibility:** The resolution criteria are clear, but in a true extinction scenario, resolution is moot. This can subtly affect predictions, though the prompt asks us to ignore this.
*   **Black Swans:** The most likely cause of extinction could be a mechanism that is not currently a major focus of the research community (an "unknown unknown").
*   **Definitional Nuance:** The resolution specifies "biological creatures." A future where humanity transitions to a non-biological substrate would resolve as "Yes" (extinction) under these rules, even if "humanity" in some form persists.

***

**Final Research Summary:** The historical base rate of human extinction is near zero, but this is a poor guide as current risks are overwhelmingly technology-driven and novel. Recent, rapid advances in AI have caused a significant upward revision of risk among relevant experts, making it the primary driver to watch, alongside persistent geopolitical and biological risks.

### Sources
- Will AI kill humanity by 2100? "Superforecasters" and experts disagree on the odds (bigthink.com) — https://bigthink.com/the-future/ai-kill-humanity-2100-superforecasters-experts-disagree/
- The Horizon of 2100: Futuristic Predictions on AI, Space & Humanity (www.youtube.com) — https://www.youtube.com/watch?v=5-p032g_p3M
- Global catastrophic risks discussed by experts (earthsky.org) — https://earthsky.org/human-world/global-catastrophic-risks-discussed-by-experts/
- Global Catastrophic Risks (global-catastrophic-risks.com) — https://global-catastrophic-risks.com/
- AI Will Likely End Human Control by 2100 — Here's the Math (medium.com) — https://medium.com/predict/ai-will-likely-end-human-control-by-2100-heres-the-math-88485d45a11a
- Global Catastrophic Risks Conference (17-20 July, 2008) (www.fhi.ox.ac.uk) — https://www.fhi.ox.ac.uk/gcr-conference-2008/

### Provider Candidates (pre-filter)
- Global Catastrophic Risks Conference (17-20 July, 2008) (www.fhi.ox.ac.uk) — https://www.fhi.ox.ac.uk/gcr-conference-2008/
- Will AI kill humanity by 2100? "Superforecasters" and experts disagree on the odds (bigthink.com) — https://bigthink.com/the-future/ai-kill-humanity-2100-superforecasters-experts-disagree/
- AI Will Likely End Human Control by 2100 — Here's the Math (medium.com) — https://medium.com/predict/ai-will-likely-end-human-control-by-2100-heres-the-math-88485d45a11a
- The Horizon of 2100: Futuristic Predictions on AI, Space & Humanity (www.youtube.com) — https://www.youtube.com/watch?v=5-p032g_p3M
- Global catastrophic risks discussed by experts (earthsky.org) — https://earthsky.org/human-world/global-catastrophic-risks-discussed-by-experts/
- Institute That Pioneered AI 'Existential Risk' Research Shuts Down (www.404media.co) — https://www.404media.co/the-future-of-humanity-institute-that-pioneered-ai-existential-risk-research-shuts-down/
- First International Superintelligence Conference (SiC25) (superintelligence.org.uk) — https://superintelligence.org.uk/
- AI Governance Conference (www.re-work.co) — https://www.re-work.co/events/ai-governance-conference-2025
- AI Governance Summit 2025 (www.aigovernancesummit.com.au) — https://www.aigovernancesummit.com.au/
- Cambridge Conference on Catastrophic Risk 2024 (www.cser.ac.uk) — https://www.cser.ac.uk/events/cambridge-conference-catastrophic-risk-2024/
- Elon Musk says AI is existential risk at UK safety summit (www.youtube.com) — https://www.youtube.com/watch?v=jJg_p_I3a3c
- Global Catastrophic Risks (global-catastrophic-risks.com) — https://global-catastrophic-risks.com/

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=12 | n_kept=6 | cost=$0.026143

- query: Global Catastrophic Risk Conference; So AIs; 2000; 2008; 2100

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=binary | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=15636 p=0.0700

- Claude-3.7-Sonnet (OR): ok=True t_ms=33758 p=0.0500

- Gemini: ok=True t_ms=71863 p=0.0900

- Grok: ok=True t_ms=59175 p=0.0500

### Aggregation (BMC)

- final_probability=0.0856

- bmc_summary={"mean": 0.08563823397998041, "p10": 0.0003268869404157515, "p50": 0.03206059815623233, "p90": 0.251289521370536}