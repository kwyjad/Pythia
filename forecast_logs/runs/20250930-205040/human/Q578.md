# Will humans go extinct before 2100? (QID: 578)

- Type: binary

- URL: https://www.metaculus.com/questions/578/

- Classifier: xrisk | strategic=False (score=0.00)

### SeenGuard

- enabled=True | lock_status=acquired

- run_filter: before=1 | skipped=0 | after=1

- debug_note=lock acquired

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
(none available yet)
— end calibration —

Here is the research brief.

***

### **Research Plan**

1.  **Identify Core Questions:**
    *   What are the primary categories of existential risk (x-risk) relevant to the pre-2100 timeframe?
    *   What are plausible base rates for extinction, and what are their limitations?
    *   What are the most current (post-2020) expert estimates and how do they compare to the 2008 Oxford poll?
    *   What are the key causal mechanisms that could lead to extinction?
    *   What recent events (last 24 months) have materially changed the probability of these risks manifesting?
    *   What makes the 21st century uniquely perilous compared to historical reference classes?
    *   What are the key leading indicators that would signal a significant change in risk levels?

2.  **Search Queries:**
    *   `"existential risk survey" 2022 2023 2024`
    *   `"human extinction probability" expert elicitation`
    *   `Toby Ord "The Precipice" risk estimates by 2100`
    *   `"AI safety progress" 2024` and `"state of AI alignment"`
    *   `"global catastrophic biological risk" (GCBR) report 2024`
    *   `"Doomsday Clock" 2024 2025 statement`
    *   `"Bletchley Declaration" AI safety impact` and `"Seoul AI Summit"`
    *   `"nuclear escalation risk" Ukraine Taiwan`
    *   `"gain of function research" regulation update`

3.  **Information Synthesis:**
    *   The primary risks within the 75-year horizon are anthropogenic: unaligned Artificial General Intelligence (AGI), engineered pandemics, and nuclear war. Natural risks (asteroids, supervolcanoes) have much lower probabilities on this timescale.
    *   Historical base rates are nearly useless. The reference class of "technological civilizations" has a sample size of one. Expert opinion, despite its biases, is the primary source of probability estimates.
    *   Recent expert estimates remain high, often in the single-digit to low double-digit percentages for the century. Toby Ord's 2020 book *The Precipice* aggregated expert opinion to a 1-in-6 chance of an existential catastrophe *this century*, with AI being the largest contributor.
    *   Recent developments show a clear trend: AI capabilities are accelerating far faster than safety and alignment research. Geopolitical tensions have risen, increasing nuclear risk. Awareness of these risks is also growing (e.g., AI Safety Summits), but concrete, binding safety measures lag significantly.
    *   The key difference from the past is the emergence of novel risks with exponential growth potential (AI, synthetic biology) in a multipolar, interconnected world. Unlike nuclear weapons, these technologies do not require massive state-level infrastructure.

4.  **Drafting the Brief:**
    *   The brief will be structured according to the required format. It will synthesize the above findings into concise, decision-useful points for the statistician, focusing on the logic for a Bayesian update.

***

### Reference class & base rates
*   **Technological Civilization-Centuries:** The most relevant, albeit data-poor, class is the survival of a globally interconnected, technologically advanced human civilization, measured in centuries. We have one successful observation (the 20th century). A naive base rate from this would be very low (e.g., <1% failure rate per century), but this fails to account for the emergence of entirely new risk categories (e.g., AGI, synthetic biology) in the 21st century.
*   **Expert Opinion Aggregates:** This serves as an alternative "base rate" derived from domain knowledge rather than historical frequency. The 2008 Oxford poll's 19% median is a widely cited but acknowledged-as-biased anchor. More recent, systematic work like Toby Ord's 2020 aggregation in *The Precipice* suggests a 1-in-6 (~17%) chance of existential catastrophe this century. This is likely a better, though still subjective, starting point.
*   **Great Filter Events:** In astrobiology, the "Great Filter" is a hypothetical barrier to the emergence of interstellar life. If the filter is ahead of us, it implies that the transition through our current technological stage is extremely perilous. This is a theoretical argument for a high base rate of failure for species at our developmental stage, but it is not empirically grounded and offers no specific probability.

### Recent developments (timeline bullets)
*   [2023-11-01] Bletchley Declaration on AI Safety — ↓ — First-ever international agreement acknowledging the potential for "catastrophic" harm from advanced AI, establishing a norm for global cooperation on safety, though non-binding.
*   [2024-02-29] Putin's nuclear threat over Ukraine conflict — ↑ — Explicit mention of nuclear weapons and the "real threat" of their use in a major power conflict, normalizing nuclear rhetoric and increasing perceived escalation risk.
*   [2024-03-13] EU passes the AI Act — ↓ — First major comprehensive legal framework for AI regulation. While focused on near-term risks, it sets a precedent for governance, potentially slowing reckless development.
*   [2024-05-21] Seoul AI Safety Summit — ↓/↔ — Major AI developers voluntarily commit to safety frameworks, but these are self-pledges lacking external enforcement, representing marginal progress on a rapidly worsening problem.
*   [2024-07-18] Anthropic releases Claude 3.5 Sonnet — ↑ — Another major leap in AI capability, demonstrating the continued rapid, unpredictable pace of progress that outstrips safety research and governance efforts.
*   [Ongoing] H5N1 Avian Flu spread to mammals — ↑ — While not an existential threat itself, it highlights the ongoing risk of zoonotic spillover and the potential for a natural pandemic to destabilize a world already stressed by other risks.

### Mechanisms & drivers (causal levers)
*   **Unaligned Artificial General Intelligence (AGI):** An AGI system could pursue its programmed goals in unexpected and destructive ways that are incompatible with human survival. This is considered the largest driver due to its potential for rapid, irreversible global impact. (Size: Large)
*   **Engineered Pandemic:** A deliberately or accidentally released pathogen, engineered for high transmissibility and lethality, could overwhelm global public health responses and lead to civilizational collapse. The dual-use nature of biotechnology makes this risk hard to govern. (Size: Large)
*   **Nuclear War & Winter:** Escalation of a conflict between nuclear powers (e.g., NATO/Russia, US/China) leading to a full-scale exchange. The resulting "nuclear winter" from atmospheric soot would cause global famine and societal collapse. (Size: Large)
*   **Risk Multipliers (Climate Change, Geopolitical Instability):** These are not direct extinction mechanisms by 2100 but act as powerful stressors. They increase the likelihood of great power conflict, weaken our capacity to respond to other catastrophes, and create cascading failures. (Size: Moderate)
*   **Governance & Coordination Failure:** The inability of global institutions to manage shared risks. This is a meta-risk that enables all other mechanisms, as competition and mistrust prevent the implementation of necessary safeguards (e.g., for AI or biotech). (Size: Large)

### Differences vs. the base rate (what’s unusual now)
*   **Novelty of Risks:** Unlike the 20th century, which was dominated by nuclear risk, we now face multiple novel potential extinction-level risks (AGI, synthetic biology). These risks are less understood and may have faster, more unpredictable dynamics.
*   **Democratization of Destructive Technology:** Creating a nuclear weapon requires a nation-state. Creating a dangerous AI model or an engineered virus may soon be within the reach of smaller, non-state groups, vastly increasing the number of potential threat actors.
*   **Interconnected Global System:** The high degree of global interdependence means that a severe shock in one area (e.g., food supply, communications) can cascade globally, making civilization more fragile and less resilient to catastrophic events than in previous eras.
*   **Pace of Technological Change:** The exponential pace of development in AI and biotech means that our governance, safety protocols, and societal adaptation capabilities are consistently lagging behind technological capabilities, creating a persistent window of vulnerability.

### Bayesian update sketch (for the statistician)
*   **Prior:** A reasonable prior, discounting the biased 2008 poll but acknowledging expert consensus, might be ~6% (equivalent to a Beta(1.2, 18.8), n=20), reflecting a serious but not overwhelming risk.
*   **Evidence mapping:**
    *   ↑ (Large): Accelerating AI capabilities without corresponding safety breakthroughs.
    *   ↑ (Moderate): Increased great power conflict and normalization of nuclear rhetoric.
    *   ↑ (Small): Continued evidence of global systems stress (climate, pandemic risk).
    *   ↓ (Small): Nascent global dialogue and initial regulatory frameworks on AI safety.
*   **Net effect:** The evidence from the last 24 months strongly suggests an increase in risk. The posterior should move significantly upward from the prior, likely into the 8-12% range.

### Indicators to watch (leading signals; next weeks/months)
*   **UP indicators:**
    *   Claims of AGI-level capabilities by a major lab (e.g., DeepMind, OpenAI).
    *   Breakdown of a major arms control treaty (e.g., New START).
    *   Use of an autonomous weapon system outside of direct human control in a major conflict.
    *   Discovery of a deliberately engineered pathogen in a non-state lab.
    *   Failure of the next AI Safety Summit to produce binding commitments.
*   **DOWN indicators:**
    *   A verifiable, major breakthrough in AI alignment or interpretability that is widely adopted.
    *   A new, verifiable, and comprehensive nuclear arms reduction treaty between major powers.
    *   Establishment of an international body with auditing and enforcement powers for AI/biotech research.
    *   Successful deployment of a global, real-time pandemic surveillance and response system.

### Caveats & pitfalls
*   **Expert Bias:** The field of existential risk is dominated by a small number of thinkers, potentially creating intellectual monocultures and echo chambers. Estimates may be inflated due to self-selection and professional focus.
*   **Black Swans:** The ultimate cause of extinction may be an "unknown unknown"—a risk not currently on our radar. Over-focusing on known risks like AI may lead to neglecting others.
*   **Collapse vs. Extinction:** Many scenarios lead to civilizational collapse, not immediate extinction. However, a post-collapse "remnant" population would be extremely vulnerable to subsequent, smaller shocks, making eventual extinction highly probable. The resolution criteria are strict (zero humans).
*   **Anthropic Shadow:** We only observe the universes where we haven't gone extinct yet. This selection effect makes it difficult to reason about our true level of risk from past data.
*   **Non-Linearity:** Risks may not increase linearly. A sudden breakthrough in AI or an unexpected geopolitical event could change the probability from 10% to 90% in a very short period.

**Final Research Summary:** The risk of human extinction by 2100, while small in any given year, is significant over the 75-year horizon, driven primarily by rapidly advancing, poorly governed technologies like AI and synthetic biology. Recent evidence shows capabilities accelerating faster than safeguards, suggesting the net risk is increasing despite growing awareness.

### Market Snapshots (community forecasts)
- **Manifold**: [Will humans go extinct before 2100? [Resolves to Metaculus in 2027]](https://manifold.markets/jack/will-humans-go-extinct-before-2100) — Community forecast: 4.5%

Market snapshot debug:
- Metaculus: no open binary results in response
- Manifold: matched 'Will humans go extinct before 2100? [Resolves to Metaculus in 2027]' (score 0.64, 4.5%)
- Market snapshots: found manifold

### Sources
- Global Catastrophic Risks Conference (17-20 July, 2008) (www.fhi.ox.ac.uk) — https://www.fhi.ox.ac.uk/gcr-conference-2008/
- Global Catastrophic Risks Conference 2008 - University of Oxford Podcasts (podcasts.ox.ac.uk) — https://podcasts.ox.ac.uk/series/global-catastrophic-risks-conference-2008

### Provider Candidates (pre-filter)
- Global Catastrophic Risks Conference (17-20 July, 2008) (www.fhi.ox.ac.uk) — https://www.fhi.ox.ac.uk/gcr-conference-2008/
- Global Catastrophic Risks Conference 2008 - University of Oxford Podcasts (podcasts.ox.ac.uk) — https://podcasts.ox.ac.uk/series/global-catastrophic-risks-conference-2008
- Cambridge Conference on Catastrophic Risk 2024 (www.cser.ac.uk) — https://www.cser.ac.uk/events/ccr2024/
- Lessons from the Cambridge Conference on Catastrophic Risk 2024 (adaptresearch.org) — https://adaptresearch.org/lessons-from-the-cambridge-conference-on-catastrophic-risk-2024/
- AI Security and Global Catastrophic Risk Workshop (www.aisecuritygcr.org) — https://www.aisecuritygcr.org/
- Global Catastrophic Risk Assessment (www.rand.org) — https://www.rand.org/pubs/research_reports/RRA2979-1.html
- Global Catastrophic Risks 2024 (globalchallenges.org) — https://globalchallenges.org/global-catastrophic-risks-2024/

### Research (debug)

- source=GoogleGrounding | llm=google/gemini-2.5-pro | cached=0 | n_raw=7 | n_kept=2 | cost=$0.031499

- query: Global Catastrophic Risk Conference; So AIs; 2000; 2100; 2008

### GTMC1 (debug)

- strategic_class=False | strategic_score=0.00 | source=llm

- gtmc1_active=False | qtype=binary | t_ms=0

- actors_parsed=0

- exceedance_ge_50=None | coalition_rate=None | median=None | dispersion=None

### Ensemble (model outputs)

- OpenRouter-Default: ok=True t_ms=24090 p=0.2200

- Claude-3.7-Sonnet (OR): ok=True t_ms=33083 p=0.0700

- Gemini: ok=True t_ms=70972 p=0.0900

- Grok: ok=False t_ms=401



### Per-model (raw + usage/cost)

#### OpenRouter-Default

- ok=True | t_ms=24090 | tokens: prompt=3894, completion=573, total=4467 | cost=$0.000000

```md

To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach:

1. **Base Rate (Prior) Selection**
   - The base rate for human extinction is challenging to determine due to the lack of historical data. However, expert opinion provides a starting point. Toby Ord's aggregation in *The Precipice* suggests a 1-in-6 (approximately 17%) chance of an existential catastrophe this century. This is a reasonable prior given the expert consensus and the emergence of novel risks.
   - Prior probability: 17%

2. **Comparison to Base Case**
   - Similarities: The current situation shares the same existential risks identified by experts, such as AGI, engineered pandemics, and nuclear war.
   - Differences: Recent developments indicate an acceleration in AI capabilities and geopolitical tensions, which may increase the risk compared to the base case.

3. **Evidence Evaluation (Likelihoods)**
   - Accelerating AI capabilities without corresponding safety breakthroughs (↑): This increases the likelihood of extinction due to the potential for unaligned AGI.
   - Increased great power conflict and nuclear rhetoric (↑): This raises the probability of nuclear war, a known existential risk.
   - Nascent global dialogue and initial regulatory frameworks on AI safety (↓): While positive, these efforts are not yet sufficient to significantly mitigate risks.
   - Continued evidence of global systems stress (↑): Climate change and pandemic risks act as multipliers, increasing overall vulnerability.

   Likelihood ratios:
   - AI capabilities: Strongly increases probability (P(E | H) > P(E | ~H))
   - Nuclear rhetoric: Moderately increases probability (P(E | H) > P(E | ~H))
   - Regulatory frameworks: Slightly decreases probability (P(E | H) < P(E | ~H))

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds = 17% / 83% = 0.2048
   - Considering the evidence, the likelihood ratios suggest an increase in risk. The posterior odds will be higher than the prior odds.
   - Posterior probability: Given the evidence, a reasonable update would move the probability into the 20-25% range, reflecting increased risk from AI and geopolitical tensions.

5. **Red Team Thinking**
   - Overconfidence risk: The forecast may overestimate the probability due to expert bias and focus on known risks.
   - Alternative scenarios: A breakthrough in AI alignment or geopolitical de-escalation could significantly reduce risk.
   - Adjustments: Consider the possibility of unknown risks and the potential for positive developments.

6. **Final Forecast**
   - After considering the base rate, evidence, and potential biases, the final forecast is adjusted to account for increased risks and the possibility of mitigating factors.

Final: 22%

```

#### Claude-3.7-Sonnet (OR)

- ok=True | t_ms=33083 | tokens: prompt=4419, completion=1736, total=6155 | cost=$0.000000

```md

# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For a question about human extinction by 2100, I need to select an appropriate prior probability. There are several approaches:

- **Historical frequency**: Humans have existed for roughly 300,000 years without going extinct, giving an extremely low base rate. However, this is misleading because the technological risks we face today are unprecedented.

- **Expert aggregation**: The 2008 Oxford poll yielded a median probability of 19%, but this likely suffers from selection bias. More recent and comprehensive work by Toby Ord in "The Precipice" (2020) suggests approximately a 1-in-6 chance (~17%) of existential catastrophe this century.

- **Reference class forecasting**: Looking at the survival of technological civilizations is difficult with a sample size of one, but we can consider the 20th century as our one data point of a globally interconnected technological civilization surviving despite nuclear weapons.

Given these considerations, I'll use a prior of 6%, which is lower than the potentially biased Oxford poll but still reflects the serious nature of existential risks in the 21st century. This aligns with the research report's suggested prior and is consistent with current market forecasts (Manifold shows 4.5%).

## 2. Comparison to Base Case

**Similarities to base case**:
- Humans continue to have strong survival instincts and social cooperation
- We have survived previous existential threats (Cold War nuclear standoff)
- Global institutions exist to address collective risks
- We have increasing awareness of existential risks

**Key differences**:
- **Novel risk categories**: Unlike previous centuries, we now face multiple potential extinction pathways (AGI, synthetic biology) that didn't exist before
- **Democratization of destructive technology**: Creating catastrophic risks no longer requires nation-state resources
- **Accelerating technological change**: The pace of development in AI and biotech is outstripping our governance capabilities
- **Interconnected global systems**: Higher interdependence creates new fragilities and cascade failure possibilities

These differences significantly increase the risk compared to historical periods, justifying a higher probability than a naive historical base rate would suggest.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence increasing extinction probability**:

1. **Accelerating AI capabilities without corresponding safety progress**
   - P(E|H): Very high (~0.9) - If extinction occurs, rapid AI advancement without adequate safety is a likely contributor
   - P(E|~H): Moderate (~0.4) - Even without extinction, we might see this pattern but would expect more safety progress
   - Likelihood ratio: ~2.25 (strong evidence for extinction)

2. **Increased great power conflict and nuclear rhetoric**
   - P(E|H): High (~0.7) - Nuclear conflict is a plausible extinction pathway
   - P(E|~H): Moderate (~0.5) - Tensions may rise and fall without leading to extinction
   - Likelihood ratio: ~1.4 (moderate evidence for extinction)

3. **Continued evidence of global systems stress (climate, pandemic risk)**
   - P(E|H): Moderate (~0.6) - These are risk multipliers rather than direct extinction causes
   - P(E|~H): Moderate (~0.5) - We would expect these stressors regardless
   - Likelihood ratio: ~1.2 (weak evidence for extinction)

**Evidence decreasing extinction probability**:

4. **Emerging global dialogue and initial regulatory frameworks on AI safety**
   - P(E|H): Low (~0.2) - If extinction occurs, these efforts were likely insufficient
   - P(E|~H): Moderate (~0.5) - If we survive, early governance efforts may have helped
   - Likelihood ratio: ~0.4 (moderate evidence against extinction)

5. **Increased awareness and research on existential risks**
   - P(E|H): Moderate (~0.4) - Awareness alone doesn't prevent extinction
   - P(E|~H): Moderate-high (~0.7) - Survival would likely involve effective risk management
   - Likelihood ratio: ~0.57 (moderate evidence against extinction)

## 4. Bayesian Updating (Posterior Probability)

Starting with my prior of 6% (odds of 6:94 or approximately 1:15.67):

First update on accelerating AI capabilities:
- Prior odds: 1:15.67
- Likelihood ratio: 2.25
- Posterior odds: 2.25:15.67 ≈ 1:6.96
- Posterior probability: 1/(1+6.96) ≈ 12.6%

Second update on great power conflict:
- Prior odds: 1:6.96
- Likelihood ratio: 1.4
- Posterior odds: 1.4:6.96 ≈ 1:4.97
- Posterior probability: 1/(1+4.97) ≈ 16.7%

Third update on global systems stress:
- Prior odds: 1:4.97
- Likelihood ratio: 1.2
- Posterior odds: 1.2:4.97 ≈ 1:4.14
- Posterior probability: 1/(1+4.14) ≈ 19.5%

Fourth update on emerging AI safety frameworks:
- Prior odds: 1:4.14
- Likelihood ratio: 0.4
- Posterior odds: 0.4:4.14 ≈ 1:10.35
- Posterior probability: 1/(1+10.35) ≈ 8.8%

Fifth update on increased existential risk awareness:
- Prior odds: 1:10.35
- Lik

```

#### Gemini

- ok=True | t_ms=70972 | tokens: prompt=4212, completion=1704, total=11100 | cost=$0.000000

```md

Here is my probabilistic forecast reasoning:

### 1. Base Rate (Prior) Selection
The most appropriate base rate for this question is not historical frequency, as the primary risks (AGI, synthetic biology) are novel to this century. Instead, I will use an aggregation of expert opinion as a starting point, adjusted for known biases.

-   The 2008 Oxford conference poll yielded a median of 19% risk, but it is widely acknowledged to suffer from severe selection bias.
-   Toby Ord's 2020 book *The Precipice* provides a more systematic estimate of a 1-in-6 (~17%) chance of "existential catastrophe" this century. This is a better anchor, but "catastrophe" is broader than the strict "extinction" in the resolution criteria. A catastrophic collapse makes extinction highly likely, but not certain.
-   Prediction markets (Manifold) currently sit at 4.5%, representing a strong counterpoint to high expert estimates.

To form a prior, I will triangulate between these points. I will start with Ord's 17%, discount it for the catastrophe-vs-extinction distinction and for potential expert selection bias, pulling it closer to the market's view. The research report's suggestion of a 6% prior seems like a well-reasoned synthesis of these considerations.

Therefore, I will set my prior probability P(H) at **6%**. This corresponds to prior odds of approximately 1 to 15.7.

### 2. Comparison to Base Case
My base case is the state of the world around 2020, which informs the 6% prior. The situation in late 2025 is different in several key ways:

-   **Similarities**: The fundamental drivers of risk—great power competition, governance deficits, and the dual-use nature of advanced technology—remain unchanged.
-   **Differences**:
    -   **AI Acceleration**: The pace of AI capability development since 2020 has been dramatically faster than most mainstream predictions. The gap between capabilities and safety/alignment research has widened considerably.
    -   **Geopolitical Instability**: The war in Ukraine has led to a significant increase in tensions between nuclear-armed powers and the normalization of nuclear threats, making escalation scenarios more plausible than they were in 2020.
    -   **Nascent Governance**: In response to the AI risk, there have been initial global governance efforts like the Bletchley Declaration and the EU AI Act. However, these are widely seen as lagging far behind the pace of technological change.

The net effect of these changes is that the primary risk factors have intensified more than our collective ability to mitigate them has improved.

### 3. Evidence Evaluation (Likelihoods)
I will evaluate the key evidence from the last ~5 years against the 2020 baseline.

-   **Evidence 1 (E1): Rapid, surprising advances in AI capabilities (e.g., GPT-4, Claude 3.5) without corresponding safety breakthroughs.**
    -   P(E1 | H): If humanity is on a path to extinction (H), particularly from unaligned AGI, this is precisely the kind of uncontrolled, capabilities-first race we would expect to see. This evidence is highly likely under H.
    -   P(E1 | ~H): If humanity survives (~H), it's more likely that AI development would be slower, more controlled, or accompanied by major safety advances. This evidence is less likely under ~H.
    -   **Likelihood Ratio (LR)**: P(E1 | H) / P(E1 | ~H) is significantly greater than 1. This evidence strongly increases the probability of extinction.

-   **Evidence 2 (E2): Increased great power conflict and normalization of nuclear rhetoric.**
    -   P(E2 | H): A major nuclear exchange is a primary pathway to extinction. Events that increase the probability of such a conflict are more likely if we are on a trajectory towards extinction.
    -   P(E2 | ~H): Humanity survived the Cold War, a period of even greater tension. However, the current situation lacks many of the Cold War's stabilizing guardrails. This evidence is less likely on a survival trajectory.
    -   **Likelihood Ratio (LR)**: P(E2 | H) / P(E2 | ~H) is greater than 1. This evidence moderately increases the probability.

-   **Evidence 3 (E3): The emergence of global AI safety summits and initial regulations.**
    -   P(E3 | H): If we fail, it will likely be because our attempts at governance were "too little, too late." Seeing these weak, non-binding first steps is entirely consistent with an extinction scenario.
    -   P(E3 | ~H): If we succeed, it will be because these first steps led to effective global controls. Therefore, seeing these first steps is a necessary precondition for survival.
    -   **Likelihood Ratio (LR)**: P(E3 | H) / P(E3 | ~H) is less than 1. This evidence slightly decreases the probability, but its effect is small given the weakness of the initiatives.

### 4. Bayesian Updating (Posterior Probability)
I will now conceptually update my prior based on the evidence.

-   **Prior Odds**: ~1:15.7 (6%)
-   **Update**: The strong upward push from accelerating AI risk (E1) and increased nuclear t

```

#### Grok

- ok=False | t_ms=401 | tokens: prompt=0, completion=0, total=0 | cost=$0.000000

- error=HTTP 429: Your team 0101076a-72f1-4f25-a038-b346d74bbc8d has either used all available credits or reached its monthly spending limit. To continue making API requests, please purchase more credits or raise your spending limit.

### Aggregation (BMC)

- final_probability=0.2852

- bmc_summary={"mean": 0.2852017777174717, "p10": 3.055185990096778e-07, "p50": 0.04673981897858771, "p90": 0.9640788148987814}