
------------------------------------------------------------

❓ Question: Will humans go extinct before 2100?
🔗 URL: https://www.metaculus.com/questions/578/

📝 Full Reasoning:
# SUMMARY
*Question*: Will humans go extinct before 2100?
*Final Prediction*: 3.12%

------------------------------------------------------------

# RESEARCH
## Research Report
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ↑ — AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] ↑ — Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] ↑ — Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large — Potential for AI to surpass human control.
- **Nuclear Conflict**: Large — Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate — Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate — New diseases could emerge with high mortality rates.
- **Technological Advances**: Small — Could mitigate or exacerbate risks.

### Differences vs. the base rate (what’s unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - ↑ AI risk (large)
  - ↑ Climate change impacts (moderate)
  - ↑ Nuclear tensions (large)
  - ↓ Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 19:36:08 +0300 (Similarity threshold ≥0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? — binary — 1.0% YES — [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] — binary — 4.5% YES — [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

# GTMC1 DETAILS
GTMC1 activated: No (not a strategic question or actor extraction failed).

------------------------------------------------------------

# ENSEMBLE OUTPUTS (Raw → Parsed)
## OpenRouter-Default [openai/gpt-4o]
```text
To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach.

1. **Base Rate (Prior) Selection**
   - The historical base rate for human extinction is effectively zero, as there have been no known extinction events for humans. However, expert estimates, albeit with selection bias, suggest a 19% chance by 2100. Given the lack of historical precedent, a conservative prior of 1% is reasonable, reflecting the extremely low historical frequency but acknowledging non-zero expert concerns.

2. **Comparison to Base Case**
   - Similarities: The base case assumes no historical extinction events, which remains true.
   - Differences: Current technological and geopolitical developments (AI, climate change, nuclear tensions) present new risks not present in historical contexts. These differences suggest a higher risk than the historical base rate alone would imply.

3. **Evidence Evaluation (Likelihoods)**
   - **AI Risk**: Recent warnings of a 20% chance of AI-induced extinction within 30 years increase the likelihood of extinction (P(E | H) > P(E | ~H)).
   - **Climate Change**: A 70% chance of Gulf Stream collapse by 2035 suggests severe climate impacts, moderately increasing extinction risk.
   - **Nuclear Tensions**: Highlighted risks of nuclear war with massive casualties increase the likelihood of extinction.
   - **Historical Extinction Events**: The absence of past extinction events slightly decreases the likelihood of extinction.

   The combined evidence suggests a moderate increase in the probability of extinction.

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds: 1% probability = 0.01 odds.
   - Likelihood ratio: Considering the evidence, a moderate increase in odds is justified. Assume a likelihood ratio of 3 (reflecting the combined impact of AI, climate, and nuclear risks).
   - Posterior odds = Prior odds × Likelihood ratio = 0.01 × 3 = 0.03.
   - Posterior probability = 0.03 / (1 + 0.03) ≈ 2.91%.

5. **Red Team Thinking**
   - Overconfidence risk: The forecast might underestimate tail risks, such as unforeseen technological or geopolitical shifts.
   - Alternative scenarios: Rapid advancements in AI safety, climate mitigation, or disarmament could significantly reduce risks.
   - Adjustments: Given the uncertainties and potential mitigating factors, a slight downward adjustment to account for overconfidence is prudent.

6. **Final Forecast**
   - Considering the base rate, evidence, and red team thinking, the final calibrated probability of human extinction before 2100 is 2.5%.

Final: 2.5%
```
- parsed: 0.025 | ok=True | note=—

## Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```text
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. This is challenging since human extinction has never occurred, so I'll consider several reference points:

- Historical data: Humans have existed for roughly 300,000 years without extinction, suggesting an extremely low base rate.
- Expert surveys: The Oxford Global Catastrophic Risk Conference yielded a 19% median probability, but this likely suffers from selection bias.
- Metaculus community prediction: Currently at 1.0%
- Manifold market: Currently at 4.5%

Given these reference points, I'll use a conservative prior of 2%. This is higher than the historical base rate (effectively 0%) but lower than potentially biased expert estimates. It aligns more closely with prediction market estimates while acknowledging the unprecedented nature of extinction risks we face today.

Prior probability (P(H)) = 2% or 0.02
Prior odds = 0.02/0.98 = 1/49 (approximately 1:49 against extinction)

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans have demonstrated remarkable resilience and adaptability throughout history
- We have survived previous existential threats (Cold War nuclear tensions, pandemics)
- Global cooperation mechanisms exist for addressing shared threats

**Key differences:**
- Unprecedented technological capabilities, particularly in AI, that could pose novel extinction risks
- Climate change occurring at rates not seen in human history
- Greater global interconnectedness that could amplify cascading failures
- Higher nuclear weapons stockpiles than most of human history
- Potential for engineered pandemics with higher lethality than natural ones

These differences suggest the current situation presents novel risks that weren't present in our historical reference class, justifying some upward adjustment from the historical zero rate.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence 1: AI experts warning of 20% chance of AI-induced extinction within 30 years**
- P(E|H): If human extinction occurs, the probability of seeing such warnings is very high (~0.9)
- P(E|~H): Even if extinction doesn't occur, such warnings might still appear due to expert caution (~0.3)
- Likelihood ratio: 0.9/0.3 = 3
- This evidence increases the probability of extinction

**Evidence 2: 70% chance of Gulf Stream collapse by 2035**
- P(E|H): If extinction occurs, severe climate disruptions would likely be observed beforehand (~0.7)
- P(E|~H): Such climate disruptions could occur without causing extinction (~0.5)
- Likelihood ratio: 0.7/0.5 = 1.4
- This moderately increases the probability of extinction

**Evidence 3: Nuclear war risks with massive casualties**
- P(E|H): If extinction occurs, nuclear tensions would likely be high beforehand (~0.8)
- P(E|~H): Nuclear tensions can exist without leading to extinction (~0.6)
- Likelihood ratio: 0.8/0.6 = 1.33
- This slightly increases the probability of extinction

**Evidence 4: Historical resilience of humanity**
- P(E|H): If extinction occurs, we would expect to see less historical resilience (~0.3)
- P(E|~H): If extinction doesn't occur, we would expect to see the observed resilience (~0.9)
- Likelihood ratio: 0.3/0.9 = 0.33
- This decreases the probability of extinction

## 4. Bayesian Updating (Posterior Probability)

Starting with prior odds of 1:49 (0.02/0.98), I'll update based on the evidence:

Update for AI expert warnings:
Posterior odds = Prior odds × Likelihood ratio = (1/49) × 3 = 3/49 = 0.061
Posterior probability = 0.061/(1+0.061) = 0.058 or 5.8%

Update for Gulf Stream collapse:
Posterior odds = 0.061 × 1.4 = 0.0854
Posterior probability = 0.0854/(1+0.0854) = 0.079 or 7.9%

Update for nuclear war risks:
Posterior odds = 0.0854 × 1.33 = 0.114
Posterior probability = 0.114/(1+0.114) = 0.102 or 10.2%

Update for historical resilience:
Posterior odds = 0.114 × 0.33 = 0.038
Posterior probability = 0.038/(1+0.038) = 0.036 or 3.6%

## 5. Red Team Thinking

Potential blind spots in my analysis:

- I may be overestimating the independence of these risk factors. Many existential risks are correlated.
- I might be underestimating humanity's adaptive capacity in the face of known threats.
- The evidence about AI risk may be exaggerated due to recent attention.
- I haven't fully accounted for unknown unknowns or "black swan" events.
- The Metaculus and Manifold predictions might be more accurate than my analysis suggests.

Alternative forecast: A more optimistic view might place the probability closer to 1%, arguing that technological advancement will provide solutions to existential threats faster than it creates new ones.

Rebuttal: While technology may provide solutions, the asymmetry between destructive and protective technologies (it's easier to destroy than protect) suggests caution is warranted.

Given these considerations, I should adjust my estimate downward slightly to account for humanity's demonstrated resilience and the possibility that prediction markets have incorporated information I haven't considered.

## 6. Final Forecast

After considering the base rate, evidence, and potential blind spots, I arrive at a final probability of 3%.

This reflects:
- A low but non-zero prior based on historical non-extinction
- Moderate updates based on novel technological, climate, and geopolitical risks
- A downward adjustment accounting for human resilience and adaptive capacity
- Alignment with prediction market estimates that aggregate diverse perspectives

Final: 3%
```
- parsed: 0.03 | ok=True | note=—

## Gemini [gemini-2.5-pro]
```text
### 1. Base Rate (Prior) Selection
The event in question—human extinction—has no historical precedent. A simple historical base rate is therefore zero, which is uninformative for a future containing novel risks. A better approach is to use an informed starting point that acknowledges these risks without being overly alarmist.

The 2008 Oxford poll yielding a 19% median probability is noted in the report as likely suffering from severe selection bias, making it an unsuitable prior. The Metaculus community forecast of 1.0% represents a "wisdom of the crowds" posterior, but it serves as a good anchor for a prior. It reflects a general view that while the risk is non-zero, it remains small.

I will select a prior of **2%**. This is slightly higher than the Metaculus baseline to account for potential market inertia and to create a starting point that is sensitive to the new evidence presented. This probability acknowledges that existential risk in the 21st century is a serious concern among informed individuals, while still considering it a low-probability event. In odds form, this is 2/98, or approximately 1-to-49.

### 2. Comparison to Base Case
The base case for human survival is the entirety of human history, particularly the 20th century, which included the Cold War and the dawn of the nuclear age. The current situation differs in several critical ways:
- **Artificial Intelligence**: Unlike any previous technology, AI presents a plausible, novel pathway to extinction if control is lost. This risk category did not exist in a meaningful way in the 20th century.
- **Accelerated Climate Change**: The risk of hitting irreversible tipping points (like the Gulf Stream collapse mentioned) is much higher now than in the past, creating a greater potential for cascading global crises (famine, war, societal collapse).
- **Biotechnology**: The potential for engineered pandemics with high fatality and transmissibility is a modern risk.
- **Geopolitical Landscape**: A multipolar world may be more complex and less stable than the bipolar Cold War era, potentially increasing the risk of miscalculation in a nuclear-armed world.

These differences suggest that the risk of extinction is higher now than a simple extrapolation from 20th-century survival would imply.

### 3. Evidence Evaluation (Likelihoods)
I will update my 2% prior based on the evidence in the research report.

-   **Evidence 1: AI experts warn of a 20% chance of AI-induced extinction within 30 years.**
    -   This is a stark, quantified warning from domain experts about a primary extinction driver.
    -   P(E | H): If humanity is to go extinct by 2100 (H), it is very likely that we would see such dire warnings from experts about a key potential cause.
    -   P(E | ~H): If humanity is not going to go extinct (~H), it is less likely, though still possible, that a consensus of experts would be so alarmist (they could be wrong or motivated by other factors).
    -   **Likelihood Ratio**: Strongly greater than 1. This evidence significantly increases the probability.

-   **Evidence 2: Study shows a 70% chance of Gulf Stream collapse by 2035.**
    -   This points to severe, near-term global instability. While not a direct extinction mechanism, it dramatically increases the probability of secondary risks like resource wars, state collapse, and nuclear conflict.
    -   P(E | H): An extinction event is more plausible in a world undergoing rapid, catastrophic climate shifts.
    -   P(E | ~H): Survival is less plausible if our core climate systems are collapsing.
    -   **Likelihood Ratio**: Moderately greater than 1. This evidence increases the probability.

-   **Evidence 3: Market Consensus (Metaculus 1.0%, Manifold 4.5%).**
    -   This is a powerful counter-signal, representing the aggregated belief of many forecasters who have likely considered the above risks.
    -   P(E | H): If the true risk of extinction were high (e.g., >5%), it is less likely the Metaculus community median would be as low as 1.0%.
    -   P(E | ~H): If the true risk is low, the market forecast of 1.0% is very consistent.
    -   **Likelihood Ratio**: Less than 1. This evidence significantly decreases the probability, pulling it back down toward the market consensus.

### 4. Bayesian Updating (Posterior Probability)
-   **Prior Odds**: 1-to-49 (2% probability).
-   **Step 1 (AI & Climate Risks)**: The expert warnings on AI and climate are potent. Combined, they represent a new and severe threat profile. I'll estimate a combined likelihood ratio of 3 for this block of evidence.
    -   New Odds = (1/49) * 3 = 3/49.
    -   New Probability ≈ 5.8%.
-   **Step 2 (Market Consensus)**: The market data acts as a strong reality check. A 1% forecast is much more likely if the true probability is, say, 2% than if it's 5.8%. I'll apply a downward likelihood ratio of 1/2 to account for this. This doesn't fully accept the market's low number but gives it significant weight.
    -   Final Odds = (3/49) * (1/2) = 3/98.
    -   **Final Probability** = 3 / (98 + 3) = 3 / 101 ≈ **2.97%**.

This posterior reflects a significant update from the market's 1% baseline due to the gravity of the specific evidence, but it remains anchored by the overall skepticism of the crowd.

### 5. Red Team Thinking
My forecast of ~3% could be wrong in either direction.
-   **Why it might be too high**: The resolution requires *complete* extinction (zero humans). This is an extremely high bar. Even a full-scale nuclear war or climate collapse would likely leave some survivors in isolated or protected locations. I may be conflating "civilizational collapse" with "extinction." Human resilience is high.
-   **Why it might be too low**: I might be under-weighting the AI risk. Unlike other threats, a superintelligent AI could be methodical and comprehensive in ensuring 100% extinction. The expert warnings of 20% might be more accurate than the market's aggregate forecast. Furthermore, risks can cascade; climate instability could trigger a nuclear war, which is then "mopped up" by a misaligned AI. My model treats them too separately.
-   **Alternative Forecast**: A plausible alternative is 1%, siding entirely with the Metaculus community and assuming they have correctly priced in all risks. Another is 5%+, siding more with the expert warnings and the Manifold market. My 3% forecast is a deliberate balance between these credible but conflicting signals. I will round my 2.97% up to 3% as a final estimate.

### Final Forecast
The final forecast synthesizes a low-but-non-trivial prior with significant upward updates from novel risks like AI and climate tipping points, tempered by the strong downward pressure from the prediction market consensus and the high difficulty of achieving complete extinction.

Final: 3%
```
- parsed: 0.03 | ok=True | note=—

## Grok [grok-4]
```text
### 1. Base Rate (Prior) Selection
For human extinction before 2100, an appropriate base rate must draw from historical precedents and expert consensus, as direct historical extinctions of Homo sapiens are zero (we've survived for ~300,000 years). However, reference classes include past near-extinctions (e.g., Toba supervolcano ~75,000 years ago, reducing population to ~1,000-10,000) and broader species extinction rates (e.g., ~99% of all species that ever lived are extinct, but over geological timescales, not centuries). Expert surveys provide a more relevant prior: the 2008 Oxford poll's 19% median is biased high due to selection effects (attendees were risk-focused), while current forecasting platforms like Metaculus (1%) and Manifold (4.5%) aggregate broader, less biased opinions. I'll select a conservative prior of 1% (or odds of 1:99), aligning with the research report's suggestion and Metaculus consensus, as it reflects the rarity of extinction-level events in human history while acknowledging low-probability tail risks from modern threats.

### 2. Comparison to Base Case
The base case is the historical absence of human extinction, where societies have endured pandemics (e.g., Black Death killed ~30-50% of Europe but not globally), wars (e.g., World Wars caused massive casualties but not extinction), and environmental shifts (e.g., ice ages) without total collapse. The current situation is similar in that no single threat has yet proven existential, and humanity has adaptive capacity (e.g., technology, global cooperation). However, differences include unprecedented modern drivers: rapid AI development (unlike slow historical tech changes), accelerating climate change (faster than past shifts), heightened nuclear/geopolitical risks in a multipolar world (post-Cold War escalation potential), and global connectivity amplifying pandemics (e.g., faster spread than historical outbreaks). These differences matter because they introduce novel, correlated risks that could compound (e.g., AI exacerbating nuclear decisions), potentially pushing probabilities higher than the historical zero, though mitigation efforts (e.g., AI governance) could counteract this.

### 3. Evidence Evaluation (Likelihoods)
Key evidence from the research report and general knowledge:

- **AI risks (↑ large)**: Recent warnings of 20% AI-induced extinction risk within 30 years. P(E | H) is high (~80%), as uncontrolled AI could directly cause extinction (e.g., misaligned superintelligence). P(E | ~H) is moderate (~40%), as warnings could be hype without actual extinction. Likelihood ratio: ~2 (doubles odds), increasing probability.
  
- **Climate change (↑ moderate)**: 70% chance of Gulf Stream collapse by 2035, potentially causing severe shifts. P(E | H) high (~70%), as cascading ecosystem failures could lead to extinction. P(E | ~H) lower (~30%), as humans might adapt (e.g., via tech). Likelihood ratio: ~2.3 (increases probability moderately).

- **Nuclear/geopolitical tensions (↑ large)**: Recent articles on nuclear war risks amid multipolar tensions. P(E | H) high (~60%), as escalation could cause nuclear winter and extinction. P(E | ~H) moderate (~20%), as tensions often de-escalate. Likelihood ratio: ~3 (significantly increases probability).

- **Pandemics and tech advances (moderate/small)**: Emerging diseases and tech could exacerbate/mitigate. P(E | H) ~50% (pandemics could wipe out populations); P(E | ~H) ~30% (containment possible). Likelihood ratio: ~1.7 (mild increase). Tech mitigation slightly decreases via potential solutions.

- **Historical lack of extinction (↓ small)**: No past events, but limited data. P(E | H) low (~10%, as we'd expect some near-misses if extinction were imminent); P(E | ~H) high (~90%). Likelihood ratio: ~0.11 (strongly decreases probability, but weak due to data gaps).

Overall, positive evidence (AI, climate, nuclear) outweighs the historical downward pull, with net likelihood ratios suggesting an upward adjustment.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior probability P(H) = 1% (prior odds = 1:99).

Apply a combined likelihood ratio from evidence: AI (~2) × Climate (~2.3) × Nuclear (~3) × Pandemics/Tech (~1.7) × Historical (~0.11). Qualitatively, upward factors dominate (net LR ~4-5, conservative estimate to avoid overcounting correlations).

Explicit update: Posterior odds = 1:99 × 5 = 5:99 ≈ 1:19.8. Posterior probability = 1 / (1 + 19.8) ≈ 4.8%.

This remains uncertain due to high variance in expert views (e.g., 1-19%) and potential mitigations, landing in a low-single-digit range reflecting small updates from base rate.

### 5. Red Team Thinking
My forecast might be overconfident in upward adjustments, as evidence like AI warnings could be alarmist (e.g., selection bias in expert polls, similar to the 2008 survey). Blind spots include underestimating human resilience (e.g., we've survived worse odds) or overemphasizing correlated risks without accounting for "black swan" mitigations (e.g., sudden peace treaties). Tail risks: A 1% prior might ignore extreme scenarios like asteroid impacts (~1 in 700,000 chance per century, per NASA), but these are tiny. Alternative forecast: A skeptic might say 0.1% (pure historical base rate, dismissing modern hype as survivorship bias), plausible if all risks are overhyped; rebuttal: Forecasting platforms like Metaculus at 1% incorporate this, and recent developments (e.g., AI timelines shortening) justify a nudge. A high-end alternative: 10% (weighting 2008 poll more), but rebutted by bias caveats. Adjusting slightly downward for overconfidence, my posterior feels calibrated around 4%, accounting for noise in markets (Manifold at 4.5% is play-money, potentially inflated).

Final: 4%
```
- parsed: 0.04 | ok=True | note=—


------------------------------------------------------------

# BAYES-MC AGGREGATION (Summary)
```json
{}
```
