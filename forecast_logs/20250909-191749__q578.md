
------------------------------------------------------------

❓ Question: Will humans go extinct before 2100?
🔗 URL: https://www.metaculus.com/questions/578/

📝 Full Reasoning:
# SUMMARY
*Question*: Will humans go extinct before 2100?
*Final Prediction*: 5.75%

------------------------------------------------------------

# RESEARCH
## Research Report
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ↑ — AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] ↑ — Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] ↑ — Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large — Potential for AI to surpass human control.
- **Nuclear Conflict**: Large — Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate — Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate — New diseases could emerge with high mortality rates.
- **Technological Advances**: Small — Could mitigate or exacerbate risks.

### Differences vs. the base rate (what’s unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - ↑ AI risk (large)
  - ↑ Climate change impacts (moderate)
  - ↑ Nuclear tensions (large)
  - ↓ Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 19:17:49 +0300 (Similarity threshold ≥0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? — binary — 1.0% YES — [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] — binary — 4.5% YES — [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

# GTMC1 DETAILS
GTMC1 activated: No (not a strategic question or actor extraction failed).

------------------------------------------------------------

# ENSEMBLE OUTPUTS (Raw → Parsed)
## OpenRouter-Default [openai/gpt-4o]
```text
To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach.

### 1. Base Rate (Prior) Selection
- **Base Rate**: Historically, the base rate for human extinction events is effectively zero, as humans have not gone extinct in recorded history. However, expert estimates, albeit with selection bias, suggest a 19% chance by 2100. Given the historical absence of extinction events, a more conservative prior of 1% is reasonable, reflecting the low historical frequency but acknowledging expert concerns.
- **Initial Prior**: 1% probability (0.01 odds).

### 2. Comparison to Base Case
- **Similarities**: The current situation shares the historical absence of extinction events.
- **Differences**: Recent developments in AI, climate change, and geopolitical tensions introduce new risks not present in historical contexts. These differences suggest a need to adjust the prior probability upwards.

### 3. Evidence Evaluation (Likelihoods)
- **AI Risk**: AI experts warn of a 20% chance of AI-induced extinction within 30 years. This is a significant risk factor, increasing the likelihood of extinction.
  - **Likelihood Ratio**: High (P(E | H) > P(E | ~H)).
- **Climate Change**: A 70% chance of Gulf Stream collapse by 2035 could lead to severe climate shifts, increasing extinction risk.
  - **Likelihood Ratio**: Moderate (P(E | H) > P(E | ~H)).
- **Nuclear Conflict**: Increased geopolitical tensions raise the risk of nuclear war, a potential extinction-level event.
  - **Likelihood Ratio**: High (P(E | H) > P(E | ~H)).
- **Historical Extinction Events**: The absence of past extinction events slightly decreases the probability.
  - **Likelihood Ratio**: Low (P(E | H) < P(E | ~H)).

### 4. Bayesian Updating (Posterior Probability)
- **Prior Odds**: 0.01
- **Net Likelihood Ratio**: Considering the evidence, the likelihood ratio is significantly greater than 1, suggesting an increase in probability.
- **Posterior Odds Calculation**: 
  - Assume a combined likelihood ratio of 5 (reflecting the significant increase in risk from AI and nuclear conflict).
  - Posterior Odds = Prior Odds × Likelihood Ratio = 0.01 × 5 = 0.05
  - Posterior Probability = 0.05 / (1 + 0.05) ≈ 4.76%

### 5. Red Team Thinking
- **Overconfidence Check**: The forecast might overestimate risks due to recent alarming developments. Historical resilience and potential mitigating factors (e.g., international cooperation) could reduce risks.
- **Alternative Scenarios**: Consider breakthroughs in AI safety, climate mitigation, and disarmament treaties, which could lower extinction risks.
- **Adjustment**: Given these considerations, a slight downward adjustment is warranted to account for potential mitigating factors.

### 6. Final Forecast
- **Final Probability**: After considering the base rate, evidence, and red team thinking, the final probability of human extinction before 2100 is adjusted to reflect both the increased risks and potential mitigating factors.

Final: 3%
```
- parsed: 0.03 | ok=True | note=—

## Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```text
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. This is challenging since human extinction has never occurred, so I'll consider several reference points:

- Historical data: Humans have existed for roughly 300,000 years with no extinction events (0% historical rate)
- Metaculus community forecast: 1.0%
- Manifold market: 4.5%
- Oxford Global Catastrophic Risk Conference: 19% (likely selection-biased)
- Background extinction rate for mammalian species: ~0.1-1% per century

Given these reference points, I'll select a base rate of 1% as my prior, which aligns with the Metaculus community prediction and falls within reasonable bounds for a low-probability, high-impact event. This acknowledges both our species' historical resilience and the genuine existential risks we face.

Prior probability: P(H) = 1% or 1:99 odds

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans remain highly adaptable with global presence
- We've survived previous existential threats (Cold War, pandemics)
- Technological advancement continues to improve our resilience

**Differences from base case:**
- Unprecedented AI development with potential for unaligned superintelligence
- Accelerating climate change with potential tipping points (Gulf Stream collapse)
- Heightened nuclear tensions in a multipolar world
- Increased global connectivity amplifying pandemic risks
- Technological capabilities that could both mitigate and create novel risks

These differences suggest the current situation may present higher extinction risk than historical periods, warranting careful consideration of new evidence.

## 3. Evidence Evaluation (Likelihoods)

**Evidence 1: AI experts warning of 20% chance of AI-induced extinction within 30 years**
- P(E|H): If human extinction occurs by 2100, how likely would we see AI experts warning about it? Very likely (~80%)
- P(E|~H): If human extinction doesn't occur, how likely would we see these warnings? Moderately likely (~20%) as experts may be overly cautious
- Likelihood ratio: P(E|H)/P(E|~H) = 80%/20% = 4
- This evidence increases the probability of extinction

**Evidence 2: 70% chance of Gulf Stream collapse by 2035**
- P(E|H): If extinction occurs, how likely would we see major climate system collapses? Fairly likely (~60%)
- P(E|~H): If extinction doesn't occur, how likely would we see these climate warnings? Moderately likely (~30%) as climate models may overestimate immediate impacts
- Likelihood ratio: P(E|H)/P(E|~H) = 60%/30% = 2
- This evidence moderately increases the probability of extinction

**Evidence 3: Increased nuclear war risks**
- P(E|H): If extinction occurs, how likely would we see heightened nuclear tensions? Very likely (~70%)
- P(E|~H): If extinction doesn't occur, how likely would we see these tensions? Somewhat likely (~35%) as nuclear tensions have fluctuated historically without leading to extinction
- Likelihood ratio: P(E|H)/P(E|~H) = 70%/35% = 2
- This evidence moderately increases the probability of extinction

## 4. Bayesian Updating (Posterior Probability)

Starting with prior odds of 1:99 (1%), I'll update based on the evidence:

**Update on AI expert warnings:**
- Prior odds: 1:99
- Likelihood ratio: 4
- Posterior odds: 1:99 × 4 = 4:99 ≈ 1:25
- Posterior probability: 4/(4+99) ≈ 3.9%

**Update on Gulf Stream collapse:**
- Prior odds: 1:25
- Likelihood ratio: 2
- Posterior odds: 1:25 × 2 = 2:25 = 1:12.5
- Posterior probability: 1/(1+12.5) ≈ 7.4%

**Update on nuclear war risks:**
- Prior odds: 1:12.5
- Likelihood ratio: 2
- Posterior odds: 1:12.5 × 2 = 2:12.5 = 1:6.25
- Posterior probability: 1/(1+6.25) ≈ 13.8%

After incorporating all evidence, the posterior probability is approximately 13.8%.

## 5. Red Team Thinking

**Potential overconfidence:**
- I may be overweighting recent alarming news over long-term human adaptability
- The evidence pieces may not be independent, causing double-counting
- Expert warnings often reflect selection bias toward catastrophic scenarios
- Historical resilience suggests humans are extremely adaptable

**Alternative scenarios:**
- Technological solutions may emerge faster than catastrophic risks materialize
- International cooperation could increase in response to existential threats
- The likelihood ratios I assigned may be too high given uncertainty

**Best alternative forecast:**
A more conservative estimate might be 5-7%, arguing that while risks exist, human ingenuity and self-preservation instincts will likely prevail as they have historically. The counterargument is that past performance doesn't guarantee future results when facing novel threats like superintelligent AI.

Given these considerations, I should adjust my estimate downward somewhat, as the market consensus and historical resilience suggest I may be overestimating the risk.

## 6. Final Forecast

After considering the base rate, evidence, and potential biases, I believe the probability of human extinction before 2100 is approximately 8%. This reflects significant existential risks from AI, climate change, and nuclear conflict, while acknowledging humanity's historical resilience and potential for technological solutions.

Final: 8%
```
- parsed: 0.08 | ok=True | note=—

## Gemini [gemini-2.5-pro]
```text
Here is my probabilistic forecast reasoning:

### 1. Base Rate (Prior) Selection
The appropriate base rate for human extinction is not the historical frequency (which is zero), as we have entered the "Anthropocene," an era defined by human-driven planetary change and unprecedented technological power. A better starting point is an informed prior that accounts for these new anthropogenic risks.

- The 19% median from the 2008 Global Catastrophic Risk Conference is noted but heavily discounted due to extreme selection bias.
- The historical rate of natural extinction for a species like ours is extremely low (e.g., <0.01% per century) and is not a useful prior for an age of nuclear weapons and AI.
- The Metaculus community forecast of 1.0% represents a well-calibrated "wisdom of the crowds" estimate, aggregating information from many informed individuals. This is a strong candidate for a prior.

I will select a prior of **2%**. This is slightly higher than the Metaculus baseline to account for potential inertia in community forecasts and to lean slightly towards the higher-risk assessment suggested by domain experts, without fully succumbing to their potential biases. In odds, this is 2/98, or 1:49.

### 2. Comparison to Base Case
The base case is the late 20th century, the only other period where anthropogenic extinction was plausible (due to nuclear weapons). The current situation is significantly different and arguably more dangerous:

- **New Risk Factors**: Advanced AI and biotechnology (e.g., engineered pandemics) are entirely new potential existential threats that did not exist in the 20th century.
- **Intensified Existing Risks**: Climate change has moved from a future concern to a present reality with accelerating, non-linear effects and potential tipping points (like the Gulf Stream collapse).
- **Geopolitical Instability**: The bipolar stability of the Cold War, governed by Mutually Assured Destruction, has been replaced by a more complex and less predictable multipolar world, potentially increasing the risk of nuclear miscalculation.

These differences all point towards a higher probability of extinction than in the 20th-century base case.

### 3. Evidence Evaluation (Likelihoods)
I will update my 2% prior based on the new evidence provided in the research report.

- **Evidence 1: AI experts warn of a 20% chance of AI-induced extinction within 30 years.**
  - This is a very strong piece of evidence. Let H be "human extinction by 2100" and E be this expert warning.
  - P(E | H): If humanity is to go extinct, it's highly plausible that an uncontrolled AI would be the cause, and that experts would be sounding the alarm beforehand. This probability is high.
  - P(E | ~H): If humanity is not going to go extinct, it's less likely (but still possible) that a large consensus of experts would issue such a dire and specific warning. They could be wrong or overly alarmist.
  - **Likelihood Ratio (P(E|H)/P(E|~H))**: This is significantly greater than 1. This evidence strongly increases the probability.

- **Evidence 2: Study shows a 70% chance of Gulf Stream collapse by 2035.**
  - This is a major risk multiplier. A collapse would not directly cause extinction but would trigger catastrophic climate shifts, famine, and resource conflicts, which could lead to global societal collapse and wars fought with weapons of mass destruction.
  - **Likelihood Ratio**: This is also greater than 1. The discovery of a plausible, near-term mechanism for global destabilization makes the extinction outcome more likely. This evidence moderately increases the probability.

- **Evidence 3: Market Consensus (Metaculus 1.0%, Manifold 4.5%).**
  - This is not new evidence to update upon so much as a strong anchor for my prior and final forecast. It suggests that a final probability in the low single digits is considered reasonable by an aggregated group of forecasters. My initial prior of 2% is well-aligned with this. The Manifold forecast of 4.5% suggests that updates in response to recent news are pushing the consensus higher.

### 4. Bayesian Updating (Posterior Probability)
- **Prior Odds**: 1:49 (2% probability).
- **Update 1 (AI Risk)**: The expert warning is a powerful update. I'll use a conservative likelihood ratio of 1.7 to reflect some skepticism about expert predictions while still acknowledging the gravity of the warning.
  - Posterior Odds = 1:49 * 1.7 ≈ 1:28.8
  - This corresponds to a probability of 1 / (1 + 28.8) ≈ 3.3%.
- **Update 2 (Climate Risk)**: The Gulf Stream news is a further negative update. I'll apply another conservative likelihood ratio of 1.5.
  - Final Odds = 1:28.8 * 1.5 ≈ 1:19.2
  - **Final Probability** = 1 / (1 + 19.2) ≈ 4.95%.

This step-by-step update, using conservative likelihoods, moves the probability from 2% to approximately 5%. This reflects a significant increase in perceived risk due to the new information.

### 5. Red Team Thinking
- **Argument for a Lower Forecast (<5%)**: I might be overreacting to recent, alarming news. The Metaculus community is at 1%, and they are a formidable benchmark. Humanity has proven remarkably resilient, and technological solutions to these problems may emerge. Extinction is an extremely high bar to clear; a catastrophe that kills 99.9% of people is not extinction.
- **Argument for a Higher Forecast (>5%)**: My updates might be too conservative. The risks from AI, climate, and geopolitics are correlated and could create a cascading failure that is worse than the sum of its parts. The expert poll of 19%, while biased, shouldn't be dismissed entirely. Perhaps the true risk lies in the uncomfortable middle ground between the public's optimism and the experts' pessimism.
- **Conclusion**: My 5% forecast sits between the two market indicators (Metaculus 1%, Manifold 4.5%). It acknowledges the gravity of the new evidence without swinging to the high double-digits suggested by the biased expert poll. It represents a belief that the risk is significantly higher than the Metaculus baseline, primarily due to the recent, specific, and credible warnings about AI and climate tipping points.

### Final Forecast
After weighing the base rate, the specific evidence, and potential biases, my final assessment is that the recent developments have materially increased the near-term existential risk.

Final: 5%
```
- parsed: 0.05 | ok=True | note=—

## Grok [grok-4]
```text
### 1. Base Rate (Prior) Selection
For human extinction before 2100, an appropriate base rate must account for the fact that humanity has never gone extinct in recorded history (over ~300,000 years of Homo sapiens existence), making the historical frequency effectively zero. However, this is a poor reference class because modern existential risks (e.g., nuclear weapons, AI, engineered pandemics) did not exist historically. A better reference class is expert elicitations on global catastrophic risks, adjusted for biases. The 2008 Oxford poll gave a median of 19%, but as noted, it's likely inflated due to selection bias toward high-risk perceivers. More reliable are aggregated prediction markets and recent expert syntheses: Metaculus community forecast is 1%, Manifold is 4.5%, and Toby Ord's "The Precipice" (2020) estimates ~16.7% (1 in 6) for extinction by 2100 from all causes. Balancing these, I'll select a prior of 5% (or odds of 1:19), as it reflects a low historical base rate nudged upward by expert consensus on emerging risks, without over-relying on the biased 19% poll.

### 2. Comparison to Base Case
The base case is the historical human survival rate through various crises (e.g., pandemics like the Black Death, which killed ~30-50% of Europe's population but not all humans; or the Cold War nuclear standoff, which peaked in tension but avoided Armageddon). Similarities include humanity's resilience—technological adaptation, global cooperation (e.g., treaties like the Nuclear Non-Proliferation Treaty), and the fact that no single event has ever wiped out all humans. Differences are stark and increase risk: Today's situation involves novel, high-leverage threats like superintelligent AI (absent historically), accelerating climate change (faster than past shifts), and engineered pandemics enabled by biotech (unlike natural ones). Geopolitical multipolarity (e.g., US-China-Russia tensions) differs from bipolar Cold War dynamics, potentially raising escalation risks. These differences matter because they introduce "fat-tail" uncertainties—low-probability, high-impact events that could cascade globally in a hyper-connected world, unlike isolated historical threats.

### 3. Evidence Evaluation (Likelihoods)
I'll evaluate key pieces from the research report, considering P(E | H) (evidence given extinction occurs) vs. P(E | ~H) (given it doesn't), and the likelihood ratio (LR = P(E | H) / P(E | ~H)). Higher LR (>1) increases probability; lower (<1) decreases it.

- **AI risks (e.g., 2025 warning of 20% AI-induced extinction in 30 years)**: P(E | H) is high (~80%) because uncontrolled AI could directly cause extinction (e.g., via misaligned goals). P(E | ~H) is moderate (~40%) as AI warnings occur even without extinction (hype or cautionary tales). LR ≈ 2 (increases probability moderately, as it's a large driver but speculative).
  
- **Climate change (e.g., 70% chance of Gulf Stream collapse by 2035)**: P(E | H) is moderate (~60%) since severe climate shifts could lead to societal collapse and extinction via famine/war. P(E | ~H) is also moderate (~50%) as such predictions are common in non-extinction scenarios (e.g., adaptation occurs). LR ≈ 1.2 (slight increase, as it's a moderate driver with high uncertainty and potential mitigation).

- **Nuclear war risks (e.g., 2025 article on geopolitical tensions)**: P(E | H) is high (~70%) as nuclear winter could cause extinction. P(E | ~H) is lower (~30%) since tensions often de-escalate without war. LR ≈ 2.3 (increases probability notably, given large causal leverage and current multipolar risks).

- **Historical lack of extinction events**: P(E | H) is low (~10%) because if extinction were likely, we'd expect precursors; P(E | ~H) is high (~90%) as survival is the norm. LR ≈ 0.11 (decreases probability strongly, anchoring against overreaction to new risks).

- **Other factors (pandemics, global connectivity)**: Net LR ≈ 1.5 (moderate increase, as connectivity amplifies spread but tech advances could counter it).

Overall, evidence leans toward increase (net LR ~2-3 from upward drivers outweighing the historical anchor).

### 4. Bayesian Updating (Posterior Probability)
Starting with prior probability P(H) = 5% (prior odds = 0.05 / 0.95 ≈ 1:19).

Group evidence into a net likelihood ratio. From above, upward evidence (AI, nuclear, climate) gives LR ≈ 2.5, tempered by historical anchor (LR ≈ 0.11), yielding net LR ≈ 1.8 (conservative, avoiding big swings per calibration guidance).

Posterior odds = Prior odds × LR = (1:19) × 1.8 ≈ 1:10.56.

Posterior probability = (1) / (1 + 10.56) ≈ 8.6%.

This remains uncertain due to wide variance in expert estimates (1-19%) and data gaps on tail risks; confidence is moderate, as evidence nudges up from the prior but doesn't overwhelm the low base rate.

### 5. Red Team Thinking
My forecast might be overconfident in the upward nudge—prediction markets (1-4.5%) are lower, suggesting I could be overweighting recent "↑" developments (e.g., AI hype often overstates short-term risks, and Gulf Stream collapse might not lead to extinction). Blind spots include underestimating human adaptability (e.g., geoengineering for climate, AI safety progress). Tail risks: A black-swan event like a supervolcano (low base rate but possible) or cascading failures (AI + nuclear) could push probability higher; conversely, breakthroughs in global governance could drop it to near-zero. Best alternative forecast: 1% (per Metaculus), plausible if viewing recent warnings as noise and historical resilience as dominant—rebuttal: Markets undervalue long-term risks due to short horizons, and Ord's 16.7% counters this conservatism. Adjusting for this, I'll nudge down slightly to account for potential overreaction, broadening uncertainty to avoid narrow confidence.

Final: 7%
```
- parsed: 0.07 | ok=True | note=—


------------------------------------------------------------

# BAYES-MC AGGREGATION (Summary)
```json
{}
```
