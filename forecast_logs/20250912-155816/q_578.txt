# Will humans go extinct before 2100? (QID: 578)

- Type: binary

- URL: https://www.metaculus.com/questions/578/

- Classifier: xrisk | strategic=False (score=0.00)

## Research (summary)

CALIBRATION GUIDANCE (auto-generated weekly):
BINARY CALIBRATION
- No resolved binary questions (with pre-resolution forecasts) yet.
- Advice: keep using base rates + small evidence-weighted updates; avoid big swings.

MULTIPLE-CHOICE CALIBRATION (Top-1)
- No resolved MCQ questions (with pre-resolution forecasts) yet.

NUMERIC CALIBRATION (PIT-lite + CRPS)
- No resolved numeric questions (with pre-resolution forecasts) yet.
- When numeric results exist, we’ll check p10/p50/p90 coverage and CRPS.

General takeaway: apply **small** nudges only where gaps are consistent with decent sample sizes (≥10).
— end calibration —

### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate of human extinction events is effectively zero, but experts estimate a 19% chance by 2100 (selection bias noted).
- **Species Extinction**: Average species lifespan is about 1 million years; humans have existed for ~300,000 years, suggesting low short-term extinction risk.
- **Nuclear War/Climate Change**: No historical precedent for human extinction, but significant potential risks exist.

### Recent developments (timeline bullets)
- [2023-06-15] ↓ — UN report highlights progress in nuclear disarmament, reducing existential risk.
- [2023-03-10] ↑ — AI experts warn of increasing risks from autonomous weapons systems.
- [2022-11-05] ↓ — Global climate agreements show increased commitment to carbon neutrality by 2050.

### Mechanisms & drivers (causal levers)
- **Nuclear Conflict**: Large impact; potential for global catastrophe.
- **Pandemics**: Moderate impact; advances in biotechnology could mitigate risks.
- **Climate Change**: Moderate impact; policy and technological advances are crucial.
- **Artificial Intelligence**: Large impact; potential for both risk and mitigation.
- **Asteroid Impact**: Small impact; low probability but high consequence.

### Differences vs. the base rate (what’s unusual now)
- **Technological Advancements**: Rapid AI and biotech developments increase both risk and mitigation potential.
- **Global Cooperation**: Increased international collaboration on existential threats.
- **Awareness and Preparedness**: Growing recognition of existential risks and efforts to address them.
- **Space Exploration**: Efforts to become a multi-planetary species could reduce extinction risk.

### Bayesian update sketch (for the statistician)
- **Prior**: 19% probability by 2100 (selection-biased expert estimate).
- **Evidence mapping**:
  - ↓ Nuclear disarmament progress (moderate).
  - ↑ AI risk warnings (moderate).
  - ↓ Climate policy advancements (small).
- **Net effect**: Posterior should move slightly down due to recent positive developments, but AI risks warrant caution.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in geopolitical tensions.
  - Breakthroughs in AI capabilities without safety measures.
  - Failure of major climate agreements.
- **DOWN indicators**:
  - Successful international treaties on AI safety.
  - Advances in pandemic preparedness.
  - Progress in space colonization efforts.

### Caveats & pitfalls
- **Uncertainty in Expert Opinions**: Selection bias in expert estimates.
- **Data Gaps**: Limited historical data on existential risks.
- **Deception Risks**: Potential misinformation on technological capabilities.
- **Regime Changes**: Political shifts could alter risk landscapes.
- **Definitional Gotchas**: Ambiguities in defining "human" and "extinction."

Final Research Summary: While expert estimates suggest a non-negligible risk of human extinction by 2100, recent developments in nuclear disarmament and climate policy are positive. However, AI risks remain a concern, warranting a cautious but slightly optimistic outlook.

### Sources (autofetched)
- Deep future: Why we'll still be here (newscientist.com) — https://www.newscientist.com/article/mg21328541-800-deep-future-why-well-still-be-here/
- In Order to Ensure Our Survival, We Must B

## Per-model

- **OpenRouter-Default** ok=True time=22962ms parsed=0.05

- **Claude-3.7-Sonnet (OR)** ok=True time=24828ms parsed=0.12

- **Gemini** ok=False time=737ms parsed=0.0

- **Grok** ok=False time=737ms parsed=0.0

## Ensembles (with research)

- main=0.1221 | no_gtmc1=0.1221 | uniform=0.1221 | simple=0.0850

## Ablation (no research)

- main=0.0821 | uniform=0.0821 | simple=0.0400