📊 Forecast Run 20250909-211148
Timestamp: 2025-09-09 21:12:55 +0300
============================================================

------------------------------------------------------------

❓ **Question:** Will humans go extinct before 2100?
🔗 **URL:** https://www.metaculus.com/questions/578/

📝 **Summary**

- Final Prediction: **5.33% YES**

------------------------------------------------------------

## RESEARCH
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ↑ — AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] ↑ — Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] ↑ — Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large — Potential for AI to surpass human control.
- **Nuclear Conflict**: Large — Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate — Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate — New diseases could emerge with high mortality rates.
- **Technological Advances**: Small — Could mitigate or exacerbate risks.

### Differences vs. the base rate (what’s unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - ↑ AI risk (large)
  - ↑ Climate change impacts (moderate)
  - ↑ Nuclear tensions (large)
  - ↓ Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 21:11:48 +0300 (Similarity threshold ≥0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? — binary — 1.0% YES — [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] — binary — 4.5% YES — [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

## GTMC1 DETAILS
_GTMC1 not used for this question._

------------------------------------------------------------

## BAYES-MC DETAILS
```json
{
  "method": "Beta weak-prior pool",
  "alpha": 0.235,
  "beta": 3.965,
  "p10": 8.884384196671074e-06,
  "p50": 0.009388539970025794,
  "p90": 0.17362078441225726,
  "n_members": 4
}
```

------------------------------------------------------------

## MODEL REASONING (RAW)
### OpenRouter-Default [openai/gpt-4o]
```
1. **Base Rate (Prior) Selection**
   - The historical base rate for human extinction events is effectively zero, as there have been no known instances of human extinction. However, expert estimates, such as the 19% chance by 2100 from the Global Catastrophic Risk Conference, suggest a higher probability. Given the selection bias in that estimate, a more conservative prior is appropriate. A reasonable base rate might be around 1%, reflecting the low historical frequency but acknowledging expert concerns.
   - Initial prior: 1% probability.

2. **Comparison to Base Case**
   - Similarities: The current situation shares the ongoing presence of existential risks like nuclear conflict and pandemics, which have been concerns for decades.
   - Differences: Rapid advancements in AI, increasing climate instability, and heightened geopolitical tensions are more pronounced now than in the past, potentially increasing the risk of extinction.

3. **Evidence Evaluation (Likelihoods)**
   - **AI Risk**: AI experts warn of a 20% chance of AI-induced extinction within 30 years. This significantly increases the likelihood of extinction (P(E | H) > P(E | ~H)).
   - **Climate Change**: A 70% chance of Gulf Stream collapse by 2035 could lead to severe climate shifts, moderately increasing the likelihood of extinction.
   - **Nuclear Conflict**: Recent articles highlight nuclear war risks, which also increase the likelihood of extinction.
   - **Historical Extinction Events**: The lack of historical extinction events slightly decreases the likelihood of extinction.
   - Overall, the evidence suggests a moderate increase in the probability of extinction.

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds: 1% probability = 0.01 odds.
   - Likelihood ratio: Considering the significant increase in AI and nuclear risks, and moderate climate risks, the likelihood ratio might be around 3 (qualitatively estimated).
   - Posterior odds = Prior odds × Likelihood ratio = 0.01 × 3 = 0.03.
   - Posterior probability = 0.03 / (1 + 0.03) ≈ 2.91%.

5. **Red Team Thinking**
   - Overconfidence risk: The forecast might underestimate the potential for mitigating factors, such as international cooperation on AI and climate change.
   - Alternative scenarios: Successful global governance frameworks could significantly reduce risks.
   - Adjustments: Consider a slight reduction in the probability to account for potential mitigating factors.

6. **Final Forecast**
   - After considering the base rate, evidence, and potential mitigating factors, the final forecast is adjusted to reflect a slightly lower probability than the initial posterior calculation.
   - Final: 2.5%
```

### Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. This is challenging since human extinction has never occurred, so I'll consider several reference points:

- Historical data: Humans have existed for roughly 300,000 years with no extinction events, suggesting an extremely low base rate.
- Expert surveys: The Oxford Global Catastrophic Risk Conference yielded a 19% median probability, but this likely suffers from selection bias.
- Metaculus community prediction: Currently at 1.0%
- Manifold market: Currently at 4.5%

Given these reference points and acknowledging the selection bias in the Oxford survey, I'll use a base rate of 3%. This is higher than Metaculus but lower than Manifold, and significantly lower than the potentially biased Oxford survey.

Prior probability P(H) = 3% or 0.03
Prior odds = 0.03/(1-0.03) = 0.031

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans have demonstrated remarkable resilience and adaptability throughout history
- Global cooperation exists for some existential threats
- Technology continues to advance, potentially offering solutions to threats

**Differences from base case:**
- Unprecedented technological capabilities (especially AI) with unknown risks
- Climate change accelerating at rates not seen in human history
- Nuclear arsenals remain substantial despite reductions
- Global connectivity increases pandemic spread potential
- Multiple simultaneous existential threats rather than isolated risks

These differences suggest the current situation may carry higher risk than historical periods, warranting careful consideration of new evidence.

## 3. Evidence Evaluation (Likelihoods)

**Evidence 1: AI risk warnings**
- P(E|H): If humans go extinct, there's a high probability (0.7) that AI experts would warn of extinction risks
- P(E|~H): If humans don't go extinct, there's a moderate probability (0.3) experts would still warn about risks (being cautious)
- Likelihood ratio = 0.7/0.3 = 2.33
- This evidence increases the probability of extinction

**Evidence 2: Gulf Stream collapse prediction**
- P(E|H): If humans go extinct, there's a moderate probability (0.5) of severe climate shifts like Gulf Stream collapse
- P(E|~H): If humans don't go extinct, there's a lower probability (0.2) of such severe predictions
- Likelihood ratio = 0.5/0.2 = 2.5
- This evidence increases the probability of extinction

**Evidence 3: Nuclear war risk assessment**
- P(E|H): If humans go extinct, there's a high probability (0.6) of heightened nuclear tensions
- P(E|~H): If humans don't go extinct, there's a moderate probability (0.3) of such warnings
- Likelihood ratio = 0.6/0.3 = 2.0
- This evidence increases the probability of extinction

**Evidence 4: Historical resilience of humanity**
- P(E|H): If humans go extinct, there's a low probability (0.1) we would have survived numerous past threats
- P(E|~H): If humans don't go extinct, there's a high probability (0.8) we would have demonstrated such resilience
- Likelihood ratio = 0.1/0.8 = 0.125
- This evidence decreases the probability of extinction

## 4. Bayesian Updating (Posterior Probability)

Starting with prior odds of 0.031, I'll update sequentially:

**Update on AI risk warnings:**
Posterior odds = 0.031 × 2.33 = 0.072

**Update on Gulf Stream collapse prediction:**
Posterior odds = 0.072 × 2.5 = 0.18

**Update on nuclear war risk assessment:**
Posterior odds = 0.18 × 2.0 = 0.36

**Update on historical resilience:**
Posterior odds = 0.36 × 0.125 = 0.045

Converting to probability:
Posterior probability = 0.045/(1+0.045) = 0.043 or 4.3%

## 5. Red Team Thinking

**Potential overconfidence:**
- I may be underestimating unprecedented risks from advanced AI
- Climate tipping points could trigger cascading effects beyond current models
- Simultaneous crises could overwhelm human adaptive capacity
- Unknown unknowns (threats we haven't identified yet)

**Alternative scenarios:**
- Technological advancement could accelerate solutions faster than threats emerge
- International cooperation might increase in response to existential threats
- Human space colonization could provide extinction insurance

**Best alternative forecast:**
A higher probability (10-15%) might be justified if we believe AI poses a truly unprecedented risk unlike anything in human history, and if we think climate tipping points are more likely than current models suggest.

**Rebuttal:**
While these risks are serious, humanity has strong survival incentives and has repeatedly demonstrated adaptability. Complete extinction requires the elimination of all humans everywhere, which is an extremely high bar even for catastrophic events.

Given these considerations, I'll adjust my forecast slightly upward to account for unprecedented technological risks and potential underestimation of climate impacts.

## 6. Final Forecast

After considering the base rate, evidence, and red team thinking, I believe the probability of human extinction before 2100 is slightly higher than my calculated posterior but still relatively low.

Final: 5%
```

### Gemini [gemini-2.5-pro]
```
Here is my probabilistic reasoning process:

### 1. Base Rate (Prior) Selection
The event in question, human extinction, has no historical precedent. A simple historical base rate is therefore 0, which is unhelpful as it doesn't account for novel anthropogenic risks. A better starting point is to use an aggregated expert or informed community forecast as a prior.

The 2008 Oxford conference's median estimate of 19% is noted, but it is likely inflated due to strong selection bias. A more reliable, though still imperfect, anchor is the Metaculus community forecast, which stands at 1.0%. This represents the aggregated belief of a large number of forecasters who are incentivized to be accurate and have considered much of the publicly available information. It is a sober estimate that acknowledges the existence of risk without being alarmist.

I will select a prior probability P(H) of 1%, which is equivalent to prior odds of 1:99.

### 2. Comparison to Base Case
The base case for humanity is survival over hundreds of thousands of years against natural risks (plagues, ice ages, etc.). The current situation is fundamentally different due to the emergence of powerful anthropogenic risks:
*   **Artificial Intelligence**: The rapid, potentially exponential, development of AI presents a risk qualitatively different from any previous technology.
*   **Nuclear Weapons**: The capacity for self-destruction has existed for decades, and current geopolitical instability increases the risk of its use.
*   **Climate Change**: We are entering an era of unprecedented climate instability with potential for cascading ecosystem and societal collapse.
*   **Global Connectivity**: While a source of strength, it also makes humanity more vulnerable to fast-spreading engineered pandemics.

These differences suggest that the historical record of survival is a poor guide, and the probability of extinction is significantly higher now than in any previous century. My 1% prior is intended to already account for this general state of affairs.

### 3. Evidence Evaluation
I will now evaluate the new evidence presented in the research report to update my 1% prior.

*   **Evidence 1: AI experts warn of a 20% chance of AI-induced extinction within 30 years.**
    *   This is a very strong piece of evidence. Let H be "human extinction by 2100" and E be this expert warning.
    *   P(E | H): If humanity does go extinct, it is highly probable that AI would be a leading cause, and it is also highly probable that experts would have been issuing dire warnings beforehand. This likelihood is high.
    *   P(E | ~H): If humanity survives, it's still possible experts would be worried (they could be wrong or we could avert the danger). However, a warning of this magnitude and specificity is less likely in a world where the risk is actually negligible.
    *   **Likelihood Ratio (P(E|H)/P(E|~H))**: This is significantly greater than 1. The warning is specific and severe, suggesting a substantial update is needed. I estimate this evidence makes extinction at least 2.5 to 3 times more likely than I previously thought. This evidence strongly increases the probability.

*   **Evidence 2: Study shows a 70% chance of Gulf Stream collapse by 2035.**
    *   This points to a severe climate catastrophe that would cause famine, mass migration, and war, dramatically increasing global instability.
    *   While a Gulf Stream collapse is unlikely to cause extinction directly, it acts as a powerful "risk multiplier," increasing the probability of other catastrophic events like a nuclear exchange.
    *   **Likelihood Ratio**: This is greater than 1, but the link to *extinction* is less direct than for AI. It makes the world a much more dangerous place, making extinction more plausible. I estimate a likelihood ratio of around 1.2. This evidence moderately increases the probability.

*   **Evidence 3: Increased Geopolitical Tensions / Nuclear Risk.**
    *   The report highlights this as a "large" causal lever. Similar to climate change, this increases the background level of risk and the chance of a conflict escalating to a civilization-ending nuclear exchange.
    *   **Likelihood Ratio**: Again, this is greater than 1. A world with high tensions is more likely to end in extinction than a peaceful one. I estimate a likelihood ratio of around 1.2. This evidence moderately increases the probability.

### 4. Bayesian Updating (Posterior Probability)
I will start with the prior and sequentially apply the evidence.

*   **Prior Odds**: 1:99 (equivalent to 1% probability).
*   **Step 1: Update for AI Risk (LR ≈ 2.5)**
    *   Posterior Odds = Prior Odds × LR = (1:99) × 2.5 = 2.5:99
    *   Posterior Probability = 2.5 / (2.5 + 99) ≈ 2.4%
*   **Step 2: Update for Climate & Nuclear Risk Multipliers (Combined LR ≈ 1.2 * 1.2 ≈ 1.44)**
    *   Posterior Odds = (2.5:99) × 1.44 ≈ 3.6:99
    *   Posterior Probability = 3.6 / (3.6 + 99) ≈ 3.5%

This mechanical update brings the initial 1% probability to approximately 3.5%. This final figure reflects the initial low base rate, but acknowledges that recent, severe warnings about specific risk factors warrant a significant upward revision.

### 5. Red Team Thinking
My forecast of ~3.5% is higher than Metaculus (1%) but lower than Manifold (4.5%) and much lower than the expert polls (19%).

*   **Argument for a lower forecast (<3.5%)**: I might be overreacting to recent news. The bar for *complete extinction* is extraordinarily high; even after a nuclear winter or climate collapse, small, resilient human groups would likely survive. The wisdom of the Metaculus crowd at 1% may have correctly priced in this resilience.
*   **Argument for a higher forecast (>3.5%)**: I might be underestimating the unique nature of AI risk. Unlike other catastrophes, a misaligned superintelligence could systematically and efficiently eliminate all remaining humans. In this view, the 20% expert estimate is not alarmism but a realistic assessment, and my forecast should be higher.
*   **Conclusion**: My 3.5% estimate attempts to balance these concerns. It acknowledges that AI is the primary driver that makes this century different, and that other risks are potent multipliers. It remains a low-probability event, but the evidence suggests the risk is higher than the 1% baseline. The Manifold market's 4.5% provides some support for a number in this range. I will round my final forecast to 4% to reflect a slightly stronger weighting of the recent negative evidence.

### Final Forecast
After starting with a 1% prior anchored on the community consensus, I have updated based on strong evidence of increased risk from AI, and moderate evidence from climate instability and geopolitical tensions. The high bar for complete extinction provides a strong downward pressure on any forecast, but the unique nature of AI risk warrants a significant increase from the baseline. My final probability balances these factors.

Final: 4%
```

### Grok [grok-4]
```
### 1. Base Rate (Prior) Selection
For human extinction before 2100, an appropriate base rate must draw from historical data on species survival and expert estimates of existential risks, as direct historical precedents for human extinction are zero (humanity has persisted for ~300,000 years without extinction). However, we can use reference classes like past mass extinction events (e.g., five major ones in Earth's history, affecting ~75-96% of species, but not all), or expert surveys on global catastrophic risks. The 2008 Oxford poll gives a median of 19%, but as noted, it's biased toward high-risk views due to selection effects. More neutral aggregators like Metaculus (1%) and Manifold (4.5%) provide community-driven baselines, reflecting a broader consensus. Long-term human survival rates (e.g., no extinction over millennia despite wars, pandemics, and disasters) suggest a very low base rate.

I choose a prior probability of 1% (or odds of 1:99), aligning with Metaculus as a starting point. This is conservative, based on the absence of extinction events in human history and the fact that most expert forecasts (outside biased samples) cluster in the low single digits. It incorporates general statistics on resilience: humanity has survived ice ages, plagues (e.g., Black Death killed ~30-50% of Europe but not globally), and two world wars without total collapse.

### 2. Comparison to Base Case
The base case is historical human survival through catastrophic threats, such as pandemics (e.g., 1918 flu killed ~50 million but humanity rebounded) or geopolitical conflicts (e.g., Cold War nuclear standoffs without escalation to extinction). Similarities include ongoing risks like pandemics and wars, where human adaptability (technology, cooperation) has prevented total wipeout. For instance, current geopolitical tensions mirror the Cold War, but with more actors (multipolarity), and climate change echoes past environmental shifts but at an accelerated pace.

Differences are significant: Rapid AI development introduces novel risks without historical analogs (e.g., superintelligent AI misalignment, unlike past tech like nuclear weapons, which we've managed so far). Climate change is anthropogenic and faster than natural cycles, with potential tipping points like Gulf Stream collapse. Global connectivity amplifies pandemics beyond historical outbreaks. These differences matter because they introduce unprecedented, high-uncertainty levers that could cascade into extinction, potentially warranting an upward adjustment from the base rate, though mitigated by modern mitigation efforts (e.g., AI safety research, climate accords).

### 3. Evidence Evaluation (Likelihoods)
Key pieces from the research report and background:

- **AI risks (↑ large)**: Warnings of 20% extinction chance in 30 years. P(E | H) is high (~80%) because uncontrolled AI could directly cause extinction (e.g., via rogue systems). P(E | ~H) is lower (~30%) as such warnings might arise from hype or caution without actual risk. Likelihood ratio: ~2.7 (increases probability moderately, as it's speculative but from experts).

- **Climate change (↑ moderate, e.g., 70% Gulf Stream collapse)**: P(E | H) ~70% (severe shifts could contribute to extinction via famine/ecosystem collapse). P(E | ~H) ~40% (studies might overestimate without leading to total extinction). Likelihood ratio: ~1.75 (increases probability slightly, as extinction-level climate impacts are rare even in models).

- **Nuclear war risks (↑ large)**: Recent articles on tensions. P(E | H) ~60% (escalation could cause nuclear winter, leading to extinction). P(E | ~H) ~20% (tensions often de-escalate). Likelihood ratio: ~3 (increases probability notably, given geopolitical volatility).

- **Historical lack of extinction (↓ small)**: Zero events. P(E | H) ~10% (if extinction were imminent, we'd expect precursors). P(E | ~H) ~100% (consistent with survival). Likelihood ratio: ~0.1 (decreases probability strongly, anchoring against overreaction).

- **Pandemics and tech advances (moderate/small)**: Balanced; pandemics could spread faster now (↑), but tech mitigates (↓). Net likelihood ratio ~1.2 (slight increase).

Overall, evidence leans upward due to novel risks, but historical resilience pulls downward.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior odds: 1:99 (P(H) = 1%).

First explicit update: Combine major upward evidence (AI + nuclear + climate, net LR ~2.5 from qualitative averaging of ratios). Posterior odds = 1:99 × 2.5 = 2.5:99 ≈ 1:39.6. Posterior P = 1 / (1 + 39.6) ≈ 2.5%.

Second update: Incorporate downward historical evidence (LR ~0.5, accounting for small weight). Posterior odds = 1:39.6 × 0.5 ≈ 1:79.2. Posterior P = 1 / (1 + 79.2) ≈ 1.2%.

The resulting posterior is around 1-2%, reflecting a slight increase from the prior due to recent developments, but significant uncertainty remains due to data gaps and potential mitigations (e.g., AI governance). Confidence is low-moderate; the probability could swing with new indicators.

### 5. Red Team Thinking
My forecast might be underconfident by anchoring too heavily to low base rates and Metaculus (1%), potentially ignoring tail risks like cascading failures (e.g., AI-triggered nuclear war amid climate chaos). Blind spots include over-reliance on expert consensus, which has been wrong before (e.g., underestimated pandemics pre-COVID). Alternative scenarios: A high-end forecast (e.g., 10-20% like the Oxford poll) is plausible if AI timelines accelerate unexpectedly or if climate tipping points chain-react, rebutted by humanity's track record of adaptation and the fact that most "doomsday" predictions (e.g., Y2K, peak oil) fizzled. Tail risks (e.g., unknown unknowns like asteroid impacts) could push it higher, but I see no strong evidence for big swings. On rebuttal, overconfidence in low probabilities ignores black swans, but calibration advice urges small updates. I'll nudge up slightly to account for this, ensuring the forecast isn't too dismissive of expert warnings.

The best alternative forecast might be 5% (closer to Manifold), arguing recent AI/climate news warrants more weight; rebuttal is selection bias and historical overprediction of doom. No major adjustment needed, but I'll widen implicit uncertainty to cover tails.

Final: 2%
```

