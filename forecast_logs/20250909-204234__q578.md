üìä Forecast Run 20250909-204234
Timestamp: 2025-09-09 20:43:47 +0300
============================================================

------------------------------------------------------------

‚ùì **Question:** Will humans go extinct before 2100?
üîó **URL:** https://www.metaculus.com/questions/578/

üìù **Summary**

- Final Prediction: **6.18% YES**

------------------------------------------------------------

## RESEARCH
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ‚Üë ‚Äî AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] ‚Üë ‚Äî Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] ‚Üë ‚Äî Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large ‚Äî Potential for AI to surpass human control.
- **Nuclear Conflict**: Large ‚Äî Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate ‚Äî Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate ‚Äî New diseases could emerge with high mortality rates.
- **Technological Advances**: Small ‚Äî Could mitigate or exacerbate risks.

### Differences vs. the base rate (what‚Äôs unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - ‚Üë AI risk (large)
  - ‚Üë Climate change impacts (moderate)
  - ‚Üë Nuclear tensions (large)
  - ‚Üì Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 20:42:34 +0300 (Similarity threshold ‚â•0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? ‚Äî binary ‚Äî 1.0% YES ‚Äî [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] ‚Äî binary ‚Äî 4.5% YES ‚Äî [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

## GTMC1 DETAILS
_GTMC1 not used for this question._

------------------------------------------------------------

## BAYES-MC DETAILS
```json
{
  "method": "Beta weak-prior pool",
  "alpha": 0.27,
  "beta": 3.9299999999999997,
  "p10": 3.227105870887235e-05,
  "p50": 0.014491279770632345,
  "p90": 0.1948722987475172,
  "n_members": 4
}
```

------------------------------------------------------------

## MODEL REASONING (RAW)
### OpenRouter-Default [openai/gpt-4o]
```
To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach.

### 1. Base Rate (Prior) Selection
The historical base rate for human extinction is effectively zero, as there have been no known extinction events in human history. However, expert estimates, such as the 19% probability from the Global Catastrophic Risk Conference, suggest a higher risk, though this is likely biased. A more conservative prior might be around 1%, reflecting the historical absence of extinction events but acknowledging potential risks.

**Initial Prior Probability:** 1%

### 2. Comparison to Base Case
**Similarities:** 
- The absence of historical extinction events aligns with the base rate of zero.

**Differences:**
- Recent developments in AI, climate change, and geopolitical tensions introduce new risks not present in historical contexts. These factors suggest a higher risk than the historical base rate.

### 3. Evidence Evaluation (Likelihoods)
- **AI Risk:** Recent warnings of a 20% chance of AI-induced extinction within 30 years increase the likelihood of extinction. This is a significant risk factor.
  - Likelihood Ratio: High (increases probability)
  
- **Climate Change:** A 70% chance of Gulf Stream collapse by 2035 could lead to severe climate shifts, increasing extinction risk.
  - Likelihood Ratio: Moderate (increases probability)
  
- **Nuclear Conflict:** Ongoing geopolitical tensions and potential for nuclear war increase extinction risk.
  - Likelihood Ratio: High (increases probability)
  
- **Historical Extinction Events:** The absence of past extinction events slightly decreases the probability.
  - Likelihood Ratio: Low (decreases probability)

### 4. Bayesian Updating (Posterior Probability)
Using the prior probability of 1% and considering the evidence:
- **AI and Nuclear Risks:** These are significant and increase the probability substantially.
- **Climate Change:** Moderately increases the probability.
- **Historical Context:** Slightly decreases the probability.

**Posterior Odds Calculation:**
- Prior Odds = 0.01 / 0.99 ‚âà 0.0101
- Combined Likelihood Ratio (qualitative estimate): Given the high impact of AI and nuclear risks, assume a moderate increase, say a factor of 5.
- Posterior Odds = 0.0101 √ó 5 = 0.0505
- Posterior Probability = 0.0505 / (1 + 0.0505) ‚âà 4.8%

### 5. Red Team Thinking
- **Overconfidence Check:** The forecast might underestimate the impact of mitigating factors like international cooperation on AI and nuclear disarmament.
- **Tail Risks:** Consider low-probability, high-impact events like unforeseen technological breakthroughs that could mitigate risks.
- **Alternative Forecast:** A more optimistic view might focus on successful global governance reducing risks, suggesting a lower probability.

### 6. Final Forecast
Considering the base rate, evidence, and potential mitigating factors, the final forecast reflects a moderate increase from the prior due to significant risks from AI and nuclear conflict, balanced by historical context and potential mitigations.

Final: 5%
```

### Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. This is challenging since human extinction has never occurred, so I'll consider several reference points:

- Historical data: Humans have existed for roughly 300,000 years without extinction, suggesting an extremely low base rate.
- Expert surveys: The Oxford Global Catastrophic Risk Conference yielded a 19% median probability, but this likely suffers from selection bias.
- Prediction markets: Metaculus community forecast is at 1.0%, while Manifold shows 4.5%.

Given these considerations, I'll use a base rate of 2% as my prior probability. This is higher than the historical rate (effectively 0%) but lower than potentially biased expert estimates, and roughly in line with prediction market consensus. This represents a 1 in 50 chance of extinction.

Prior odds: 2% / 98% = 1:49 or about 0.02

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans have demonstrated remarkable resilience and adaptability throughout history
- We've survived previous existential threats (Cold War nuclear tensions, pandemics)
- Global cooperation mechanisms exist for addressing shared threats

**Key differences:**
- Unprecedented technological capabilities, especially in AI, that could pose novel extinction risks
- Climate change occurring at rates not seen in human history
- Greater global interconnectedness that could amplify cascading failures
- Higher nuclear weapons stockpiles than most of human history

These differences suggest the current situation may have higher extinction risk than historical averages.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence 1: AI expert warnings (20% chance of AI-induced extinction within 30 years)**
- P(E|H): If human extinction occurs, the probability of seeing such warnings is very high (~0.9)
- P(E|~H): If extinction doesn't occur, we'd still expect some experts to warn about it (~0.3)
- Likelihood ratio: 0.9/0.3 = 3.0
- This evidence increases the probability of extinction

**Evidence 2: Gulf Stream collapse risk (70% chance by 2035)**
- P(E|H): If extinction occurs, severe climate disruptions would likely precede it (~0.8)
- P(E|~H): If extinction doesn't occur, we might still see climate disruptions but adapt (~0.6)
- Likelihood ratio: 0.8/0.6 = 1.33
- This moderately increases the probability

**Evidence 3: Nuclear war risks**
- P(E|H): If extinction occurs, nuclear conflict is a plausible pathway (~0.7)
- P(E|~H): If extinction doesn't occur, we'd still expect ongoing nuclear tensions (~0.5)
- Likelihood ratio: 0.7/0.5 = 1.4
- This moderately increases the probability

**Evidence 4: Historical resilience of humanity**
- P(E|H): If extinction occurs, we'd expect to see past near-misses (~0.6)
- P(E|~H): If extinction doesn't occur, human resilience would be a factor (~0.9)
- Likelihood ratio: 0.6/0.9 = 0.67
- This decreases the probability

## 4. Bayesian Updating

Starting with my prior odds of 0.02 (1:49), I'll update based on the evidence:

Update for AI expert warnings:
Posterior odds = 0.02 √ó 3.0 = 0.06
Posterior probability = 0.06/(1+0.06) ‚âà 5.7%

Update for Gulf Stream collapse risk:
Posterior odds = 0.06 √ó 1.33 = 0.0798
Posterior probability = 0.0798/(1+0.0798) ‚âà 7.4%

Update for nuclear war risks:
Posterior odds = 0.0798 √ó 1.4 = 0.1117
Posterior probability = 0.1117/(1+0.1117) ‚âà 10.0%

Update for historical resilience:
Posterior odds = 0.1117 √ó 0.67 = 0.0748
Posterior probability = 0.0748/(1+0.0748) ‚âà 7.0%

After incorporating all evidence, my posterior probability is approximately 7.0%.

## 5. Red Team Thinking

**Potential overconfidence:**
- I may be underestimating unprecedented risks from emerging technologies
- The evidence pieces aren't fully independent, which could lead to double-counting
- Historical resilience may not apply to novel threats like superintelligent AI

**Alternative scenarios:**
- Lower probability (1-3%): Human adaptability and technological solutions might effectively mitigate existential risks
- Higher probability (15-20%): Compounding risks from multiple sources (AI, climate, nuclear) could create unforeseen extinction pathways

**Best alternative forecast:**
A stronger case could be made for a 3-4% probability by emphasizing humanity's track record of solving existential challenges and the fact that prediction markets (which aggregate diverse perspectives) show lower probabilities than my estimate.

**Rebuttal to alternative:**
While humanity has shown resilience, we've never faced the particular combination of risks present today, especially regarding advanced AI and climate tipping points. The Metaculus forecast of 1% may underweight recent developments.

After this critical evaluation, I'll adjust my forecast slightly downward to 6%, which better balances historical resilience against novel risks while remaining consistent with the range of expert and market estimates.

## 6. Final Forecast

Final: 6%
```

### Gemini [gemini-2.5-pro]
```
Here is my probabilistic forecast reasoning:

### 1. Base Rate (Prior) Selection
The event in question is human extinction within the next ~75 years. A direct historical base rate is zero, as *Homo sapiens* has never gone extinct. However, this is not a useful prior because it doesn't account for novel, 20th and 21st-century anthropogenic risks like nuclear weapons and artificial intelligence.

A better starting point is to use an aggregated expert or market forecast as a prior, as this incorporates a wider set of information. The 2008 Oxford conference median of 19% is noted to be heavily skewed by selection bias and is too high to be a credible prior. The Metaculus community forecast of 1.0% is a much more robust anchor. It represents the aggregated belief of thousands of forecasters who are incentivized for accuracy and have access to the same public information about current risks.

Therefore, I will select **1%** as my prior probability, P(H). This corresponds to prior odds of 1:99.

### 2. Comparison to Base Case
The base case can be considered the late 20th century, the first period with anthropogenic existential risk (nuclear weapons). The current situation is different in several critical ways that increase the risk:
- **Artificial Intelligence**: The rapid, unpredictable development of powerful AI introduces a completely novel existential risk category that did not exist in the 20th century.
- **Accelerated Climate Change**: The effects of climate change and the risk of hitting irreversible tipping points are far more acute and well-understood now than in the past.
- **Biotechnology**: Advances like CRISPR make the creation of engineered pandemics more feasible.
- **Geopolitical Instability**: A shift from a bipolar (Cold War) to a multipolar world may increase the chances of miscalculation and conflict between great powers.

These differences suggest that the risk level is likely higher now than it was 30-40 years ago, and an upward adjustment from a simple historical baseline is warranted.

### 3. Evidence Evaluation (Likelihoods)
I will evaluate the key evidence from the research report to update my 1% prior.

-   **Evidence 1: AI experts warn of a 20% chance of AI-induced extinction within 30 years.**
    -   This is a very strong piece of evidence. P(E|H), the probability of seeing such dire expert warnings if extinction (H) were to occur, is high. P(E|~H), the probability of seeing these warnings if we survive, is lower; while experts can be wrong or alarmist, a specific 20% figure is a very strong signal.
    -   This evidence points to a specific mechanism that could overcome the "extinction is hard" problem (i.e., an unaligned superintelligence could be methodical enough to eliminate all human survivors, unlike a nuclear war or pandemic).
    -   **Likelihood Ratio (P(E|H)/P(E|~H))**: Significantly greater than 1. This evidence strongly increases the probability.

-   **Evidence 2: Study shows a 70% chance of Gulf Stream collapse by 2035.**
    -   This points to massive civilizational stress (famine, migration, conflict) but not directly to extinction. It's a "risk of a risk."
    -   P(E|H) is moderately high, as such a collapse would be a plausible part of an extinction-level sequence of events. P(E|~H) is also quite high, as humanity could potentially survive such a collapse, albeit in a severely diminished state.
    -   **Likelihood Ratio**: Moderately greater than 1. This evidence increases the probability by showing the fragility of our planetary systems.

-   **Evidence 3: The high bar for "extinction".**
    -   The resolution criteria require *zero* known humans to be alive. This is a crucial piece of information that functions as evidence. Many catastrophes (nuclear war, natural pandemics, climate collapse) would likely kill billions but leave some survivors in remote or sheltered locations.
    -   P(E|H): If extinction happens, this high bar must be cleared. P(E|~H): If we don't go extinct, it's overwhelmingly likely because some humans survived a catastrophe, not because no catastrophe occurred. Therefore, the difficulty of clearing this bar makes ~H much more likely than H for most risk scenarios.
    -   **Likelihood Ratio**: Less than 1. This evidence strongly decreases the probability for any risk *except* those that could plausibly ensure total eradication (e.g., AI).

### 4. Bayesian Updating (Posterior Probability)
I start with my prior odds of 1:99 (1% probability).

The evidence from AI experts is the most powerful upward update. It presents a plausible scenario for clearing the high bar of complete extinction. The climate instability evidence adds to this by increasing the background level of risk. Let's combine these factors into a single likelihood ratio. Given the severity of the AI warnings, I estimate this new information makes the extinction hypothesis roughly 3 times more likely than the survival hypothesis.

-   **Posterior Odds = Prior Odds √ó Likelihood Ratio**
-   Posterior Odds = (1:99) √ó 3 = 3:99 = 1:33
-   **Posterior Probability** = 1 / (1 + 33) = 1/34 ‚âà 2.94%

This calculation brings the probability to approximately 3%. This feels like a reasonable update, as it takes the expert warnings seriously without fully accepting them at face value, and it remains anchored to the sober market prior.

### 5. Red Team Thinking
My forecast of 3% is triple the Metaculus prediction of 1%. Why might I be wrong? I might be suffering from availability bias, overreacting to recent scary headlines about AI that the Metaculus community has already priced in. The "wisdom of the crowd" is a powerful force, and deviating from it is risky. They may be correctly assessing that humanity will rise to the challenge and implement safeguards for AI and other technologies.

On the other hand, why might Metaculus be too low? The community could be suffering from normalcy bias, finding it psychologically difficult to assign a meaningful probability to the end of the world. They may also be underestimating the exponential pace of AI development and the correlated nature of global risks. The Manifold market at 4.5% suggests that a forecast higher than 1% is very plausible. My 3% forecast sits comfortably between these two market indicators, suggesting it is a reasonable synthesis of the available evidence. I will stick with this estimate.

### Final Forecast
My final forecast reflects the 1% market-based prior, updated by recent and severe evidence regarding novel risks like AI, which provides a plausible mechanism for complete extinction.

Final: 3%
```

### Grok [grok-4]
```
### 1. Base Rate (Prior) Selection
For human extinction before 2100, an appropriate base rate must account for the fact that humanity has never gone extinct in recorded history (over ~300,000 years of Homo sapiens existence), making the historical frequency effectively zero. However, this is a poor direct prior for future-oriented existential risks because it doesn't capture rare, high-impact events that haven't occurred yet but are theoretically possible (e.g., nuclear war, pandemics, or AI misalignment). Instead, I draw from expert elicitations and forecasting communities as a reference class. The 2008 Oxford poll gave a median of 19%, but as noted, it's biased toward high-risk views due to selection effects. More reliable are aggregated forecasts: Metaculus community predicts 1% (as of 2025), and Manifold at 4.5%. Other sources like Toby Ord's "The Precipice" (2020) estimate ~16% for existential catastrophe by 2100, but this includes non-extinction catastrophes. Balancing these, I select a prior probability of 2% (or odds of 1:49), which aligns with low-end expert consensus (e.g., Metaculus) while acknowledging some upward bias in high estimates. This is conservative, reflecting the rarity of extinction-level events but incorporating modern risk awareness.

### 2. Comparison to Base Case
The base case is the historical absence of human extinction, where humanity has survived pandemics (e.g., Black Death killed ~30-50% of Europe but not globally), wars (e.g., World Wars with millions dead but no extinction), and natural disasters (e.g., supervolcanoes like Toba ~75,000 years ago, which reduced but didn't eliminate populations). The current situation is similar in that no single risk has yet escalated to extinction levels, and human resilience (e.g., technology, global cooperation) has historically mitigated threats. However, differences include unprecedented drivers: rapid AI advancement (unlike slow historical tech changes), accelerating climate change (faster than past shifts), heightened geopolitical tensions in a nuclear-armed multipolar world (more volatile than Cold War bipolarity), and global connectivity enabling faster pandemic spread. These differences matter because they introduce novel, interconnected risks that could compound (e.g., AI exacerbating nuclear decisions), potentially increasing the probability beyond historical norms, though mitigation efforts (e.g., AI governance) could counteract this.

### 3. Evidence Evaluation (Likelihoods)
Key evidence from the research report and background:

- **AI risks (‚Üë large)**: Warnings of 20% AI-induced extinction risk within 30 years. P(E | H) is high (~80%) because uncontrolled AI could directly cause or enable extinction (e.g., misaligned superintelligence). P(E | ~H) is moderate (~40%) as such warnings could arise from hype or caution without true risk. Likelihood ratio: ~2 (increases probability moderately, as it's speculative but from experts).

- **Climate change (‚Üë moderate)**: 70% chance of Gulf Stream collapse by 2035, potentially causing severe shifts. P(E | H) high (~70%) if extinction involves cascading ecosystem failures. P(E | ~H) also fairly high (~50%) due to ongoing climate trends without extinction. Likelihood ratio: ~1.4 (slight increase, as extinction-level climate impacts are speculative and mitigable).

- **Nuclear war risks (‚Üë large)**: Highlighted geopolitical tensions with potential for massive casualties. P(E | H) high (~60%) as nuclear winter could lead to extinction. P(E | ~H) moderate (~30%) given persistent tensions without escalation. Likelihood ratio: ~2 (increases probability, but no full-scale war has happened yet).

- **Historical lack of extinction (‚Üì small)**: Zero past events. P(E | H) low (~10%) if extinction were imminent, we'd expect precursors; P(E | ~H) very high (~99%) as status quo. Likelihood ratio: ~0.1 (strongly decreases probability, anchoring to rarity).

- **Pandemics and other drivers (moderate)**: Increased potential due to connectivity, but historical survival (e.g., COVID-19) suggests resilience. Likelihood ratio: ~1.2 (slight increase).

Overall, upward evidence (AI, nuclear, climate) outweighs the downward historical anchor, suggesting a net moderate increase.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior odds: 2% probability = odds of 1:49 (prior odds = 0.02 / 0.98 ‚âà 0.0204).

Combine likelihood ratios qualitatively then quantitatively for a key update. Net likelihood ratio from evidence: AI (~2) √ó Nuclear (~2) √ó Climate (~1.4) √ó Historical (~0.1) √ó Other (~1.2) ‚âà 0.672 (upward factors tempered by historical anchor).

Posterior odds = Prior odds √ó Net LR ‚âà 0.0204 √ó 0.672 ‚âà 0.0137.

Posterior probability = Posterior odds / (1 + Posterior odds) ‚âà 0.0137 / 1.0137 ‚âà 1.35%.

This is a conservative update; uncertainty remains high due to speculative evidence and potential mitigations (e.g., AI regulation). The posterior is confident in low probability but acknowledges tail risks pushing it slightly above the prior.

### 5. Red Team Thinking
My forecast might be underconfident by overly weighting the historical zero base rate, ignoring that we're in a uniquely risky era with no direct precedents‚Äîblack swan events like extinction can't have "historical data" until they happen, potentially leading to underestimation. Blind spots include over-reliance on noisy forecasts (e.g., Metaculus could be optimistically biased by survivalist assumptions). Tail risks: Cascading failures (e.g., AI-triggered nuclear war amid climate famine) could spike probability to 10-20% in worst-case scenarios. Alternative forecast: A higher 10% (e.g., aligning with Ord or biased poll), plausible if recent developments signal accelerating risks and mitigations fail; rebuttal is that human adaptability and international efforts (e.g., treaties) have historically prevented catastrophe, and most expert aggregates trend low. To account for this, I nudge up slightly for tail uncertainty, ensuring the estimate isn't too anchored.

Final: 3%
```

