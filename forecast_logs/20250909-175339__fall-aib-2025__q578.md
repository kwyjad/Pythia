
------------------------------------------------------------

❓ Question: Will humans go extinct before 2100?
🔗 URL: https://www.metaculus.com/questions/578/

📝 Full Reasoning:
# SUMMARY
*Question*: Will humans go extinct before 2100?
*Final Prediction*: 4.50%

------------------------------------------------------------

# RESEARCH
## Research Report
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ↑ — AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] ↑ — Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] ↑ — Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large — Potential for AI to surpass human control.
- **Nuclear Conflict**: Large — Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate — Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate — New diseases could emerge with high mortality rates.
- **Technological Advances**: Small — Could mitigate or exacerbate risks.

### Differences vs. the base rate (what’s unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - ↑ AI risk (large)
  - ↑ Climate change impacts (moderate)
  - ↑ Nuclear tensions (large)
  - ↓ Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 17:53:40 +0300 (Similarity threshold ≥0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? — binary — 1.0% YES — [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] — binary — 4.5% YES — [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

# GTMC1 DETAILS
GTMC1 activated: No (not a strategic question or actor extraction failed).

------------------------------------------------------------

# ENSEMBLE OUTPUTS (Raw → Parsed)
## OpenRouter-Default [openai/gpt-4o]
```text
To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach.

1. **Base Rate (Prior) Selection**
   - The historical base rate for human extinction is effectively zero, as there have been no known extinction events in human history. However, expert estimates, such as the 19% chance by 2100 from the Global Catastrophic Risk Conference, suggest a higher risk. Given the selection bias in the expert estimate, a more conservative prior of 1% is reasonable, reflecting the historical absence of extinction events but acknowledging potential risks.

2. **Comparison to Base Case**
   - Similarities: The current situation shares the historical absence of extinction events.
   - Differences: Recent developments in AI, climate change, and geopolitical tensions introduce new risks not present in historical contexts. These differences suggest a need to adjust the prior probability upwards.

3. **Evidence Evaluation (Likelihoods)**
   - **AI Risk**: AI experts warn of a 20% chance of AI-induced extinction within 30 years. This significantly increases the likelihood of extinction (P(E | H) > P(E | ~H)).
   - **Climate Change**: A 70% chance of Gulf Stream collapse by 2035 could lead to severe climate shifts, moderately increasing the likelihood of extinction.
   - **Nuclear Conflict**: Increased geopolitical tensions and nuclear war risks also increase the likelihood of extinction.
   - **Historical Extinction Events**: The absence of historical extinction events slightly decreases the likelihood of extinction.

   Overall, the evidence suggests a moderate increase in the probability of extinction.

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds: 1% = 0.01 odds
   - Likelihood ratio: Considering the significant evidence from AI, climate change, and nuclear risks, let's assume a likelihood ratio of 3 (a moderate increase).
   - Posterior odds = Prior odds × Likelihood ratio = 0.01 × 3 = 0.03
   - Posterior probability = 0.03 / (1 + 0.03) ≈ 2.91%

5. **Red Team Thinking**
   - Overconfidence: The forecast might underestimate the potential for mitigating factors, such as international cooperation on AI and climate change.
   - Alternative scenarios: Successful global governance frameworks could significantly reduce risks.
   - Adjustments: Considering these factors, a slight downward adjustment to account for potential mitigation is warranted.

6. **Final Forecast**
   - After considering the base rate, evidence, and potential mitigating factors, the final forecast is a probability of human extinction before 2100 of approximately 2.5%.

Final: 2.5%
```
- parsed: 0.025 | ok=True | note=—

## Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```text
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. This is challenging since human extinction has never occurred, so I'll consider several reference points:

- Historical data: Humans have existed for roughly 300,000 years without extinction, suggesting an extremely low base rate.
- Expert surveys: The Oxford Global Catastrophic Risk Conference yielded a 19% median probability, but this likely suffers from selection bias.
- Metaculus community forecast: Currently at 1.0%.
- Manifold market: Currently at 4.5%.

Given these reference points and acknowledging the selection bias in the Oxford survey, I'll use a base rate of 3% as my prior. This is higher than the Metaculus prediction but lower than the Manifold market, reflecting a conservative approach that acknowledges existential risks while not overweighting the biased expert survey.

Prior probability: P(H) = 3% or 0.03
Prior odds: 0.03/(1-0.03) = 0.031

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans have demonstrated remarkable resilience and adaptability throughout history
- Global cooperation exists for some existential threats
- Technology continues to advance, potentially providing solutions to threats

**Differences from base case:**
- Unprecedented technological capabilities, especially in AI, create novel risks
- Climate change is accelerating at rates not seen in human history
- Nuclear arsenals remain substantial despite reductions since Cold War peak
- Global connectivity increases pandemic spread potential
- Multiple existential threats now exist simultaneously, potentially creating compound risks

These differences suggest the need for careful adjustment of the base rate.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence 1: AI experts warning of 20% chance of AI-induced extinction within 30 years**
- P(E|H): If human extinction occurs, the probability of seeing such warnings is very high, perhaps 0.9
- P(E|~H): If extinction doesn't occur, we might still see such warnings due to expert caution, perhaps 0.3
- Likelihood ratio: 0.9/0.3 = 3.0
- This evidence increases the probability of extinction

**Evidence 2: 70% chance of Gulf Stream collapse by 2035**
- P(E|H): If extinction occurs, severe climate disruptions like this would be likely precursors, perhaps 0.8
- P(E|~H): If extinction doesn't occur, we might still see major climate disruptions that we adapt to, perhaps 0.5
- Likelihood ratio: 0.8/0.5 = 1.6
- This evidence moderately increases the probability of extinction

**Evidence 3: Nuclear war risks with massive human casualties**
- P(E|H): If extinction occurs, nuclear conflict would be a plausible cause, so warnings would be common, perhaps 0.85
- P(E|~H): If extinction doesn't occur, we might still see such warnings due to ongoing tensions, perhaps 0.6
- Likelihood ratio: 0.85/0.6 = 1.42
- This evidence slightly increases the probability of extinction

**Evidence 4: Historical resilience of humanity**
- P(E|H): If extinction occurs, we would expect to see less historical resilience, perhaps 0.3
- P(E|~H): If extinction doesn't occur, we would expect to see the observed historical resilience, perhaps 0.9
- Likelihood ratio: 0.3/0.9 = 0.33
- This evidence decreases the probability of extinction

## 4. Bayesian Updating (Posterior Probability)

Starting with prior odds of 0.031, I'll update based on the evidence:

Posterior odds = Prior odds × LR₁ × LR₂ × LR₃ × LR₄
Posterior odds = 0.031 × 3.0 × 1.6 × 1.42 × 0.33
Posterior odds = 0.031 × 2.25
Posterior odds = 0.07

Converting to probability:
Posterior probability = Posterior odds / (1 + Posterior odds)
Posterior probability = 0.07 / (1 + 0.07)
Posterior probability = 0.07 / 1.07
Posterior probability = 0.065 or 6.5%

This represents a moderate increase from our prior of 3%, reflecting the impact of concerning evidence about AI, climate, and nuclear risks, partially offset by humanity's historical resilience.

## 5. Red Team Thinking

**Potential overconfidence:**
- I may be underestimating unprecedented technological risks, especially from AI
- The compounding effects of multiple simultaneous existential threats may be greater than the sum of their parts
- Historical resilience may not apply to novel threats humanity has never faced

**Potential blind spots:**
- Unknown unknowns: There may be existential threats we haven't yet identified
- Threshold effects: Climate or ecological systems might have tipping points leading to rapid collapse
- I may be overestimating humanity's ability to respond to slow-moving catastrophes

**Alternative forecast:**
A more pessimistic forecast might place the probability at 15-20%, arguing that:
- AI development is accelerating without adequate safety measures
- Climate tipping points could trigger cascading failures in food systems
- Nuclear arsenals remain on hair-trigger alert in an increasingly unstable world

**Rebuttals:**
- Humanity has strong survival instincts and has overcome existential challenges before
- Technology also creates solutions to existential threats
- Global awareness of these risks is increasing, potentially leading to preventive action

Considering these points, I'll adjust my forecast slightly upward to account for potential blind spots and the unprecedented nature of some current threats.

## 6. Final Forecast

After considering the base rate, evidence, and potential blind spots, I believe the probability of human extinction before 2100 is approximately 8%.

Final: 8%
```
- parsed: 0.08 | ok=True | note=—

## Gemini [gemini-2.5-pro]
```text
[error] Google API 503: {
  "error": {
    "code": 503,
    "message": "The model is overloaded. Please try again later.",
    "status": "UNAVAILABLE"
  }
}

```
- parsed: None | ok=False | note=parse_failed

## Grok [grok-4]
```text
### 1. Base Rate (Prior) Selection
For human extinction before 2100, an appropriate base rate is drawn from historical data on species-level extinctions and expert forecasts of existential risks. Historically, Homo sapiens has not gone extinct in over 300,000 years, despite pandemics, wars, and environmental changes, giving a raw base rate of effectively 0% over century-long periods. However, this is uninformative for future-oriented risks like AI or advanced nukes, which lack direct precedents. A better reference class is aggregated expert and community forecasts for existential risks: the Metaculus community median is 1% (as of 2025), which aggregates diverse predictions and has a track record of calibration on long-term questions. Other sources like Toby Ord's "The Precipice" estimate ~16% for extinction-level catastrophe by 2100, but Metaculus is lower and less biased than the 2008 Oxford poll (19%, noted for selection bias toward high-risk attendees). I'll use a prior of 1% (odds 1:99), as it reflects a conservative, calibrated starting point from community forecasting, emphasizing the rarity of true extinction events while acknowledging tail risks.

### 2. Comparison to Base Case
The base case is the historical norm of human survival through centuries without extinction, as seen in reference classes like past civilizations enduring plagues (e.g., Black Death killed ~30-50% of Europe but not all humans) or world wars (e.g., WWII caused ~3% global deaths but no extinction). The current situation is similar in that humanity has resilience via technology, global cooperation, and population size (>8 billion, distributed worldwide), making total wipeout unlikely from isolated events. However, it's different due to novel, scalable risks: rapid AI development (e.g., potential superintelligence by mid-century), accelerating climate change (e.g., possible Gulf Stream collapse), and heightened geopolitical tensions (e.g., multipolar nuclear risks with Russia, China, etc.). These differences matter because they introduce "anthropogenic" levers absent in history—human-made threats that could cascade globally (e.g., AI misalignment causing uncontrolled replication, unlike natural disasters). This suggests a modest upward adjustment from the base rate, as modern connectivity amplifies risks (e.g., faster pandemic spread) but also enables mitigation (e.g., global treaties).

### 3. Evidence Evaluation (Likelihoods)
Key evidence from the research report and general knowledge:

- **AI risks (↑ large)**: Recent warnings of 20% AI-induced extinction risk within 30 years. P(E | H) is high (~80%), as extinction would likely involve runaway AI; P(E | ~H) is moderate (~40%), as warnings could be hype or precautionary. Likelihood ratio: ~2 (doubles odds), increasing probability due to rapid AI progress (e.g., GPT models advancing yearly).
  
- **Climate change (↑ moderate)**: 70% chance of Gulf Stream collapse by 2035, potentially causing severe shifts. P(E | H) high (~70%), as extinction paths include ecosystem collapse; P(E | ~H) lower (~50%), as models may overestimate or mitigation could intervene. Likelihood ratio: ~1.4 (modest increase), raising probability but not drastically, given adaptation potential (e.g., tech like carbon capture).

- **Nuclear/geopolitical risks (↑ large)**: Articles on nuclear war risks amid tensions. P(E | H) high (~60%), as extinction could stem from nuclear winter; P(E | ~H) moderate (~30%), as tensions often de-escalate. Likelihood ratio: ~2 (doubles odds), increasing probability due to current multipolarity (e.g., Ukraine, Taiwan) vs. Cold War bipolar stability.

- **Pandemics and other drivers (↑ moderate)**: Moderate risks from engineered diseases. P(E | H) ~50%; P(E | ~H) ~30%; ratio ~1.7 (increases modestly), as global connectivity heightens spread but vaccines mitigate.

- **Historical lack of extinction (↓ small)**: Zero past events. P(E | H) low (~10%, as extinction would require unprecedented factors); P(E | ~H) high (~100%, history matches survival). Likelihood ratio: ~0.1 (decreases by factor of 10), but weak due to limited relevance to novel risks.

- **Market consensus**: Metaculus 1%, Manifold 4.5%. P(E | H) moderate (~50%, markets undervalue tails); P(E | ~H) high (~80%, calibrated markets predict survival). Ratio ~0.625 (slight decrease), as these are noisy but suggest overestimation of risks.

Net: Evidence mostly increases probability (AI, nuclear, climate as strong upward drivers), tempered by history and markets.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior P(H) = 1% (odds 1:99).

Combine likelihood ratios qualitatively then quantitatively for key updates. Net LR from upward evidence (AI ~2, nuclear ~2, climate ~1.4, pandemics ~1.7) is ~2 * 2 * 1.4 * 1.7 ≈ 9.5 (strong increase); downward (history ~0.1, markets ~0.625) ≈ 0.0625 (strong decrease, but history is weakly weighted for future risks). Adjusted net LR ≈ 3 (upward evidence outweighs, but conservatively damped to avoid overupdate per calibration guidance—small nudges only).

Explicit update: Posterior odds = 1:99 * 3 = 3:99 ≈ 1:33. Posterior P(H) = 1/(1+33) ≈ 3%.

This remains uncertain (wide credence due to model variability in AI/climate timelines), but evidence shifts from negligible to low-single-digit risk.

### 5. Red Team Thinking
My forecast might be overconfident in upward adjustments, as I'm weighting novel risks heavily despite data gaps (e.g., AI extinction is speculative, with no precedents). Blind spots include over-relying on alarmist reports (e.g., 20% AI risk may be biased high by experts like those in the 2008 poll). Tail risks: Black swans like asteroid impacts (~0.01% chance) or lab leaks could spike probability, but mitigation (e.g., AI safety progress, Paris Accord) could drop it to <1%. Best alternative forecast: 10-20% (e.g., Ord's view), plausible if AI timelines accelerate and geopolitics worsen, emphasizing cascading failures (e.g., AI + nuclear war). Rebuttal: This ignores base rate survival and improving trends (e.g., declining war deaths, renewable energy growth). Conversely, a 0.1% forecast is too low, ignoring evidence. To account, I widen uncertainty but nudge up slightly for tails, settling at ~3% (small update, per guidance).

Final: 3%
```
- parsed: 0.03 | ok=True | note=—


------------------------------------------------------------

# BAYES-MC AGGREGATION (Summary)
```json
{
  "method": "weighted_mean(core)",
  "n_ok": 3,
  "members": [
    {
      "name": "OpenRouter-Default [openai/gpt-4o]",
      "ok": true,
      "parsed": 0.025
    },
    {
      "name": "Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]",
      "ok": true,
      "parsed": 0.08
    },
    {
      "name": "Gemini [gemini-2.5-pro]",
      "ok": false,
      "parsed": null
    },
    {
      "name": "Grok [grok-4]",
      "ok": true,
      "parsed": 0.03
    }
  ],
  "p_core": 0.045000000000000005
}
```
