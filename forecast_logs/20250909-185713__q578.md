
------------------------------------------------------------

‚ùì Question: Will humans go extinct before 2100?
üîó URL: https://www.metaculus.com/questions/578/

üìù Full Reasoning:
# SUMMARY
*Question*: Will humans go extinct before 2100?
*Final Prediction*: 3.38%

------------------------------------------------------------

# RESEARCH
## Research Report
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ‚Üë ‚Äî AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] ‚Üë ‚Äî Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] ‚Üë ‚Äî Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large ‚Äî Potential for AI to surpass human control.
- **Nuclear Conflict**: Large ‚Äî Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate ‚Äî Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate ‚Äî New diseases could emerge with high mortality rates.
- **Technological Advances**: Small ‚Äî Could mitigate or exacerbate risks.

### Differences vs. the base rate (what‚Äôs unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - ‚Üë AI risk (large)
  - ‚Üë Climate change impacts (moderate)
  - ‚Üë Nuclear tensions (large)
  - ‚Üì Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 18:57:13 +0300 (Similarity threshold ‚â•0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? ‚Äî binary ‚Äî 1.0% YES ‚Äî [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] ‚Äî binary ‚Äî 4.5% YES ‚Äî [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

# GTMC1 DETAILS
GTMC1 activated: No (not a strategic question or actor extraction failed).

------------------------------------------------------------

# ENSEMBLE OUTPUTS (Raw ‚Üí Parsed)
## OpenRouter-Default [openai/gpt-4o]
```text
To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach.

1. **Base Rate (Prior) Selection**
   - The historical base rate for human extinction is effectively zero, as there have been no known extinction events in human history. However, expert estimates, albeit with selection bias, suggest a 19% chance by 2100. Given the lack of historical precedent, a more conservative prior of 1% is reasonable, reflecting the extremely low frequency of such events historically.

2. **Comparison to Base Case**
   - Similarities: The current situation shares the baseline of no historical human extinction events.
   - Differences: Recent developments in AI, climate change, and geopolitical tensions introduce new risks not present in historical contexts. These factors necessitate an upward adjustment from the historical base rate.

3. **Evidence Evaluation (Likelihoods)**
   - **AI Risk**: AI experts warn of a 20% chance of AI-induced extinction within 30 years. This significantly increases the likelihood of extinction compared to the base case.
   - **Climate Change**: A 70% chance of Gulf Stream collapse by 2035 could lead to severe climate shifts, moderately increasing extinction risk.
   - **Nuclear Conflict**: Increased geopolitical tensions and potential for nuclear war also raise the likelihood of extinction.
   - **Historical Extinction Events**: The absence of past extinction events slightly decreases the likelihood compared to the base case.

   The likelihood ratios for AI risk and nuclear conflict are large, while climate change is moderate. The historical absence of extinction events provides a small counterbalance.

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds: 1% probability translates to odds of 1:99.
   - Considering the evidence, particularly the large likelihood ratios for AI and nuclear risks, the posterior odds increase significantly.
   - Posterior probability calculation:
     - Assume a combined likelihood ratio of 5 (qualitative estimate based on evidence strength).
     - Posterior odds = 1:99 √ó 5 = 5:99.
     - Posterior probability = 5 / (5 + 99) ‚âà 4.8%.

5. **Red Team Thinking**
   - Overconfidence risk: The forecast might overestimate the impact of AI and nuclear risks due to uncertainty in timelines and geopolitical developments.
   - Alternative scenarios: Successful global governance in AI and disarmament treaties could significantly reduce risks.
   - Adjustments: Consider a slightly lower probability to account for potential mitigating factors and uncertainties.

6. **Final Forecast**
   - After considering the base rate, evidence, and potential mitigating factors, the final calibrated probability of human extinction before 2100 is adjusted to reflect a moderate increase from the prior.

Final: 3%
```
- parsed: 0.03 | ok=True | note=‚Äî

## Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```text
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. This is challenging since human extinction has never occurred, so I'll consider several reference points:

- Historical data: Humans have existed for roughly 300,000 years without extinction, suggesting an extremely low base rate.
- Expert surveys: The Oxford Global Catastrophic Risk Conference yielded a 19% median probability, but this likely suffers from selection bias.
- Metaculus community prediction: Currently at 1.0%
- Manifold market: Currently at 4.5%

Given these reference points and acknowledging the selection bias in the Oxford survey, I'll use a prior of 3%. This is higher than the Metaculus prediction but lower than the Manifold market, reflecting both the historical resilience of humanity and the genuine existential risks we face.

Prior probability (P(H)) = 3% or 0.03
Prior odds = 0.03/(1-0.03) = 0.031

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans have demonstrated remarkable resilience and adaptability throughout history
- Previous existential threats (Cold War nuclear tensions, pandemics) did not lead to extinction
- Global cooperation mechanisms exist for addressing shared threats

**Differences from base case:**
- Unprecedented technological capabilities, particularly in AI, create novel extinction risks
- Climate change is occurring at a rate not seen in human history
- Modern weapons systems have greater destructive potential than ever before
- Global interconnectedness increases vulnerability to cascading failures
- Multiple existential threats could interact and compound each other

These differences suggest the need for an upward adjustment from historical base rates.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence 1: AI experts warning of 20% chance of AI-induced extinction within 30 years**
- P(E|H): If human extinction occurs, there's a high probability (~0.7) we would see such warnings
- P(E|~H): If human extinction doesn't occur, there's still a moderate probability (~0.3) of such warnings due to expert caution
- Likelihood ratio: 0.7/0.3 = 2.33
- This evidence increases the probability of extinction

**Evidence 2: 70% chance of Gulf Stream collapse by 2035**
- P(E|H): If human extinction occurs, there's a moderate probability (~0.5) of such climate tipping points
- P(E|~H): If human extinction doesn't occur, there's still a significant probability (~0.4) of such climate events (without causing extinction)
- Likelihood ratio: 0.5/0.4 = 1.25
- This evidence slightly increases the probability of extinction

**Evidence 3: Nuclear war risks with massive human casualties**
- P(E|H): If human extinction occurs, there's a high probability (~0.6) of nuclear conflict concerns
- P(E|~H): If human extinction doesn't occur, there's still a significant probability (~0.5) of such concerns
- Likelihood ratio: 0.6/0.5 = 1.2
- This evidence slightly increases the probability of extinction

**Evidence 4: Historical resilience of humanity**
- P(E|H): If human extinction occurs, there's a low probability (~0.2) we would observe historical resilience
- P(E|~H): If human extinction doesn't occur, there's a high probability (~0.8) of such resilience
- Likelihood ratio: 0.2/0.8 = 0.25
- This evidence decreases the probability of extinction

## 4. Bayesian Updating (Posterior Probability)

Starting with the prior odds of 0.031, I'll update based on the evidence:

Update for Evidence 1 (AI risk):
- Posterior odds = 0.031 √ó 2.33 = 0.072
- Posterior probability = 0.072/(1+0.072) = 0.067 or 6.7%

Update for Evidence 2 (Climate tipping points):
- Posterior odds = 0.072 √ó 1.25 = 0.09
- Posterior probability = 0.09/(1+0.09) = 0.083 or 8.3%

Update for Evidence 3 (Nuclear risks):
- Posterior odds = 0.09 √ó 1.2 = 0.108
- Posterior probability = 0.108/(1+0.108) = 0.097 or 9.7%

Update for Evidence 4 (Historical resilience):
- Posterior odds = 0.108 √ó 0.25 = 0.027
- Posterior probability = 0.027/(1+0.027) = 0.026 or 2.6%

After considering all evidence, the posterior probability is approximately 2.6%.

## 5. Red Team Thinking

**Potential blind spots:**
- I may be underestimating the compounding effects of multiple simultaneous crises
- The likelihood ratios for AI risk might be too conservative given unprecedented technological development
- I might be overweighting historical resilience in a fundamentally different technological landscape
- Unknown unknowns (risks we haven't identified yet) aren't adequately factored in
- The market consensus may be anchored to status quo bias

**Alternative forecast:**
A reasonable alternative forecast might be closer to 5-7%, arguing that:
- AI development is accelerating with inadequate safety measures
- Climate tipping points could trigger cascading failures in food systems
- Nuclear arsenals remain on high alert with increasing geopolitical tensions
- Pandemic risks are elevated due to increased human-animal contact and lab research

**Rebuttals to alternative forecast:**
- Humans have strong survival instincts and would likely take drastic measures before extinction
- Technological development also creates new solutions to existential threats
- Geographic dispersion of humanity provides redundancy against many catastrophes
- The definition of extinction requires ALL humans to die, which is an extremely high bar

Considering these points, I'll adjust my forecast slightly upward to 3.5% to account for potential underestimation of novel risks and compounding effects.

## 6. Final Forecast

Final: 3.5%
```
- parsed: 0.035 | ok=True | note=‚Äî

## Gemini [gemini-2.5-pro]
```text
Here is a probabilistic forecast following the specified methodology.

### 1. Base Rate (Prior) Selection
The event in question‚Äîcomplete human extinction‚Äîis unprecedented. A direct historical frequency is zero, which is not a useful prior. A better approach is to use a proxy for a "low-information" but reasoned starting point. The Metaculus community forecast, which aggregates the predictions of many forecasters, serves as an excellent proxy for such a prior. It reflects a collective judgment that the risk is small but non-zero.

- **Base Rate (Prior):** I will set the prior probability P(H) at 1%.
- **Reasoning:** This aligns with the long-term Metaculus community prediction, which represents a well-regarded "wisdom of the crowds" estimate. It acknowledges that while extinction is an extreme event, there are recognized, non-trivial risks in the modern world.
- **Initial Odds:** A 1% probability corresponds to prior odds of 1 to 99 (1:99).

### 2. Comparison to Base Case
The base case, reflected in the 1% prior, assumes a certain level of background risk from known threats like nuclear war and natural pandemics, similar to what was experienced in the late 20th century.

- **Similarities:** The risk of great power conflict and nuclear exchange remains, as does the potential for natural pandemics.
- **Differences:** The current situation differs from the historical base case in several critical ways that increase the risk:
    1.  **Artificial Intelligence (AI):** The rapid, accelerating development of powerful AI systems introduces a novel existential risk category that was purely theoretical in the past. The potential for loss of control or misuse is a significant new factor.
    2.  **Accelerating Climate Change:** We are moving from theoretical models of climate change to observing real-world, potentially non-linear effects like the destabilization of the Gulf Stream. This increases the probability of cascading failures in global systems (food, political stability).
    3.  **Increased Geopolitical Instability:** The shift from a bipolar (Cold War) to a multipolar world order may increase the number of potential flashpoints and the risk of miscalculation leading to a global conflict.

These differences suggest that the historical baseline of risk is no longer adequate and an upward adjustment from the prior is warranted.

### 3. Evidence Evaluation (Likelihoods)
I will evaluate the key new pieces of information from the research report. Let H be the hypothesis "humans go extinct by 2100."

-   **Evidence 1: AI experts warn of a 20% chance of AI-induced extinction within 30 years.**
    -   This is a very strong piece of evidence. Such a stark warning from domain experts is far more likely if the risk is real (P(E|H) is high) than if it's a false alarm (P(E|~H) is lower). While expert opinion can be wrong, this points to a specific, potent mechanism for extinction.
    -   **Likelihood Ratio:** Significantly greater than 1. This evidence strongly increases the probability.

-   **Evidence 2: Study shows a 70% chance of Gulf Stream collapse by 2035.**
    -   This points to a high probability of a major climate catastrophe that could trigger famines, mass migrations, and state collapse, creating conditions ripe for global conflict. This is a plausible pathway to extinction. The existence of such a high-probability tipping point is more likely if we are on a trajectory towards extinction (H) than if we are not (~H).
    -   **Likelihood Ratio:** Greater than 1. This evidence increases the probability.

-   **Evidence 3: Manifold market forecast of 4.5%.**
    -   While play-money markets can be noisy, a forecast of 4.5% is a signal that a segment of the forecasting community weighs the recent evidence more heavily than the Metaculus aggregate. It provides a quantitative anchor that is significantly higher than our prior.
    -   **Likelihood Ratio:** This evidence suggests the probability is higher than our 1% prior.

The combined weight of these new pieces of evidence‚Äîa novel and potent technological risk (AI), an accelerating environmental risk (climate), and market sentiment‚Äîpoints to a substantial increase in the overall probability of extinction.

### 4. Bayesian Updating (Posterior Probability)
I will now update the prior odds based on the combined weight of the evidence.

-   **Prior Odds:** 1:99
-   **Likelihood Ratio Estimation:** The evidence presented is strong. The AI and climate risks are not just theoretical but are now backed by expert consensus and specific scientific warnings. Combined, this new information makes the extinction hypothesis (H) at least 2 to 4 times more likely than the non-extinction hypothesis (~H), compared to our starting belief. I will use a conservative combined likelihood ratio of 3.
-   **Update Step:**
    -   Posterior Odds = Prior Odds √ó Likelihood Ratio
    -   Posterior Odds = (1:99) √ó 3 = 3:99 = 1:33
-   **Posterior Probability:**
    -   Probability = Odds / (1 + Odds)
    -   Probability = 1 / (1 + 33) = 1/34 ‚âà 2.94%

This calculation suggests the probability is around 3%. This new estimate integrates the low base rate with the significant and alarming new evidence.

### 5. Red Team Thinking
-   **Why might this forecast be too high?** The primary counterargument is human resilience and ingenuity. We have a track record of identifying threats and developing countermeasures (e.g., avoiding nuclear war during the Cold War, developing vaccines). The bar for *complete* extinction is extremely high; civilizational collapse is a more probable outcome than the death of the last human. The 1% Metaculus forecast represents a strong, stable anchor from a well-calibrated community, and deviating significantly from it is risky.
-   **Why might this forecast be too low?** I may be underestimating the correlated nature of these risks. A climate catastrophe could directly lead to a nuclear exchange over scarce resources. The development of AGI could happen much faster than anticipated, leaving no time for safety measures. My chosen likelihood ratio of 3 might be too conservative given the gravity of the expert warnings.
-   **Conclusion:** The 3% forecast strikes a balance. It represents a significant update from the 1% prior, taking the new evidence seriously. However, it remains a small absolute number, acknowledging the immense difficulty of achieving complete extinction and respecting the arguments for human resilience. It sits between the low Metaculus anchor and the higher Manifold signal.

### 6. Final Forecast
After weighing the low prior probability against the strong evidence of new and accelerating risks from AI and climate change, I have updated my estimate. The confluence of these threats represents a marked departure from the historical risk environment.

Final: 3%
```
- parsed: 0.03 | ok=True | note=‚Äî

## Grok [grok-4]
```text
### 1. Base Rate (Prior) Selection
For human extinction before 2100, an appropriate base rate is drawn from historical data on species survival and expert consensus on existential risks. Historically, homo sapiens have existed for about 300,000 years without extinction, and no extinction-level events (e.g., asteroid impacts, supervolcanic eruptions, or global wars) have wiped us out in recorded history. The base rate for true extinction events in similar intelligent species is effectively zero over comparable timescales, but this is limited by the anthropic principle (we wouldn't be here to observe it if it had happened). Expert estimates, adjusted for bias, provide a better reference: the 2008 Oxford poll gave 19%, but with noted selection bias toward high-risk views. More reliable are prediction markets like Metaculus (1%) and Manifold (4.5%), which aggregate diverse, incentivized opinions and tend to be well-calibrated. I'll use a prior of 2% (or odds of 1:49), splitting the difference between these markets and acknowledging very low historical rates while incorporating some expert concern for tail risks. This is conservative, as pure historical data would suggest closer to 0%, but existential risk literature (e.g., Toby Ord's estimates around 16% for the century) justifies a small non-zero prior.

### 2. Comparison to Base Case
The base case is the historical norm: humanity persisting through millennia despite pandemics (e.g., Black Death killed ~50% of Europe but not globally), wars (e.g., World Wars caused massive deaths but not extinction), and environmental shifts (e.g., ice ages). The current situation is similar in that no immediate extinction trigger is active‚Äîgeopolitical tensions exist but haven't escalated to total war, and technology has historically mitigated risks (e.g., vaccines for pandemics). However, differences amplify risks: rapid AI development introduces novel, uncontrolled existential threats absent in history; accelerating climate change (e.g., potential Gulf Stream collapse) creates feedback loops not seen in past eras; and global connectivity heightens pandemic and conflict spread. These differences matter because they shorten timelines for cascading failures‚Äîunlike slow historical changes, modern risks could unfold in decades, potentially overwhelming adaptation. This suggests an upward adjustment from the base rate, but not drastically, as human resilience (e.g., via technology and cooperation) remains a countervailing force.

### 3. Evidence Evaluation (Likelihoods)
Key pieces of evidence from the research report and background:

- **AI risks (‚Üë large)**: Warnings of 20% AI-induced extinction in 30 years. P(E | H) is high (~80%) because uncontrolled AI could directly cause extinction (e.g., misaligned superintelligence). P(E | ~H) is moderate (~40%) as such warnings could be hype or precautionary without true risk. Likelihood ratio: ~2 (80%/40%), increasing probability moderately‚Äîit's alarming but speculative.

- **Climate change developments (‚Üë moderate, e.g., 70% Gulf Stream collapse by 2035)**: P(E | H) high (~70%) as severe shifts could lead to ecosystem collapse and famine. P(E | ~H) lower (~30%) since models might overestimate or mitigation could intervene. Likelihood ratio: ~2.3 (70%/30%), increasing probability‚Äîevidence of acceleration is stronger than historical climate variability.

- **Nuclear/geopolitical tensions (‚Üë large)**: Recent articles on risks. P(E | H) high (~60%) as escalation could cause nuclear winter. P(E | ~H) moderate (~30%) due to deterrence and diplomacy. Likelihood ratio: ~2 (60%/30%), increasing probability‚Äîcurrent multipolarity differs from Cold War stability.

- **Historical lack of extinction (‚Üì small)**: No past events. P(E | H) low (~10%) if extinction were imminent, we'd expect precursors; P(E | ~H) very high (~99%) as survival is the norm. Likelihood ratio: ~0.1 (10%/99%), decreasing probability slightly‚Äîit's reassuring but doesn't negate new risks.

- **Pandemics and tech advances (moderate/small)**: Balanced; global connectivity raises spread risk (upward), but tech mitigation (e.g., vaccines) counters it (downward). Net likelihood ratio ~1.2, slight increase.

Overall, evidence leans upward with a combined likelihood ratio of ~3-4 (product of ratios, qualitatively), driven by novel risks outweighing historical stability.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior probability P(H) = 2% (prior odds = 0.02 / 0.98 ‚âà 1:49).

Group evidence into a net likelihood ratio (LR) of ~3.5 (from above: AI ~2, climate ~2.3, nuclear ~2, historical ~0.1, net multiplicative ~3.5 after balancing).

Posterior odds = Prior odds √ó LR = (1:49) √ó 3.5 ‚âà 3.5:49 ‚âà 1:14.

Posterior probability = 1 / (1 + 14) ‚âà 6.7%.

For an explicit step: Update first with AI/climate/nuclear cluster (LR ~3): Interim odds = 1:49 √ó 3 ‚âà 3:49 ‚âà 1:16.3; interim P ‚âà 5.8%. Then adjust for historical stability (LR ~0.8, slight down): Final odds ‚âà 1:16.3 √ó 0.8 ‚âà 1:20.4; P ‚âà 4.7%. This remains uncertain due to wide variance in expert views and model imprecision‚Äîconfidence interval roughly 2-10%, reflecting noisy evidence.

### 5. Red Team Thinking
My forecast might be overconfident in upward adjustments, as prediction markets (1-4.5%) are lower, possibly better-calibrated than my qualitative LR estimates; I could be overweighting sensational recent developments (recency bias) while underweighting human adaptability (e.g., past near-misses like the Cuban Missile Crisis resolved peacefully). Tail risks include black-swan events like asteroid impacts (low probability but high impact, perhaps adding 0.5% uncaptured) or positive surprises like AI safety breakthroughs reducing risks. An alternative forecast of 1% (per Metaculus) is plausible if evidence is mostly noise‚Äîrebuttal: recent specifics (e.g., Gulf Stream study) provide genuine updates beyond base rates. Conversely, a 10% forecast (closer to biased polls) might overfit to worst-case scenarios; rebuttal: markets incorporate debiasing. To account, I'll nudge down slightly for overconfidence, widening implied uncertainty to ensure tails are covered (e.g., 1% chance of much higher risk if AI timelines accelerate unexpectedly).

Final: 4%
```
- parsed: 0.04 | ok=True | note=‚Äî


------------------------------------------------------------

# BAYES-MC AGGREGATION (Summary)
```json
{}
```
