📊 Forecast Run 20250905-103906
Timestamp: 2025-09-05 10:39:06 +0300
============================================================

------------------------------------------------------------

❓ Question: Will humans go extinct before 2100?
🔗 URL: https://www.metaculus.com/questions/578/

📝 Full Reasoning:
# SUMMARY
*Question*: Will humans go extinct before 2100?
*Final Prediction*: 0.66%

------------------------------------------------------------

# RESEARCH
## Research Report
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rates for global catastrophic events are extremely low, with no human extinction events recorded. Limitations include the unprecedented nature of modern risks.
- **Nuclear War**: While no full-scale nuclear war has occurred, the risk is estimated at low single-digit percentages per year by some experts.
- **Climate Change**: Severe climate impacts are increasing, but extinction-level events remain speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ↑ — AMOC collapse risk (70%) could trigger severe climate disruptions.
- [2025-09-05] ↑ — Climate change increases extreme weather, raising existential risk.
- [2025-09-04] ↑ — Nuclear war remains a potential global catastrophe.

### Mechanisms & drivers (causal levers)
- **Climate Change**: Large driver; accelerates extreme weather and ecological collapse.
- **Nuclear Conflict**: Large driver; potential for immediate and long-term global impacts.
- **Technological Risks**: Moderate driver; includes AI and biotechnology risks.
- **Geopolitical Instability**: Moderate driver; increases likelihood of conflict.
- **Pandemics**: Moderate driver; potential for widespread mortality.

### Differences vs. the base rate (what’s unusual now)
- **Technological Advancements**: Rapid AI and biotech developments introduce new existential risks.
- **Climate Tipping Points**: Accelerating climate change could lead to unforeseen catastrophic outcomes.
- **Geopolitical Tensions**: Increased global instability heightens risk of conflict.
- **Global Interconnectedness**: Amplifies the impact of localized disasters.

### Bayesian update sketch (for the statistician)
- **Prior**: Low probability of extinction by 2100, equivalent n = 10.
- **Evidence mapping**:
  - ↑ Large: AMOC collapse risk.
  - ↑ Moderate: Increased nuclear tensions.
  - ↑ Moderate: Accelerating climate change impacts.
  - ↓ Small: Global efforts in disarmament and climate mitigation.
- **Net effect**: Posterior probability should increase moderately due to heightened risks.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in geopolitical conflicts.
  - New climate tipping points identified.
  - Advances in potentially dangerous technologies.
- **DOWN indicators**:
  - Successful global disarmament treaties.
  - Significant climate policy advancements.
  - Breakthroughs in risk mitigation technologies.

### Caveats & pitfalls
- **Uncertainty in Predictions**: Long-term forecasts are inherently uncertain.
- **Data Gaps**: Limited historical data on extinction-level events.
- **Deception Risks**: Potential for misinformation in geopolitical contexts.
- **Regime Changes**: Political shifts can alter risk landscapes.
- **Definitional Gotchas**: Ambiguities in defining "human extinction."

**Final Research Summary**: Recent developments in climate and geopolitical risks suggest a moderate increase in the probability of human extinction by 2100, though the base rate remains low. Forecasters should monitor geopolitical tensions and climate tipping points closely.

---
### Market Consensus Snapshot
_Captured 2025-09-05 10:39:23 +0300 (Similarity threshold ≥0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? — binary — 1.0% YES — [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- No sufficiently similar market found.

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

# GTMC1 DETAILS
GTMC1 activated: No (not a strategic question or actor extraction failed).

------------------------------------------------------------

# BAYES-MC DETAILS
{
  "mean": 0.05971136602326882,
  "p10": 1.383220857758934e-06,
  "p50": 0.006614149843666503,
  "p90": 0.19583641590293357,
  "posterior_alpha": 0.19,
  "posterior_beta": 3.0100000000000002
}

------------------------------------------------------------

# MODEL REASONING (RAW)

## OpenRouter-Default [openai/gpt-4o] Reasoning
```
To forecast the probability of human extinction before 2100, we will follow a structured Bayesian approach.

1. **Base Rate (Prior) Selection**
   - The historical base rate for human extinction is effectively 0%, as no extinction events have been recorded. However, given the unprecedented nature of modern risks, we should consider a non-zero prior. The informal poll at the Global Catastrophic Risk Conference suggested a 19% probability, but this is likely biased. A more conservative base rate might be around 1%, reflecting the Metaculus community forecast and the absence of historical precedent.

2. **Comparison to Base Case**
   - Similarities: The base case assumes low probability due to historical non-occurrence of extinction events.
   - Differences: Current technological, geopolitical, and climate-related risks are unprecedented and could increase the probability of extinction.

3. **Evidence Evaluation (Likelihoods)**
   - **AMOC Collapse Risk (70%)**: This significantly increases the probability of severe climate disruptions, which could contribute to existential risk. Likelihood ratio > 1.
   - **Increased Nuclear Tensions**: While no nuclear war has occurred, the potential for catastrophic outcomes increases the risk. Likelihood ratio > 1.
   - **Accelerating Climate Change**: Increases the probability of extreme weather and ecological collapse. Likelihood ratio > 1.
   - **Global Efforts in Disarmament and Climate Mitigation**: These efforts slightly decrease the probability. Likelihood ratio < 1.

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds: 1% probability translates to odds of 0.01.
   - Evidence suggests an increase in risk due to AMOC collapse, nuclear tensions, and climate change.
   - Posterior odds = Prior odds × Likelihood ratio (net effect > 1).
   - Assuming a moderate increase, let's estimate the posterior probability at around 3%.

5. **Red Team Thinking**
   - Overconfidence risk: Given the high uncertainty and unprecedented nature of risks, we should be cautious about underestimating the probability.
   - Alternative scenario: Significant global cooperation and technological advancements could mitigate risks more effectively than anticipated.
   - Adjustments: Consider a slight increase to account for potential underestimation of risks.

6. **Final Forecast**
   - Considering the base rate, evidence, and potential biases, the final probability of human extinction before 2100 is estimated at 3%.

Final: 3%
```

## Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet] Reasoning
```
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to identify an appropriate base rate. There are several reference points:

- The Oxford Global Catastrophic Risk Conference informal poll yielded a median probability of 19%, but this likely suffers from selection bias.
- The Metaculus community forecast is currently at 1.0%.
- Historical data shows zero human extinction events in our ~300,000 year history as a species.

Given these considerations, I'll use the Metaculus community prediction of 1.0% as my base rate. This represents an aggregated judgment from a diverse group of forecasters who are incentivized for accuracy, and it avoids the selection bias of the Oxford conference. The Metaculus prediction also incorporates both historical data (zero extinctions so far) and forward-looking assessments of novel risks.

Prior probability P(H) = 1.0% or 1:99 odds.

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans have demonstrated remarkable resilience and adaptability throughout history.
- Global coordination mechanisms exist for addressing existential threats.
- Technological advancement continues to provide tools for mitigating various risks.

**Differences from base case:**
- Unprecedented technological capabilities introduce novel extinction risks (AI, biotechnology).
- Climate change is accelerating with potential tipping points like AMOC collapse (70% risk noted).
- Nuclear arsenals remain substantial despite reductions since Cold War peak.
- Global interconnectedness means localized disasters can have cascading global effects.
- Geopolitical tensions appear to be increasing rather than decreasing.

These differences suggest the need for some upward adjustment from the base rate, as we face novel risks without historical precedents.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence 1: AMOC collapse risk (70%)**
- P(E|H): If humans go extinct, severe climate disruptions like AMOC collapse are very likely to be involved, so ~0.9
- P(E|~H): If humans don't go extinct, AMOC collapse could still occur but humans would adapt, so ~0.6
- Likelihood ratio: 0.9/0.6 = 1.5
- This moderately increases the probability of extinction.

**Evidence 2: Accelerating climate change impacts**
- P(E|H): In extinction scenarios, severe climate impacts are almost certain, so ~0.95
- P(E|~H): Climate impacts will occur regardless, but not lead to extinction, so ~0.9
- Likelihood ratio: 0.95/0.9 = 1.06
- This slightly increases the probability of extinction.

**Evidence 3: Nuclear war risk**
- P(E|H): Nuclear war would be a likely contributor to extinction, so ~0.8
- P(E|~H): Nuclear deterrence has worked so far and may continue to work, so ~0.3
- Likelihood ratio: 0.8/0.3 = 2.67
- This significantly increases the probability of extinction.

**Evidence 4: Technological risks (AI, biotech)**
- P(E|H): Novel technologies would likely play a role in extinction, so ~0.85
- P(E|~H): These technologies also provide solutions and may be controlled, so ~0.4
- Likelihood ratio: 0.85/0.4 = 2.13
- This significantly increases the probability of extinction.

**Evidence 5: Global efforts in risk mitigation**
- P(E|H): If extinction occurs, these efforts would have failed, so ~0.3
- P(E|~H): These efforts contribute to preventing extinction, so ~0.7
- Likelihood ratio: 0.3/0.7 = 0.43
- This decreases the probability of extinction.

## 4. Bayesian Updating (Posterior Probability)

Starting with prior odds of 1:99 (1%), I'll update based on the evidence:

Posterior odds = Prior odds × Combined likelihood ratio
Posterior odds = (1/99) × 1.5 × 1.06 × 2.67 × 2.13 × 0.43

Calculating step by step:
- Update for AMOC collapse: (1/99) × 1.5 = 1.5/99 = 0.0152 odds
- Update for climate impacts: 0.0152 × 1.06 = 0.0161 odds
- Update for nuclear risk: 0.0161 × 2.67 = 0.0430 odds
- Update for technological risks: 0.0430 × 2.13 = 0.0916 odds
- Update for mitigation efforts: 0.0916 × 0.43 = 0.0394 odds

Converting to probability:
Posterior probability = 0.0394 / (1 + 0.0394) = 0.0379 or approximately 3.8%

## 5. Red Team Thinking

**Potential overconfidence:**
- I may be underestimating unprecedented risks without historical analogues.
- The compounding effect of multiple simultaneous crises could be greater than the sum of individual risks.
- Unknown unknowns or "black swan" events aren't accounted for.

**Alternative scenarios:**
- A more pessimistic view might emphasize the unprecedented nature of technological risks and assign higher probabilities to extinction.
- A more optimistic view would emphasize human adaptability and the fact that we've survived all challenges so far.

**Adjustments needed:**
- Given the unprecedented nature of some risks and potential unknown unknowns, I should widen my uncertainty bounds.
- The Metaculus community prediction of 1% may already incorporate sophisticated reasoning about these factors.
- My update to 3.8% represents a significant increase from the base rate, which seems appropriate given the evidence.

After considering these factors, I'll adjust my forecast slightly upward to account for the possibility of unknown risks and compounding effects, landing at approximately 4%.

## 6. Final Forecast

Final: 4%
```

## Gemini [gemini-2.5-pro] Reasoning
```
[error] Google API 503: {
  "error": {
    "code": 503,
    "message": "The model is overloaded. Please try again later.",
    "status": "UNAVAILABLE"
  }
}

```

## Grok [grok-4] Reasoning
```
### 1. Base Rate (Prior) Selection
For the event of human extinction before 2100, an appropriate base rate comes from historical and expert-estimated frequencies of existential risks. Humans have existed for approximately 300,000 years with no extinction events, implying a per-century extinction rate near 0% based on raw history (e.g., over 3,000 centuries, zero extinctions). However, this is an underestimate because modern risks (e.g., nuclear weapons, AI, climate change) are unprecedented and weren't present in most of history. Expert estimates provide a better reference class: Toby Ord's "The Precipice" (2020) estimates a 1 in 6 (≈16.7%) chance this century, while Metaculus's community forecast (as of 2025) is 1%. The 2008 Oxford poll's 19% median is likely biased high due to selection effects among risk-focused attendees. Balancing these, I select a prior probability of 2% (or odds of 1:49), as it reflects a low historical base rate adjusted modestly for expert consensus on novel risks, avoiding extremes like the biased poll or pure zero-history.

### 2. Comparison to Base Case
The base case is the historical reference class of human survival over millennia, where extinction risks were limited to natural disasters, pandemics, or environmental changes, none of which caused total extinction. The current situation is similar in that humanity has resilience (e.g., global population >8 billion, technological adaptability), and no extinction has occurred despite past close calls (e.g., Black Death, 1918 flu). However, it's different due to anthropogenic risks: rapid AI/biotech advancements introduce uncontrolled "black swan" scenarios (e.g., misaligned superintelligence), climate tipping points like AMOC collapse could trigger cascading failures, and geopolitical tensions (e.g., nuclear risks) are heightened by modern weaponry and instability. These differences matter because they amplify low-probability, high-impact events, potentially shifting odds from near-zero to low-single digits by creating novel pathways to extinction that didn't exist in the base case.

### 3. Evidence Evaluation (Likelihoods)
- **AMOC collapse risk (70% per report)**: This evidence is more likely if extinction occurs (P(E|H) ≈ 0.8, as it could trigger severe climate disruptions leading to societal collapse), than if it doesn't (P(E|~H) ≈ 0.6, as mitigation might prevent full extinction). Likelihood ratio ≈ 1.33 (mildly supportive). This increases the probability.
- **Accelerating climate change and extreme weather**: More likely under extinction (P(E|H) ≈ 0.7, as it drives ecological collapse), vs. non-extinction (P(E|~H) ≈ 0.4, with adaptation possible). LR ≈ 1.75 (moderately supportive). Increases probability.
- **Nuclear war risks and geopolitical instability**: More likely under extinction (P(E|H) ≈ 0.6, via direct/global fallout), vs. non-extinction (P(E|~H) ≈ 0.3, as deterrence holds). LR ≈ 2.0 (moderately supportive). Increases probability.
- **Technological risks (AI/biotech) and pandemics**: More likely under extinction (P(E|H) ≈ 0.5, as they enable engineered threats), vs. non-extinction (P(E|~H) ≈ 0.3). LR ≈ 1.67 (mildly supportive). Increases probability.
- **Global efforts in disarmament/climate mitigation**: Slightly more likely under non-extinction (P(E|~H) ≈ 0.4 vs. P(E|H) ≈ 0.2). LR ≈ 0.5 (mildly opposing). Decreases probability slightly.

Overall, the evidence leans toward increasing the probability due to multiple upward drivers outweighing minor mitigations.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior probability P(H) = 2% (prior odds = 0.02 / 0.98 ≈ 1:49).

For an explicit update, combine the key upward evidence (climate/nuclear/tech risks) into a net LR ≈ 2.5 (qualitative aggregation: product of moderate LRs like 1.33 × 1.75 × 2.0 × 1.67 ≈ 7.5, tempered by the mitigating LR of 0.5 and conservatism to ≈2.5 to avoid overcounting correlated evidence).

Posterior odds = prior odds × LR = (1:49) × 2.5 ≈ 2.5:49 ≈ 1:19.6.

Posterior probability = 1 / (1 + 19.6) ≈ 4.9%.

After incorporating all evidence (including smaller factors), the net effect is a moderate upward adjustment. The resulting posterior is around 3%, reflecting increased risks but high uncertainty (e.g., wide error bars due to speculative long-term forecasts and data gaps). Confidence remains low; this isn't a "sure thing" but a calibrated nudge from the prior.

### 5. Red Team Thinking
My forecast might be overconfident in upward adjustments, as historical base rates are zero, and expert estimates like Metaculus's 1% could be more reliable aggregators than my synthesis—perhaps I'm overweighting recent "doomy" developments (e.g., AMOC hype) due to availability bias. Blind spots include underestimating human resilience (e.g., space colonization or AI safety breakthroughs as tail risks for survival). Alternative forecast: A more optimistic 1% (per Metaculus), plausible if mitigation efforts accelerate (e.g., global treaties succeed 80% of the time in similar historical analogs like ozone depletion). Rebuttal: This ignores compounding modern risks, which Ord argues make this century uniquely dangerous. For tail risks, extinction could spike via unforeseen chains (e.g., AI-triggered war, 1-2% chance), or drop to near-zero with breakthroughs (e.g., 5% chance of "utopia" tech). To account, I widen my implicit distribution and adjust down slightly for overconfidence, settling on a calibrated 2%.

Final: 2%
```