
------------------------------------------------------------

❓ Question: Will humans go extinct before 2100?
🔗 URL: https://www.metaculus.com/questions/578/

📝 Full Reasoning:
# SUMMARY
*Question*: Will humans go extinct before 2100?
*Final Prediction*: 6.50%

------------------------------------------------------------

# RESEARCH
## Research Report
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] ↑ — AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] ↑ — Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] ↑ — Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large — Potential for AI to surpass human control.
- **Nuclear Conflict**: Large — Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate — Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate — New diseases could emerge with high mortality rates.
- **Technological Advances**: Small — Could mitigate or exacerbate risks.

### Differences vs. the base rate (what’s unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - ↑ AI risk (large)
  - ↑ Climate change impacts (moderate)
  - ↑ Nuclear tensions (large)
  - ↓ Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 18:40:16 +0300 (Similarity threshold ≥0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? — binary — 1.0% YES — [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] — binary — 4.5% YES — [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

# GTMC1 DETAILS
GTMC1 activated: No (not a strategic question or actor extraction failed).

------------------------------------------------------------

# ENSEMBLE OUTPUTS (Raw → Parsed)
## OpenRouter-Default [openai/gpt-4o]
```text
To forecast the probability of human extinction before 2100, we will follow a structured Bayesian reasoning approach.

### 1. Base Rate (Prior) Selection
- **Base Rate**: Historically, the base rate for human extinction events is effectively zero, as humans have not gone extinct in recorded history. However, expert estimates, despite potential biases, suggest a higher risk. For a conservative prior, we might consider a very low probability, such as 1%, reflecting the historical absence of extinction events but acknowledging expert concerns.
- **Rationale**: The historical absence of extinction events provides a strong base rate, but expert opinions and recent developments suggest non-negligible risks.

### 2. Comparison to Base Case
- **Similarities**: The current situation shares the historical context of no prior human extinction events.
- **Differences**: Unlike the past, we now face unprecedented risks from AI, climate change, and nuclear tensions, which are not reflected in historical data.

### 3. Evidence Evaluation (Likelihoods)
- **AI Risk**: Recent warnings of a 20% chance of AI-induced extinction within 30 years increase the likelihood of extinction (P(E | H) > P(E | ~H)).
- **Climate Change**: A 70% chance of Gulf Stream collapse suggests severe climate impacts, moderately increasing extinction risk.
- **Nuclear Tensions**: Highlighted risks of nuclear war significantly increase the likelihood of extinction.
- **Historical Extinction Events**: The absence of past extinction events slightly decreases the likelihood of extinction.

### 4. Bayesian Updating (Posterior Probability)
- **Prior Odds**: 1% prior probability translates to odds of 1:99.
- **Likelihood Ratios**:
  - AI Risk: Strong evidence, likelihood ratio > 1.
  - Climate Change: Moderate evidence, likelihood ratio > 1.
  - Nuclear Tensions: Strong evidence, likelihood ratio > 1.
  - Historical Events: Weak evidence, likelihood ratio < 1.
- **Posterior Odds Calculation**: 
  - Adjusting for strong evidence from AI and nuclear risks, and moderate evidence from climate change, the posterior odds increase significantly.
  - Posterior Probability: After updating, the probability increases but remains below the expert estimate of 19% due to the low prior and uncertainties.

### 5. Red Team Thinking
- **Overconfidence Check**: The forecast should account for uncertainties in AI timelines and geopolitical changes.
- **Alternative Scenarios**: Consider breakthroughs in AI governance or climate mitigation as potential risk reducers.
- **Adjustments**: Given the uncertainties and potential mitigating factors, the probability should be adjusted downward from the expert estimate.

### 6. Final Forecast
- **Calibrated Probability**: Balancing the low historical base rate with recent evidence and uncertainties, a probability of around 5% seems reasonable, reflecting both the increased risks and the potential for mitigation.

Final: 5%
```
- parsed: 0.05 | ok=True | note=—

## Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```text
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. There are several reference points to consider:

- Historical data: Humans have existed for roughly 300,000 years with no extinction events, suggesting an extremely low base rate.
- Expert surveys: The Oxford Global Catastrophic Risk Conference yielded a 19% median probability, though this likely suffers from selection bias.
- Prediction markets: Metaculus community forecast is at 1.0%, while Manifold shows 4.5%.

Given these reference points, I'll use a base rate of 2%. This is higher than the historical rate (effectively 0%) but lower than potentially biased expert estimates. It aligns more closely with prediction market estimates, which aggregate diverse perspectives.

Prior probability: P(H) = 2% or 1:49 odds

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans have demonstrated remarkable resilience and adaptability throughout history
- We have survived previous existential threats (Cold War nuclear tensions, pandemics)
- Global cooperation mechanisms exist for addressing shared threats

**Differences from base case:**
- Unprecedented technological capabilities, especially in AI, create novel extinction risks
- Climate change is accelerating at rates not seen in human history
- Modern weapons systems have far greater destructive potential than historical precedents
- Global interconnectedness increases vulnerability to cascading failures
- Multiple existential threats could interact and compound (e.g., climate crisis triggering conflicts)

These differences suggest the need for some upward adjustment from historical base rates, which is already reflected in my 2% prior.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence 1: AI experts warning of 20% chance of AI-induced extinction within 30 years**
- P(E|H): If human extinction occurs, the probability of AI experts warning about it beforehand is very high (~0.9)
- P(E|~H): If human extinction doesn't occur, the probability of such warnings is moderate (~0.3) due to expert caution
- Likelihood ratio: 0.9/0.3 = 3.0
- This evidence increases the probability of extinction

**Evidence 2: 70% chance of Gulf Stream collapse by 2035**
- P(E|H): If human extinction occurs, the probability of major climate system collapses is high (~0.8)
- P(E|~H): If human extinction doesn't occur, such climate warnings could still be accurate but not extinction-level (~0.5)
- Likelihood ratio: 0.8/0.5 = 1.6
- This moderately increases the probability of extinction

**Evidence 3: Nuclear war risks with massive human casualties**
- P(E|H): If human extinction occurs, nuclear conflict is a plausible pathway (~0.7)
- P(E|~H): If human extinction doesn't occur, nuclear tensions may still exist but without civilization-ending conflict (~0.4)
- Likelihood ratio: 0.7/0.4 = 1.75
- This moderately increases the probability of extinction

**Evidence 4: Historical resilience of human species**
- P(E|H): If human extinction occurs, it would be despite historical resilience (~0.3)
- P(E|~H): If human extinction doesn't occur, human resilience would be a contributing factor (~0.9)
- Likelihood ratio: 0.3/0.9 = 0.33
- This significantly decreases the probability of extinction

## 4. Bayesian Updating (Posterior Probability)

Starting with prior odds of 1:49 (2%), I'll update based on the evidence:

**Update on AI expert warnings:**
- Prior odds: 1:49
- Likelihood ratio: 3.0
- Posterior odds: 1:49 × 3.0 = 3:49 = 3:49 ≈ 1:16.33
- Posterior probability: 3/(3+49) ≈ 5.8%

**Update on Gulf Stream collapse:**
- Prior odds: 1:16.33
- Likelihood ratio: 1.6
- Posterior odds: 1:16.33 × 1.6 = 1.6:16.33 ≈ 1:10.2
- Posterior probability: 1.6/(1.6+16.33) ≈ 8.9%

**Update on nuclear war risks:**
- Prior odds: 1:10.2
- Likelihood ratio: 1.75
- Posterior odds: 1:10.2 × 1.75 = 1.75:10.2 ≈ 1:5.83
- Posterior probability: 1.75/(1.75+10.2) ≈ 14.6%

**Update on historical resilience:**
- Prior odds: 1:5.83
- Likelihood ratio: 0.33
- Posterior odds: 1:5.83 × 0.33 = 0.33:5.83 ≈ 1:17.67
- Posterior probability: 0.33/(0.33+5.83) ≈ 5.4%

After these updates, the posterior probability is approximately 5.4%.

## 5. Red Team Thinking

**Potential blind spots:**
- I may be underestimating the compounding effects of multiple simultaneous crises
- The likelihood ratios for AI risk might be too conservative given unprecedented technological development
- I might be overweighting historical resilience in a fundamentally different technological landscape
- Unknown unknowns (e.g., physics experiments gone wrong, unforeseen technological risks) aren't adequately captured

**Alternative forecasts:**
- A higher probability (10-15%) could be justified by emphasizing the unprecedented nature of current risks and potential for cascading failures
- A lower probability (<1%) could be justified by emphasizing human adaptability and the tendency for existential risk concerns to be overblown

**Adjustments:**
Given these considerations, I should slightly increase my estimate to account for potential underestimation of novel risks and compounding effects. I'll adjust from 5.4% to 7%.

## 6. Final Forecast

Considering the base rate, evidence evaluation, and critical assessment, I arrive at a final probability of 7% for human extinction before 2100. This reflects both the historical resilience of humanity and the unprecedented nature of current and emerging risks.

Final: 7%
```
- parsed: 0.07 | ok=True | note=—

## Gemini [gemini-2.5-pro]
```text
Here is my probabilistic reasoning process:

### 1. Base Rate (Prior) Selection
The event in question, human extinction, has no direct historical precedent. A simple base rate of zero extinctions over the ~300,000-year history of *Homo sapiens* is uninformative for a future period characterized by unprecedented technological capabilities.

A more appropriate reference class is the judgment of informed groups on this specific question. I will consider two sources for my prior:
1.  **Expert Opinion**: The 2008 Oxford conference poll yielded a median estimate of 19%. However, as noted in the background, this is likely subject to severe selection bias. People who specialize in and attend conferences on catastrophic risks are predisposed to assign higher probabilities. I view this 19% as an alarmist upper bound, not a neutral prior.
2.  **Prediction Markets**: The Metaculus community forecast is 1.0%. This represents the aggregated wisdom of a large group of forecasters with a good track record. It is a much more sober estimate that reflects the high bar for total extinction.

I will select a prior of **2%**. This is a compromise between the Metaculus forecast (1%) and the slightly higher Manifold market (4.5%), acknowledging that the Metaculus number might be overly conservative while still treating it as a strong anchor. This translates to prior odds of 2:98 or 1:49.

### 2. Comparison to Base Case
The base case is the 20th century, where humanity survived its first self-created existential threat: nuclear weapons. The current situation differs significantly:
*   **Novel Risks**: The primary difference is the rapid development of Artificial General Intelligence (AGI), which introduces a new class of risk not present historically. Unlike nuclear weapons, which require state-level actors, advanced AI could potentially act autonomously and strategically to cause extinction.
*   **Exacerbated Risks**: Climate change is accelerating beyond 20th-century projections, creating a global stressor that can act as a threat multiplier. Geopolitical instability in a multipolar world may also make nuclear miscalculation more likely than in the bipolar Cold War era.
*   **Risk Confluence**: We face a confluence of multiple, potentially interacting catastrophic risks (AI, nuclear, climate, bio-risk) simultaneously, which is unique.

These differences suggest that the risk level is higher now than in the late 20th century.

### 3. Evidence Evaluation (Likelihoods)
I will evaluate the key new pieces of information from the research report against my 2% prior.

*   **Evidence 1: AI experts warn of a 20% chance of AI-induced extinction within 30 years.**
    *   Let H be "humans go extinct by 2100" and E be "a group of AI experts issues such a stark warning."
    *   **P(E | H)**: If humanity is indeed on a path to AI-induced extinction, it is very likely that the experts closest to the technology would become alarmed and issue such warnings.
    *   **P(E | ~H)**: If humanity survives, it's still plausible experts would issue warnings. They could be wrong, overly cautious, or motivated by other factors (e.g., seeking funding for safety research). However, a specific 20% figure is a very strong claim.
    *   **Likelihood Ratio (P(E|H)/P(E|~H))**: The ratio is greater than 1. The warning is more probable if the threat is real. I'll estimate a likelihood ratio of **2.5**, meaning this evidence is 2.5 times more likely if we are headed for extinction than if we are not. This evidence significantly increases the probability.

*   **Evidence 2: Study shows a 70% chance of Gulf Stream collapse by 2035.**
    *   This points to severe global instability, famine, and conflict, but not necessarily complete extinction. Humanity is resilient and geographically dispersed.
    *   This evidence strongly supports a future of "global catastrophe" but is weaker evidence for "total extinction." It increases the probability of extinction by making the world a more dangerous and fragile place where other risks are more likely to be realized.
    *   **Likelihood Ratio**: I'll assign a modest likelihood ratio of **1.2**. It pushes the probability up, but only slightly, as the link to *extinction* is indirect.

*   **Evidence 3: Increased nuclear war risks in a multipolar world.**
    *   Similar to the AI risk, heightened geopolitical tensions are more likely in a world trending towards catastrophic conflict than one trending towards peace and survival.
    *   **Likelihood Ratio**: This is a significant factor. The complexity of a multipolar world increases the chances of miscalculation. I'll assign a likelihood ratio of **1.5**.

### 4. Bayesian Updating (Posterior Probability)
I will update my prior odds using the likelihood ratios from the evidence.

*   **Prior Odds**: 1:49
*   **Update with AI Evidence (LR = 2.5)**:
    *   Posterior Odds = Prior Odds × LR = (1/49) × 2.5 = 2.5 / 49 ≈ 1 / 19.6
*   **Update with Climate Evidence (LR = 1.2)**:
    *   Posterior Odds = (1 / 19.6) × 1.2 ≈ 1 / 16.3
*   **Update with Nuclear Evidence (LR = 1.5)**:
    *   Posterior Odds = (1 / 16.3) × 1.5 ≈ 1 / 10.9

*   **Final Posterior Odds**: Approximately 1:11.
*   **Final Posterior Probability**: P(H) = Odds / (1 + Odds) = 1 / (1 + 11) = 1/12 ≈ **8.3%**.

This step-by-step update brings the probability to a significantly higher level than the initial market-based prior.

### 5. Red Team Thinking
My calculated 8.3% is substantially higher than both the Metaculus (1%) and Manifold (4.5%) markets. Why might I be wrong?

*   **Overconfidence in Updates**: I may have assigned likelihood ratios that are too high. The world has absorbed alarming expert warnings and studies before without ending. My updates reflect a significant shift, but perhaps the system is more resilient than that.
*   **The "Extinction is Hard" Argument**: The strongest counterargument is the sheer difficulty of killing every last human. A catastrophe could kill 99.9% of the population, but the question would resolve "No". My model may not be sufficiently weighting this high bar for resolution. A superintelligent AI is the most plausible scenario for achieving this, but its probability is highly uncertain.
*   **Market Efficiency**: The markets have already processed this general information. The fact that they remain at 1% and 4.5% is strong evidence that my update may be too aggressive. The community may be implicitly assigning lower likelihood ratios or believing more strongly in mitigating factors.

Considering these points, my calculated 8.3% feels too high. The market forecasts, especially Metaculus's, are a strong corrective. I should pull my estimate down significantly to account for the "wisdom of the crowd" and the high bar for extinction. However, I believe the markets may be too slow to react to the confluence and acceleration of these specific risks. Therefore, I will not retreat all the way to the market consensus. I will land on a number between my calculation and the Manifold forecast.

Averaging my calculated 8.3% and the Manifold forecast of 4.5% gives (8.3 + 4.5) / 2 ≈ 6.4%. This feels like a reasonable adjustment, acknowledging my reasoning while respecting the market's sobriety. I will round down to a whole number.

### 6. Final Forecast
My final probability reflects a low base rate, updated by recent, alarming evidence across multiple domains (AI, climate, geopolitics), and then tempered by red-teaming and the sobering influence of prediction market consensus. The risk is non-trivial and higher than in the past, but total extinction remains an unlikely outcome.

Final: 6%
```
- parsed: 0.06 | ok=True | note=—

## Grok [grok-4]
```text
### 1. Base Rate (Prior) Selection
For human extinction before 2100, an appropriate base rate draws from historical data on species survival and expert elicitations on existential risks. Historically, Homo sapiens has survived for approximately 300,000 years without extinction, implying a near-zero base rate for extinction in any given century (e.g., if we consider the last 1,000 years as a reference class, the frequency is 0/10 centuries). However, this is an overly optimistic prior because it ignores emerging anthropogenic risks absent in pre-modern eras. Expert surveys provide a better reference: the 2008 Oxford poll gave a 19% median, but as noted, it's biased toward high-risk views. More balanced aggregates like Metaculus (1%) and Manifold (4.5%) suggest a low but non-zero rate. I'll select a prior of 2% (odds 1:49), balancing the zero historical rate with expert/market consensus on modern risks like nuclear war and pandemics, which have near-misses (e.g., Cuban Missile Crisis) but no extinctions.

### 2. Comparison to Base Case
The base case is historical human survival through natural disasters, pandemics (e.g., Black Death killed ~50% of Europe but not globally extinctive), and conflicts without advanced technology. Similarities: Humanity has shown resilience via adaptation, population growth, and global distribution, reducing the odds of total wipeout from localized events. Differences: Today's situation introduces novel, high-magnitude risks like superintelligent AI (absent historically), advanced nuclear arsenals (escalation potential far beyond past wars), and rapid climate change (e.g., Gulf Stream collapse risks not seen in human history). These differences matter because they create "fat-tail" scenarios where small probabilities can lead to total extinction, unlike past eras where risks were more contained and recoverable.

### 3. Evidence Evaluation (Likelihoods)
- **AI Risks (e.g., 20% chance of AI-induced extinction in 30 years per recent warnings)**: This evidence is more likely if extinction occurs (P(E|H) ~80%, as uncontrolled AI could be a direct path to extinction) than if it doesn't (P(E|~H) ~30%, as warnings might be hype or mitigated). Likelihood ratio ~2.7 (increases probability moderately).
- **Climate Change (e.g., 70% chance of Gulf Stream collapse by 2035)**: More likely under extinction (P(E|H) ~70%, as severe shifts could cascade to societal collapse) vs. non-extinction (P(E|~H) ~40%, as adaptation might prevent total loss). Likelihood ratio ~1.75 (increases probability slightly, but climate alone rarely causes full extinction).
- **Nuclear War Risks (e.g., recent articles on geopolitical tensions)**: More likely if extinction happens (P(E|H) ~60%, as nuclear winter could lead to famine/extinction) vs. not (P(E|~H) ~20%, as deterrence has held historically). Likelihood ratio ~3 (increases probability notably).
- **Historical Lack of Extinction**: This is more likely under non-extinction (P(E|~H) ~100%) than extinction (P(E|H) ~0%, by definition). Likelihood ratio ~0 (decreases probability, but it's already baked into the prior).
Net: Evidence mostly increases probability due to emerging risks, with a combined qualitative likelihood ratio of ~3-5 (moderate upward adjustment).

### 4. Bayesian Updating (Posterior Probability)
Starting with prior probability P(H) = 2% (prior odds = 2:98 or 1:49). Grouping evidence into a net likelihood ratio of ~4 (conservative aggregate: AI and nuclear as strong updaters, climate moderate, history neutral/downward). Posterior odds = 1:49 × 4 = 4:49 (or ~1:12.25). Posterior probability = 4 / (4 + 49) ≈ 7.5%. This remains uncertain due to wide variance in expert views and unknowns (e.g., AI timelines), with confidence intervals roughly 2-15% reflecting model uncertainty.

### 5. Red Team Thinking
My forecast might be overconfident in a low posterior by anchoring too heavily to market consensus (e.g., Metaculus at 1%), potentially underweighting black-swan tail risks like cascading failures (AI + climate + war simultaneously). Blind spots include over-reliance on human resilience, ignoring unknown unknowns (e.g., asteroid impacts or novel biotech). Best alternative forecast: 20% (per biased 2008 poll), plausible if we view current risks as unprecedented and mitigation failures as likely; rebuttal: markets and recent data suggest overestimation, as progress in AI safety and diplomacy could cap risks. Adjusting for tails, I'll nudge up slightly to widen uncertainty, but avoid big swings per calibration advice—final posterior ~8%.

Final: 8%
```
- parsed: 0.08 | ok=True | note=—


------------------------------------------------------------

# BAYES-MC AGGREGATION (Summary)
```json
{}
```
