name: Inspect Resolver DuckDB

on:
  workflow_dispatch:
    inputs:
      backfill_run_id:
        description: "Run ID of the Resolver backfill workflow (the run that produced pythia-resolver-db/resolver.duckdb)"
        required: true
        type: string

  workflow_run:
    workflows: ["Resolver â€” Initial Backfill"]
    types: [completed]

jobs:
  inspect-resolver-db:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install duckdb and gh
        run: |
          python -m pip install --upgrade pip
          python -m pip install duckdb
          sudo apt-get update
          sudo apt-get install -y gh

      - name: Authenticate gh
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh auth status

      - name: Determine backfill run ID
        env:
          EVENT_NAME: ${{ github.event_name }}
          DISPATCH_RUN_ID: ${{ inputs.backfill_run_id }}
          WORKFLOW_RUN_ID: ${{ github.event.workflow_run.id }}
        run: |
          if [ "$EVENT_NAME" = "workflow_run" ]; then
            echo "BACKFILL_RUN_ID=${WORKFLOW_RUN_ID}" >> "$GITHUB_ENV"
            echo "Using workflow_run.id=${WORKFLOW_RUN_ID}"
          else
            echo "BACKFILL_RUN_ID=${DISPATCH_RUN_ID}" >> "$GITHUB_ENV"
            echo "Using dispatch input backfill_run_id=${DISPATCH_RUN_ID}"
          fi

      - name: Download canonical resolver DB from backfill run
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p data

          echo "Downloading pythia-resolver-db for run $BACKFILL_RUN_ID ..."
          gh run download "$BACKFILL_RUN_ID" -n pythia-resolver-db --dir data

          echo "Artifacts downloaded under data/:"
          ls -R data

          # Canonical layout: data/pythia-resolver-db/resolver.duckdb
          if [ -f data/pythia-resolver-db/resolver.duckdb ]; then
            SRC="data/pythia-resolver-db/resolver.duckdb"
          else
            # Fallback: search any resolver.duckdb
            SRC=$(find data -maxdepth 4 -type f -name "resolver.duckdb" | head -n 1 || true)
          fi

          if [ -n "$SRC" ]; then
            echo "Found resolver DB at $SRC"
            if [ "$SRC" != "data/resolver.duckdb" ]; then
              cp "$SRC" data/resolver.duckdb
            else
              echo "DB already at canonical path data/resolver.duckdb; no copy needed."
            fi
          fi

          if [ ! -f data/resolver.duckdb ]; then
            echo "ERROR: resolver.duckdb not found in pythia-resolver-db for run $BACKFILL_RUN_ID"
            ls -R data
            exit 1
          fi

      - name: Inspect data/resolver.duckdb
        run: |
          python - << 'EOF'
          import os
          from pathlib import Path

          import duckdb

          out_path = Path('resolver_inspect.md')

          lines = []
          lines.append("# Resolver DuckDB Inspection")
          lines.append("")

          # Resolve DB path from RESOLVER_DB_URL if present, else fall back to data/resolver.duckdb
          db_url = os.getenv("RESOLVER_DB_URL", "duckdb:///data/resolver.duckdb")

          if db_url.startswith("duckdb:///"):
              db_path = db_url[len("duckdb:///"):]
          else:
              db_path = db_url

          db_path = Path(db_path)

          if not db_path.exists():
              lines.append(f"_Database not found at `{db_path}`_")
          else:
              print(f"[inspect] Connecting to {db_path}")
              con = duckdb.connect(str(db_path), read_only=True)

              lines.append("## Tables")
              tables = con.execute("""
                  SELECT table_name
                  FROM information_schema.tables
                  WHERE table_schema = 'main'
                  ORDER BY table_name
              """).fetchall()
              lines.append("```")
              for (tbl,) in tables:
                  lines.append(tbl)
              lines.append("```")
              lines.append("")

              lines.append("# Resolver DB Coverage Summary")
              lines.append("")

              def summarize(query: str, header: str, columns: list[str]):
                  lines.append(header)
                  lines.append("")
                  lines.append("| " + " | ".join(columns) + " |")
                  lines.append("|" + "---|" * len(columns))
                  try:
                      rows = con.execute(query).fetchall()
                      for row in rows:
                          lines.append("| " + " | ".join(str(x) for x in row) + " |")
                  except Exception as e:
                      lines.append(f"_Error executing summary: {type(e).__name__}: {e}_")
                  lines.append("")

              try:
                  rows = con.execute(
                      """
                      SELECT
                          COALESCE(source, '') AS src,
                          COALESCE(metric, '') AS metric,
                          COALESCE(hazard_code, '') AS hazard_code,
                          COALESCE(iso3, '') AS iso3,
                          COUNT(*) AS n_rows
                      FROM facts_deltas
                      GROUP BY src, metric, hazard_code, iso3
                      ORDER BY src, metric, hazard_code, iso3
                      """
                  ).fetchall()
                  lines.append("## facts_deltas row counts by source / metric / hazard_code / iso3")
                  lines.append("")
                  lines.append("| source | metric | hazard_code | iso3 | n_rows |")
                  lines.append("|--------|--------|-------------|------|--------|")
                  for src, metric, hz, iso3, n in rows:
                      lines.append(f"| {src} | {metric} | {hz} | {iso3} | {n} |")
                  lines.append("")
              except Exception as e:
                  lines.append(f"_Error summarizing facts_deltas: {type(e).__name__}: {e}_")
                  lines.append("")

              summarize(
                  """
                  SELECT
                      COALESCE(iso3, '') AS iso3,
                      COALESCE(shock_type, '') AS shock_type,
                      COUNT(*) AS n_rows
                  FROM emdat_pa
                  GROUP BY iso3, shock_type
                  ORDER BY iso3, shock_type
                  """,
                  "## emdat_pa row counts by iso3 / shock_type",
                  ["iso3", "shock_type", "n_rows"],
              )

              summarize(
                  """
                  SELECT
                      COALESCE(iso3, '') AS iso3,
                      COUNT(*) AS n_rows
                  FROM acled_monthly_fatalities
                  GROUP BY iso3
                  ORDER BY iso3
                  """,
                  "## acled_monthly_fatalities row counts by iso3",
                  ["iso3", "n_rows"],
              )

              summarize(
                  """
                  SELECT
                      COALESCE(iso3, '') AS iso3,
                      COALESCE(hazard_code, '') AS hazard_code,
                      COALESCE(metric, '') AS metric,
                      COUNT(*) AS n_questions
                  FROM questions
                  GROUP BY iso3, hazard_code, metric
                  ORDER BY iso3, hazard_code, metric
                  """,
                  "## questions by iso3 / hazard_code / metric",
                  ["iso3", "hazard_code", "metric", "n_questions"],
              )

              try:
                  rows = con.execute(
                      """
                      SELECT
                          COALESCE(q.iso3, '') AS iso3,
                          COALESCE(f.hazard_code, '') AS hazard_code,
                          COALESCE(f.metric, '') AS metric,
                          COUNT(DISTINCT f.question_id) AS n_questions,
                          COUNT(*) AS n_rows
                      FROM forecasts_ensemble AS f
                      LEFT JOIN questions AS q
                        ON q.question_id = f.question_id
                      GROUP BY iso3, hazard_code, metric
                      ORDER BY iso3, hazard_code, metric
                      """
                  ).fetchall()
                  lines.append("## forecasts_ensemble by iso3 / hazard_code / metric")
                  lines.append("")
                  lines.append("| iso3 | hazard_code | metric | n_questions | n_rows |")
                  lines.append("|------|-------------|--------|-------------|--------|")
                  for iso3, hz, metric, n_q, n_rows in rows:
                      lines.append(f"| {iso3} | {hz} | {metric} | {n_q} | {n_rows} |")
                  lines.append("")
              except Exception as e:
                  lines.append(f"_Error summarizing forecasts_ensemble: {type(e).__name__}: {e}_")
                  lines.append("")

              def dump_table_schema_and_sample(table_name: str, limit: int = 10):
                  lines.append(f"## {table_name} schema & sample")
                  lines.append("")
                  try:
                      schema = con.execute(f"PRAGMA table_info('{table_name}')").fetchall()
                      lines.append("Schema (PRAGMA table_info):")
                      lines.append("```")
                      for row in schema:
                          lines.append(str(row))
                      lines.append("```")
                      lines.append("")
                      sample = con.execute(f"SELECT * FROM {table_name} LIMIT {limit}").fetchall()
                      lines.append(f"Sample rows (up to {limit}):")
                      lines.append("```")
                      for row in sample:
                          lines.append(str(row))
                      lines.append("```")
                  except Exception as e:
                      lines.append(f"_Error reading {table_name}: {type(e).__name__}: {e}_")
                  lines.append("")

              # Key Resolver / Pythia tables of interest
              for tbl in [
                  "bucket_centroids",
                  "facts_resolved",
                  "emdat_pa",
                  "acled_monthly_fatalities",
                  "calibration_weights",
                  "calibration_advice",
                  "resolutions",
                  "scores",
                  "hs_runs",
                  "hs_scenarios",
                  "questions",
                  "forecasts_ensemble",
                  "forecasts_raw",
                  "llm_calls",
                  "question_context",
              ]:
                  dump_table_schema_and_sample(tbl, limit=10)

              con.close()

          out_path.write_text("\n".join(lines), encoding='utf-8')
          print(f"Wrote {out_path}")
          EOF

      - name: Upload resolver inspection artifact
        uses: actions/upload-artifact@v4
        with:
          name: resolver-inspect
          path: resolver_inspect.md
