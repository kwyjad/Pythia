name: Inspect Resolver DuckDB

on:
  workflow_dispatch:
    inputs:
      backfill_run_id:
        description: "Run ID of the Resolver backfill workflow (the run that produced resolver_backfill.duckdb)"
        required: true
        type: string

jobs:
  inspect-resolver-db:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install duckdb and gh
        run: |
          python -m pip install --upgrade pip
          python -m pip install duckdb
          sudo apt-get update
          sudo apt-get install -y gh

      - name: Authenticate gh
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh auth status

      - name: Download resolver_backfill.duckdb artifact from backfill run
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p data
          # Download all artifacts for the specified run into data/
          gh run download ${{ inputs.backfill_run_id }} --dir data

          echo "Downloaded artifacts into data/:"
          ls -R data

          # If your artifact is a single file named resolver_backfill.duckdb in data/,
          # rename it to data/resolver.duckdb.
          # If it ends up inside a subdirectory, adjust the mv accordingly.
          if [ -f data/resolver_backfill.duckdb ]; then
            mv data/resolver_backfill.duckdb data/resolver.duckdb
          else
            # Try to find it under subdirectories
            FILE_PATH=$(find data -maxdepth 2 -type f -name "resolver_backfill.duckdb" | head -n 1 || true)
            if [ -n "$FILE_PATH" ]; then
              mv "$FILE_PATH" data/resolver.duckdb
            fi
          fi

          if [ ! -f data/resolver.duckdb ]; then
            echo "resolver_backfill.duckdb not found in artifacts; please adjust the mv logic."
            ls -R data
            exit 1
          fi

      - name: Inspect data/resolver.duckdb
        run: |
          python - << 'EOF'
          import duckdb, os
          from pathlib import Path

          db_path = 'data/resolver.duckdb'
          out_path = Path('resolver_inspect.md')

          lines = []
          lines.append("# Resolver DuckDB Inspection")
          lines.append("")

          if not os.path.exists(db_path):
              lines.append(f"_Database not found at `{db_path}`_")
          else:
              con = duckdb.connect(db_path, read_only=True)

              lines.append("## Tables")
              tables = con.execute("""
                  SELECT table_name
                  FROM information_schema.tables
                  WHERE table_schema = 'main'
                  ORDER BY table_name
              """).fetchall()
              lines.append("```")
              for (tbl,) in tables:
                  lines.append(tbl)
              lines.append("```")
              lines.append("")

              def dump_table_schema_and_sample(table_name: str, limit: int = 10):
                  lines.append(f"## {table_name} schema & sample")
                  lines.append("")
                  try:
                      schema = con.execute(f"PRAGMA table_info('{table_name}')").fetchall()
                      lines.append("Schema (PRAGMA table_info):")
                      lines.append("```")
                      for row in schema:
                          lines.append(str(row))
                      lines.append("```")
                      lines.append("")
                      sample = con.execute(f"SELECT * FROM {table_name} LIMIT {limit}").fetchall()
                      lines.append(f"Sample rows (up to {limit}):")
                      lines.append("```")
                      for row in sample:
                          lines.append(str(row))
                      lines.append("```")
                  except Exception as e:
                      lines.append(f"_Error reading {table_name}: {type(e).__name__}: {e}_")
                  lines.append("")

              # Key Resolver / Pythia tables of interest
              for tbl in [
                  "bucket_centroids",
                  "facts_resolved",
                  "emdat_pa",
                  "acled_monthly_fatalities",
                  "calibration_weights",
                  "calibration_advice",
                  "resolutions",
                  "scores",
                  "hs_runs",
                  "hs_scenarios",
                  "questions",
                  "forecasts_ensemble",
                  "forecasts_raw",
                  "llm_calls",
                  "question_context",
              ]:
                  dump_table_schema_and_sample(tbl, limit=10)

              con.close()

          out_path.write_text("\n".join(lines), encoding='utf-8')
          print(f"Wrote {out_path}")
          EOF

      - name: Upload resolver inspection artifact
        uses: actions/upload-artifact@v4
        with:
          name: resolver-inspect
          path: resolver_inspect.md
