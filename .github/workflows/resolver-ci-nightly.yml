name: resolver-ci-nightly

on:
  schedule:
    - cron: "0 23 * * *"
  workflow_dispatch: {}

permissions:
  contents: read

concurrency:
  group: resolver-nightly
  cancel-in-progress: false

jobs:
  full_connectors:
    if: ${{ github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        connector:
          [reliefweb, ifrc_go, unhcr, unhcr_odp, dtm, who_phe, hdx, acled, emdat, gdacs, ipc, wfp_mvam, worldpop]
    env:
      ACLED_REFRESH_TOKEN: ${{ secrets.ACLED_REFRESH_TOKEN }}
      ACLED_USERNAME: ${{ secrets.ACLED_USERNAME }}
      ACLED_PASSWORD: ${{ secrets.ACLED_PASSWORD }}
      DTM_API_PRIMARY_KEY: ${{ secrets.DTM_API_PRIMARY_KEY }}
      DTM_API_SECONDARY_KEY: ${{ secrets.DTM_API_SECONDARY_KEY }}
      DTM_API_HEADER_NAME: ${{ vars.DTM_API_HEADER_NAME }}
      PYTHONPATH: ${{ github.workspace }}
      RESOLVER_DEBUG: "0"
      RESOLVER_INCLUDE_STUBS: "0"
      RESOLVER_WINDOW_DAYS: ""
      RESOLVER_MAX_PAGES: ""
      RESOLVER_MAX_RESULTS: ""
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: |
            resolver/requirements.txt
            resolver/requirements-dev.txt
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r resolver/requirements.txt
          pip install -r resolver/requirements-dev.txt
      - name: Run ${{ matrix.connector }} (full)
        env:
          RESOLVER_FAIL_ON_STUB_ERROR: "0"
        run: |
          python -m resolver.ingestion.run_all_stubs --mode real --only ${{ matrix.connector }} --log-level INFO --log-format json
      - name: Run staging schema tests
        run: pytest -q resolver/tests/test_staging_schema_all.py
      - name: Record job status
        if: always()
        env:
          CONNECTOR: ${{ matrix.connector }}
          JOB_STATUS: ${{ job.status }}
        run: |
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          connector = os.environ["CONNECTOR"]
          status = os.environ.get("JOB_STATUS", "unknown")
          log_dir = Path("resolver/logs/ingestion")
          log_dir.mkdir(parents=True, exist_ok=True)
          summary_path = log_dir / f"{connector}-job-status.json"
          summary_path.write_text(json.dumps({"connector": connector, "job_status": status}))
          PY
      - name: Upload nightly artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-${{ matrix.connector }}
          path: |
            resolver/logs/ingestion/**
            resolver/exports/**
            resolver/staging/**
          retention-days: 7

  aggregate:
    if: ${{ always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch') }}
    needs: full_connectors
    runs-on: ubuntu-latest
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: nightly-artifacts
          merge-multiple: true
      - name: Build summary
        run: |
          python - <<'PY'
          import csv
          import json
          from pathlib import Path

          root = Path("nightly-artifacts")
          records = []
          for status_file in root.rglob("*-job-status.json"):
              try:
                  data = json.loads(status_file.read_text())
              except json.JSONDecodeError:
                  continue
              connector = data.get("connector") or status_file.stem.replace("-job-status", "")
              job_status = data.get("job_status", "unknown")
              record = {
                  "connector": connector,
                  "job_status": job_status,
                  "status": "unknown",
                  "attempts": None,
                  "duration_ms": None,
                  "notes": None,
              }
              log_dir = status_file.parent
              for log_file in sorted(log_dir.glob("*")):
                  if not log_file.is_file():
                      continue
                  try:
                      lines = log_file.read_text().splitlines()
                  except Exception:
                      continue
                  for line in lines:
                      line = line.strip()
                      if not line:
                          continue
                      try:
                          payload = json.loads(line)
                      except json.JSONDecodeError:
                          continue
                      if payload.get("event") == "connector_summary":
                          record.update(
                              {
                                  "status": payload.get("status", record["status"]),
                                  "attempts": payload.get("attempts", record["attempts"]),
                                  "duration_ms": payload.get("duration_ms", record["duration_ms"]),
                                  "notes": payload.get("notes", record["notes"]),
                              }
                          )
              records.append(record)
          records.sort(key=lambda item: item["connector"] or "")
          summary_dir = Path("nightly-summary")
          summary_dir.mkdir(exist_ok=True)
          json_path = summary_dir / "summary.json"
          csv_path = summary_dir / "summary.csv"
          json_path.write_text(json.dumps(records, indent=2, sort_keys=True))
          with csv_path.open("w", newline="", encoding="utf-8") as handle:
              writer = csv.DictWriter(handle, fieldnames=["connector", "job_status", "status", "attempts", "duration_ms", "notes"])
              writer.writeheader()
              for row in records:
                  writer.writerow(row)
          PY
      - name: Upload nightly summary
        uses: actions/upload-artifact@v4
        with:
          name: nightly-summary
          path: nightly-summary
          retention-days: 7

  tests-db:
    name: tests (db backend)
    if: ${{ github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    env:
      RESOLVER_API_BACKEND: db
      RESOLVER_DB_URL: duckdb:///${{ github.workspace }}/.ci-resolver.duckdb
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-db-${{ hashFiles('pyproject.toml', 'resolver/requirements.txt', 'resolver/requirements-dev.txt', 'tools/offline_wheels/constraints-db.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-db-

      - name: Install resolver dependencies (db backend)
        run: |
          python -m pip install --upgrade pip
          pip install -r resolver/requirements.txt
          pip install -r resolver/requirements-dev.txt
          pip install -e ".[db,test]"
          python -m pip install pytest

      - name: Show environment
        run: |
          python -V
          python -c "import duckdb,sys;print('duckdb',duckdb.__version__)"
          echo "BACKEND=$RESOLVER_API_BACKEND"
          echo "DB_URL=$RESOLVER_DB_URL"

      - name: Run tests (db)
        run: |
          pytest -q
