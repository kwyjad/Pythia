# Pythia
# Copyright (c) 2025 Kevin Wyjad
# Licensed under the Pythia Non-Commercial Public License v1.0.
# See the LICENSE file in the project root for details.

---
# This workflow runs the Pythia Horizon Scanner triage stage.

name: Horizon Scanner Triage

concurrency:
  group: pythia-resolver-db
  cancel-in-progress: false

# --- Triggers ---
on:
  # Manual run from the Actions tab
  workflow_dispatch:
    inputs:
      run_spd_eval:
        description: "Run SPD aggregation evaluation (non-core)"
        required: false
        default: "false"
        type: choice
        options: ["false", "true"]
      db_run_id:
        description: "Optional: force canonical DB from Actions run ID (/actions/runs/<id>) containing artifact 'pythia-resolver-db'"
        required: false
        default: ""
        type: string
      db_artifact_name:
        description: "Artifact name to download from db_run_id (default: pythia-resolver-db). Use pythia-resolver-db-reset for bootstrap runs."
        required: false
        default: "pythia-resolver-db"
        type: string
  schedule:
    # Run at 03:00 Europe/Istanbul on the 1st of each month (00:00 UTC)
    - cron: "0 0 1 * *"

permissions:
  contents: read
  actions: read

# --- Job Definition ---
jobs:
  build-report:
    runs-on: ubuntu-latest
    env:
      RESOLVER_DB_URL: duckdb:///${{ github.workspace }}/data/resolver.duckdb
      PYTHIA_DB_URL: duckdb:///${{ github.workspace }}/data/resolver.duckdb
      PYTHIA_LLM_CONCURRENCY: "32"
      HS_MAX_WORKERS: "8"
      HS_TEMPERATURE: "0.0"
      FORECASTER_RESEARCH_MAX_WORKERS: "8"
      FORECASTER_SPD_MAX_WORKERS: "8"
      PYTHIA_WEB_RESEARCH_ENABLED: "1"
      PYTHIA_RETRIEVER_ENABLED: "1"
      PYTHIA_RETRIEVER_MODEL_ID: "gemini-2.5-flash-lite"
      PYTHIA_MODEL_SELF_SEARCH_ENABLED: "0"
      # Flip HS/Research to 1 when Gemini 3 grounding is available.
      PYTHIA_HS_RESEARCH_WEB_SEARCH_ENABLED: "0"
      PYTHIA_SPD_WEB_SEARCH_ENABLED: "1"
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      PYTHIA_WEB_RESEARCH_BACKEND: auto
      PYTHIA_WEB_RESEARCH_RECENCY_DAYS: "120"
      PYTHIA_WEB_RESEARCH_INCLUDE_STRUCTURAL: "1"
      PYTHIA_FORECASTER_SELF_SEARCH: "1"
      PYTHIA_FORECASTER_SELF_SEARCH_MAX_CALLS_PER_MODEL: "1"
      PYTHIA_FORECASTER_SELF_SEARCH_MAX_SOURCES: "6"
      HS_MODEL_ID: gemini-3-flash-preview
      PYTHIA_BLOCK_PROVIDERS: xai
      PYTHIA_SPD_ENSEMBLE_SPECS: openai:gpt-5.1,anthropic:claude-opus-4-5-20251101,google:gemini-3-pro-preview,google:gemini-3-flash-preview
      PYTHIA_GOOGLE_SPD_THINKING_LEVEL_FLASH: "low"
      PYTHIA_GOOGLE_SPD_THINKING_LEVEL_PRO: ""
      PYTHIA_GOOGLE_SPD_TIMEOUT_FLASH_SEC: "90"
      PYTHIA_GOOGLE_SPD_TIMEOUT_PRO_SEC: "120"
      PYTHIA_GOOGLE_SPD_RETRIES: "1"
      SIGNATURE_REQUIRED_TABLES: questions,hs_triage,question_research,forecasts_ensemble,llm_calls,facts_resolved,facts_deltas,acled_monthly_fatalities
      SIGNATURE_OPTIONAL_TABLES: scenarios
      CANONICAL_DB_READY: "false"
      CANONICAL_DB_RUN_ID: ""
      CANONICAL_DB_SOURCE: ""
      CANONICAL_CREATED_AT: ""
      SIGNATURE_OK: "false"
      DB_SOURCE: ""

    steps:
      # Step 1: Check out the repository's code
      - name: Check out code
        uses: actions/checkout@v4

      - name: Install gh CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y gh

      - name: Authenticate gh
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh auth status

      # Step 2: Set up Python
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Diagnostics – workspace layout and key files
      - name: Diagnostics — workspace layout and key files
        run: |
          echo "=== Diagnostics: workspace layout ==="
          echo "PWD: $(pwd)"
          echo ""
          echo "Top-level listing:"
          ls -la
          echo ""
          echo "horizon_scanner directory listing:"
          if [ -d horizon_scanner ]; then
            ls -la horizon_scanner
          else
            echo "✘ MISSING: horizon_scanner directory"
          fi
          echo ""
          echo "Checking for Horizon Scanner files:"
          for f in \
            "horizon_scanner/horizon_scanner.py" \
            "horizon_scanner/db_writer.py" \
            "horizon_scanner/hs_country_list.txt" \
            "horizon_scanner/hs_prompt.py" \
            "python_library_requirements.txt"
          do
            if [ -f "$f" ]; then
              echo "✔ Found $f"
            else
              echo "✘ MISSING: $f"
            fi
          done
          echo ""
          if [ -f python_library_requirements.txt ]; then
            echo "=== python_library_requirements.txt (root) ==="
            cat python_library_requirements.txt
          else
            echo "WARNING: python_library_requirements.txt is missing in $(pwd); dependency install will fail."
          fi

      # Step 4: Install the necessary Python libraries
      - name: Install dependencies
        run: |
          set -e
          echo "Upgrading pip..."
          python -m pip install --upgrade pip
          echo ""
          echo "Installing dependencies from python_library_requirements.txt..."
          if [ ! -f python_library_requirements.txt ]; then
            echo "ERROR: python_library_requirements.txt not found in $(pwd)."
            exit 1
          fi
          pip install -r python_library_requirements.txt

      - name: Download canonical resolver DB with signature guardrail
        id: canonical_download
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p data diagnostics
          rm -f diagnostics/db_signature_before.json
          rm -rf data/pythia-resolver-db

          OVERRIDE_RUN_ID="${{ github.event.inputs.db_run_id }}"
          DB_ARTIFACT_NAME_INPUT="${{ github.event.inputs.db_artifact_name }}"
          DB_ARTIFACT_NAME="${DB_ARTIFACT_NAME_INPUT:-pythia-resolver-db}"

          download_artifact_with_retries() {
            local run_id="$1"
            local artifact_name="$2"
            local out_dir="$3"
            local max_attempts="${4:-6}"

            local attempt=1
            while [ "$attempt" -le "$max_attempts" ]; do
              echo "Downloading artifact '${artifact_name}' from run ${run_id} (attempt ${attempt}/${max_attempts})"

              # avoid partial/old contents
              rm -rf "${out_dir}/pythia-resolver-db" || true

              if gh run download "$run_id" -n "$artifact_name" --dir "$out_dir"; then
                echo "Download OK."
                return 0
              fi

              rc=$?
              sleep_seconds=$(( 5 * attempt * attempt ))  # 5, 20, 45, 80, 125...
              echo "Download failed (exit=${rc}). Sleeping ${sleep_seconds}s before retry..."
              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
            done

            echo "ERROR: Failed to download artifact '${artifact_name}' from run ${run_id} after ${max_attempts} attempts."
            return 1
          }

          if [ -n "$OVERRIDE_RUN_ID" ]; then
            if ! echo "$OVERRIDE_RUN_ID" | grep -Eq '^[0-9]+$'; then
              echo "ERROR: db_run_id must be a numeric Actions run id from /actions/runs/<id>. Got: '$OVERRIDE_RUN_ID'"
              exit 1
            fi

            DB_RUN_ID="$OVERRIDE_RUN_ID"
            DB_SOURCE="override"
            DB_CREATED_AT="manual-override"
          fi

          if [ -z "${DB_ARTIFACT_NAME:-}" ]; then
            DB_ARTIFACT_NAME="pythia-resolver-db"
          fi

          if [ -z "${DB_RUN_ID:-}" ]; then
            CANDIDATES_FILE=$(mktemp)
            gh run list \
              --workflow "Horizon Scanner Triage" \
              --branch main \
              --status success \
              --json databaseId,createdAt \
              --limit 20 | jq -r '.[] | "\(.createdAt),\(.databaseId),Horizon Scanner Triage"' >>"${CANDIDATES_FILE}" || true
            gh run list \
              --workflow "Resolver — Initial Backfill" \
              --branch main \
              --status success \
              --json databaseId,createdAt \
              --limit 20 | jq -r '.[] | "\(.createdAt),\(.databaseId),Resolver — Initial Backfill"' >>"${CANDIDATES_FILE}" || true

            if [ ! -s "${CANDIDATES_FILE}" ]; then
              echo "ERROR: No successful Horizon Scanner or Resolver — Initial Backfill runs found; refusing to start without canonical DB."
              exit 1
            fi

            sort -r "${CANDIDATES_FILE}" > "${CANDIDATES_FILE}.sorted"

            FOUND=0

            while IFS=',' read -r CREATED RUN_ID LABEL; do
              DB_RUN_ID="$RUN_ID"
              DB_CREATED_AT="$CREATED"
              if [ "$LABEL" = "Horizon Scanner Triage" ]; then
                DB_SOURCE="pipeline"
              else
                DB_SOURCE="backfill"
              fi

              DB_ARTIFACT_NAME="pythia-resolver-db"

              rm -rf data/pythia-resolver-db
              rm -f data/resolver.duckdb

              if ! download_artifact_with_retries "$DB_RUN_ID" "$DB_ARTIFACT_NAME" data 6; then
                echo "Download failed for run ${DB_RUN_ID}; trying next candidate."
                continue
              fi

              if [ -f data/pythia-resolver-db/resolver.duckdb ]; then
                SRC="data/pythia-resolver-db/resolver.duckdb"
              else
                SRC=$(find data -maxdepth 4 -type f -name "resolver.duckdb" | head -n 1 || true)
              fi

              if [ -z "${SRC}" ]; then
                echo "resolver.duckdb not found in downloaded artifact from ${DB_RUN_ID}; trying next candidate."
                continue
              fi

              if [ "$SRC" != "data/resolver.duckdb" ]; then
                cp "$SRC" data/resolver.duckdb
              fi

              if python -m scripts.ci.db_signature write \
                  --db data/resolver.duckdb \
                  --required "${SIGNATURE_REQUIRED_TABLES}" \
                  --optional "${SIGNATURE_OPTIONAL_TABLES}" \
                  --out diagnostics/db_signature_before.json; then
                FOUND=1
                break
              else
                rc=$?
                echo "Candidate rejected (exit ${rc}); missing required tables or invalid signature. Trying next candidate."
                rm -f diagnostics/db_signature_before.json
              fi
            done < "${CANDIDATES_FILE}.sorted"

            if [ "${FOUND}" -ne 1 ]; then
              echo "ERROR: No valid canonical DB found after checking Horizon Scanner and Resolver — Initial Backfill runs."
              exit 1
            fi
          fi

          if [ -z "${DB_RUN_ID:-}" ]; then
            echo "ERROR: No successful Horizon Scanner or Resolver — Initial Backfill runs found; refusing to start without canonical DB."
            exit 1
          fi

          if [ -n "$OVERRIDE_RUN_ID" ]; then
            rm -rf data/pythia-resolver-db
            rm -f data/resolver.duckdb
            if ! download_artifact_with_retries "$DB_RUN_ID" "${DB_ARTIFACT_NAME:-pythia-resolver-db}" data 6; then
              exit 1
            fi
          fi

          if [ -f data/pythia-resolver-db/resolver.duckdb ]; then
            SRC="data/pythia-resolver-db/resolver.duckdb"
          else
            SRC=$(find data -maxdepth 4 -type f -name "resolver.duckdb" | head -n 1 || true)
          fi

          if [ -z "${SRC}" ]; then
            echo "ERROR: resolver.duckdb not found in downloaded artifact from ${DB_RUN_ID}."
            exit 1
          fi

          if [ "$SRC" != "data/resolver.duckdb" ]; then
            cp "$SRC" data/resolver.duckdb
          fi

          # Bootstrap-only: if we're starting from a fresh reset DB, ensure Pythia schema tables exist before signature check.
          if [ "${DB_ARTIFACT_NAME:-pythia-resolver-db}" = "pythia-resolver-db-reset" ]; then
            echo "Bootstrap DB detected (${DB_ARTIFACT_NAME}); ensuring Pythia schema tables exist before signature check."
            PYTHIA_DB_URL="${RESOLVER_DB_URL}" python -c "from pythia.db.schema import ensure_schema; ensure_schema(); print('ensure_schema(): OK')"
          fi

          python -m scripts.ci.db_signature write \
            --db data/resolver.duckdb \
            --required "${SIGNATURE_REQUIRED_TABLES}" \
            --optional "${SIGNATURE_OPTIONAL_TABLES}" \
            --out diagnostics/db_signature_before.json

          if [ ! -f diagnostics/db_signature_before.json ]; then
            echo "ERROR: Signature missing after validating run ${DB_RUN_ID}."
            exit 1
          fi

          {
            echo "CANONICAL_DB_READY=true"
            echo "CANONICAL_DB_SOURCE=${DB_SOURCE:-unknown}"
            echo "CANONICAL_DB_RUN_ID=${DB_RUN_ID}"
            echo "CANONICAL_CREATED_AT=${DB_CREATED_AT:-manual}"
            echo "DB_RUN_ID=${DB_RUN_ID}"
            echo "DB_SOURCE=${DB_SOURCE:-unknown}"
            echo "DB_CREATED_AT=${DB_CREATED_AT:-manual}"
            echo "DB_ARTIFACT_NAME=${DB_ARTIFACT_NAME:-pythia-resolver-db}"
          } >> "$GITHUB_ENV"
          CANONICAL_DB_SOURCE="${DB_SOURCE:-unknown}"
          CANONICAL_DB_RUN_ID="${DB_RUN_ID}"
          CANONICAL_CREATED_AT="${DB_CREATED_AT:-manual}"
          echo "Baseline signature written to diagnostics/db_signature_before.json"

          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f diagnostics/db_signature_before.json ]; then
            {
              echo "### Canonical DB signature (before HS run)"
              echo "- Source workflow: ${CANONICAL_DB_SOURCE}"
              echo "- Run ID: ${CANONICAL_DB_RUN_ID}"
              echo "- Created: ${CANONICAL_CREATED_AT}"
              echo "- Signature file: diagnostics/db_signature_before.json"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

          echo "DB selection: source=${DB_SOURCE:-unknown} run_id=${DB_RUN_ID} created=${DB_CREATED_AT:-manual} artifact=${DB_ARTIFACT_NAME:-pythia-resolver-db}"

      # Step 5: Diagnostics – confirm GEMINI_API_KEY is present
      - name: Diagnostics — GEMINI_API_KEY presence
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          if [ -z "${GEMINI_API_KEY:-}" ]; then
            echo "ERROR: GEMINI_API_KEY is NOT set."
            echo "Set it under: Settings > Secrets and variables > Actions."
            exit 1
          else
            echo "GEMINI_API_KEY is set (value hidden)."
          fi

      # Step 6: Run the main Python script (as a module)
      - name: Run Horizon Scanner
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          set -e
          mkdir -p diagnostics
          echo "=== Running horizon_scanner.horizon_scanner as a module ==="
          python -m horizon_scanner.horizon_scanner | tee diagnostics/hs_stdout.txt
          HS_RUN_ID=$(grep -E '^HS_RUN_ID=' diagnostics/hs_stdout.txt | tail -n 1 | cut -d= -f2-)
          HS_RESOLVED_ISO3S=$(grep -E '^HS_RESOLVED_ISO3S=' diagnostics/hs_stdout.txt | tail -n 1 | cut -d= -f2-)
          if [ -z "${HS_RUN_ID}" ]; then
            echo "ERROR: HS_RUN_ID not found in Horizon Scanner output."
            exit 1
          fi
          {
            echo "HS_RUN_ID=${HS_RUN_ID}"
            echo "HS_RESOLVED_ISO3S=${HS_RESOLVED_ISO3S}"
          } >> "$GITHUB_ENV"
          {
            echo "HS_RUN_ID=${HS_RUN_ID}"
            echo "HS_RESOLVED_ISO3S=${HS_RESOLVED_ISO3S}"
          } >> "$GITHUB_OUTPUT"
          echo "=== horizon_scanner.horizon_scanner completed ==="

      - name: Assert HS triage outputs
        id: assert_hs_triage
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
          HS_RUN_ID: ${{ env.HS_RUN_ID }}
          HS_RESOLVED_ISO3S: ${{ env.HS_RESOLVED_ISO3S }}
          HS_MODEL_ID: ${{ env.HS_MODEL_ID }}
        run: |
          set -e
          python -m py_compile scripts/ci/assert_hs_outputs.py
          python -m scripts.ci.assert_hs_outputs --db "${RESOLVER_DB_URL}" --hs-run-id "${HS_RUN_ID}" --stage triage

      - name: Warn on missing HS triage rows
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
          HS_RUN_ID: ${{ env.HS_RUN_ID }}
          HS_RESOLVED_ISO3S: ${{ env.HS_RESOLVED_ISO3S }}
        run: |
          python - <<'PY'
          import os
          from resolver.db import duckdb_io
          expected = ["ACE", "DI", "DR", "FL", "HW", "TC"]
          db_url = os.environ.get("RESOLVER_DB_URL", "")
          hs_run_id = os.environ.get("HS_RUN_ID", "")
          iso3_raw = os.environ.get("HS_RESOLVED_ISO3S", "")
          iso3s = [iso3.strip().upper() for iso3 in iso3_raw.split(",") if iso3.strip()]
          missing = []
          if db_url and hs_run_id and iso3s:
              con = duckdb_io.get_db(db_url)
              try:
                  rows = con.execute(
                      "SELECT upper(iso3) AS iso3, upper(hazard_code) AS hazard_code "
                      "FROM hs_triage WHERE run_id = ?",
                      [hs_run_id],
                  ).fetchall()
              finally:
                  duckdb_io.close_db(con)
              present = {}
              for iso3, hazard in rows:
                  if iso3 and hazard:
                      present.setdefault(iso3, set()).add(hazard)
              for iso3 in sorted(set(iso3s)):
                  for hazard in expected:
                      if hazard not in present.get(iso3, set()):
                          missing.append((iso3, hazard))
          if missing:
              print(f"::warning::HS triage missing {len(missing)} hazard rows (see debug bundle section).")
          PY

      - name: Upload HS triage diagnostics
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: hs-triage-diagnostics
          path: |
            diagnostics/hs_assertion.md
            diagnostics/hs_stdout.txt
            debug/hs_triage_raw/*
          if-no-files-found: warn

      - name: Create questions from latest HS triage run
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -e
          echo "Creating questions from hs_triage into resolver.duckdb..."
          python -m scripts.create_questions_from_triage --db "${PYTHIA_DB_URL}"

      - name: Assert HS questions outputs
        id: assert_hs_questions
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
          HS_RUN_ID: ${{ env.HS_RUN_ID }}
          HS_RESOLVED_ISO3S: ${{ env.HS_RESOLVED_ISO3S }}
          HS_MODEL_ID: ${{ env.HS_MODEL_ID }}
        run: |
          set -e
          python -m py_compile scripts/ci/assert_hs_outputs.py
          python -m scripts.ci.assert_hs_outputs --db "${RESOLVER_DB_URL}" --hs-run-id "${HS_RUN_ID}" --stage questions

      - name: Capture HS question count
        if: ${{ success() }}
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
          HS_RUN_ID: ${{ env.HS_RUN_ID }}
        run: |
          set -e
          python - <<'PY'
          import os

          from resolver.db import duckdb_io

          db_url = os.environ["RESOLVER_DB_URL"]
          hs_run_id = os.environ["HS_RUN_ID"]
          con = duckdb_io.get_db(db_url)
          try:
              row = con.execute(
                  "SELECT COUNT(*) FROM questions WHERE hs_run_id = ?", [hs_run_id]
              ).fetchone()
              count = int(row[0]) if row else 0
          finally:
              duckdb_io.close_db(con)

          print(f"HS_QUESTION_COUNT={count}")
          with open(os.environ["GITHUB_ENV"], "a", encoding="utf-8") as handle:
              handle.write(f"HS_QUESTION_COUNT={count}\n")
          PY

      - name: Upload HS question diagnostics
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: hs-question-diagnostics
          path: |
            diagnostics/hs_assertion.md
            diagnostics/hs_stdout.txt
            debug/hs_triage_raw/*
          if-no-files-found: warn

      # Step 7: Run Forecaster on newly written HS questions (Pythia mode)
      - name: Diagnostics — Forecaster provider activation & ensemble composition
        if: ${{ success() && env.HS_QUESTION_COUNT != '0' }}
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
          PYTHIA_DEBUG_MODELS: "1"
          PYTHIA_LLM_PROFILE: prod

          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          MODEL_COSTS_JSON: ${{ secrets.MODEL_COSTS_JSON }}
        run: |
          set -e
          mkdir -p diagnostics
          python scripts/print_forecaster_ensemble.py | tee diagnostics/forecaster_ensemble.txt
          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f diagnostics/forecaster_ensemble.txt ]; then
            cat diagnostics/forecaster_ensemble.txt >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Run Forecaster (Pythia mode on HS questions)
        if: ${{ success() && env.HS_QUESTION_COUNT != '0' }}
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
          PYTHIA_LLM_PROFILE: prod
          PYTHIA_SPD_V2_WRITE_BOTH: "1"

          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          MODEL_COSTS_JSON: ${{ secrets.MODEL_COSTS_JSON }}
        run: |
          set -e
          mkdir -p diagnostics
          echo "=== Running Forecaster in Pythia mode on active HS questions ==="
          # No limit: forecast all questions from the current HS epoch.
          # Use a batch size to keep per-batch concurrency manageable for large epochs.
          python -m forecaster.cli \
            --mode pythia \
            --limit 0 \
            --batch-size 100 \
            --purpose hs_pipeline | tee diagnostics/forecaster_stdout.txt
          FORECASTER_RUN_ID=$(grep -Eo 'run_id=fc_[0-9]+' diagnostics/forecaster_stdout.txt | tail -n 1 | cut -d= -f2)
          if [ -n "${FORECASTER_RUN_ID}" ]; then
            {
              echo "FORECASTER_RUN_ID=${FORECASTER_RUN_ID}"
            } >> "$GITHUB_ENV"
            {
              echo "FORECASTER_RUN_ID=${FORECASTER_RUN_ID}"
            } >> "$GITHUB_OUTPUT"
          else
            echo "WARNING: Forecaster run id not detected in output."
          fi
          echo "=== Forecaster (Pythia mode) completed ==="

      - name: Verify Forecaster wrote both aggregation methods
        if: ${{ success() && env.HS_QUESTION_COUNT != '0' }}
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -e
          python -m scripts.ci.verify_forecaster_aggregations --db "${PYTHIA_DB_URL}"

      - name: Evaluate SPD aggregations (Brier + log score)
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.run_spd_eval == 'true' && env.HS_QUESTION_COUNT != '0' }}
        continue-on-error: true
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          set -e
          mkdir -p debug/eval
          python scripts/evaluate_spd_aggregations.py \
            --db "${PYTHIA_DB_URL}" \
            --model-names ensemble_mean_v2,ensemble_bayesmc_v2 \
            --out-dir debug/eval
          ls -la debug/eval

      # Step 8: Diagnostics – DuckDB validation
      - name: Diagnostics — DuckDB
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -e
          echo "=== Post-run diagnostics ==="
          echo "Top-level listing after run:"
          ls -la
          echo ""

          DB_PATH="data/resolver.duckdb"
          if [ -f "$DB_PATH" ]; then
            echo "DuckDB database found at $DB_PATH"
          else
            echo "WARNING: DuckDB database $DB_PATH not found."
          fi

          python scripts/post_run_diagnostics.py --db "${PYTHIA_DB_URL}"

      - name: Pythia v2 run summary
        run: |
          set -e
          python -m scripts.dump_pythia_v2_run_summary
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}

      - name: Compare DB signature after HS run
        if: always()
        id: signature_after
        env:
          REQUIRED_TABLES: ${{ env.SIGNATURE_REQUIRED_TABLES }}
          OPTIONAL_TABLES: ${{ env.SIGNATURE_OPTIONAL_TABLES }}
        run: |
          set -euo pipefail
          if [ ! -f data/resolver.duckdb ]; then
            echo "No resolver.duckdb found; skipping signature after."
            exit 0
          fi

          python -m scripts.ci.db_signature write \
            --db data/resolver.duckdb \
            --required "${REQUIRED_TABLES}" \
            --optional "${OPTIONAL_TABLES}" \
            --out diagnostics/db_signature_after.json

          signature_ok="true"
          compare_status=0
          if [ -f diagnostics/db_signature_before.json ]; then
            set +e
            python -m scripts.ci.db_signature compare \
              --before diagnostics/db_signature_before.json \
              --after-db data/resolver.duckdb \
              --required "${REQUIRED_TABLES}" \
              --optional "${OPTIONAL_TABLES}" \
              --out diagnostics/db_signature_after.json || compare_status=$?
            set -e
            if [ "${compare_status}" -ne 0 ]; then
              signature_ok="false"
            fi
          fi

          echo "SIGNATURE_OK=${signature_ok}" >> "$GITHUB_ENV"
          echo "signature_ok=${signature_ok}" >> "$GITHUB_OUTPUT"

          if [ -n "${GITHUB_STEP_SUMMARY:-}" ]; then
            {
              echo "### Canonical DB signature (after HS run)"
              echo "- Source workflow: ${CANONICAL_DB_SOURCE}"
              echo "- Run ID: ${CANONICAL_DB_RUN_ID}"
              echo "- Signature file: diagnostics/db_signature_after.json"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Dump unified Pythia v2 debug bundle
        if: ${{ success() }}
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
          HS_RUN_ID: ${{ env.HS_RUN_ID }}
          FORECASTER_RUN_ID: ${{ env.FORECASTER_RUN_ID }}
        run: |
          set -e
          mkdir -p diagnostics
          if [ -n "${FORECASTER_RUN_ID:-}" ]; then
            python -m scripts.dump_pythia_debug_bundle --db "${RESOLVER_DB_URL}" --hs-run-id "${HS_RUN_ID}" --forecaster-run-id "${FORECASTER_RUN_ID}"
          else
            python -m scripts.dump_pythia_debug_bundle --db "${RESOLVER_DB_URL}" --hs-run-id "${HS_RUN_ID}"
          fi
          BUNDLE_PATH=$(find debug -maxdepth 1 -type f -name "pytia_debug_bundle__*.md" -print | sort | tail -n 1 || true)
          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f "${BUNDLE_PATH}" ]; then
            cat "${BUNDLE_PATH}" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Diagnostics — LLM latency p50/p95 by phase/provider/model
        if: ${{ success() }}
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
          HS_RUN_ID: ${{ env.HS_RUN_ID }}
          FORECASTER_RUN_ID: ${{ env.FORECASTER_RUN_ID }}
        run: |
          set -e
          mkdir -p diagnostics
          python -m scripts.ci.llm_latency_summary --db "${RESOLVER_DB_URL}" --hs-run-id "${HS_RUN_ID}" --forecaster-run-id "${FORECASTER_RUN_ID}" | tee diagnostics/llm_latency_summary.md
          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f diagnostics/llm_latency_summary.md ]; then
            cat diagnostics/llm_latency_summary.md >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload unified debug bundle artifact
        uses: actions/upload-artifact@v4
        with:
          name: pythia-debug-bundle
          path: |
            debug/pytia_debug_bundle__*.md
            diagnostics/llm_latency_summary.md
            diagnostics/db_signature_before.json
            diagnostics/db_signature_after.json

      - name: Upload evaluation artifacts
        if: always() && github.event_name == 'workflow_dispatch' && github.event.inputs.run_spd_eval == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pythia-eval
          path: debug/eval/*
          if-no-files-found: warn

      - name: Upload canonical resolver DB
        if: success() && env.CANONICAL_DB_READY == 'true' && steps.signature_after.outputs.signature_ok == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pythia-resolver-db
          path: data/resolver.duckdb
