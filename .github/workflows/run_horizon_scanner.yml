# Pythia
# Copyright (c) 2025 Kevin Wyjad
# Licensed under the Pythia Non-Commercial Public License v1.0.
# See the LICENSE file in the project root for details.

---
# This workflow runs the Pythia Horizon Scanner triage stage.

name: Horizon Scanner Triage

concurrency:
  group: pythia-resolver-db
  cancel-in-progress: false

# --- Triggers ---
on:
  # Manual run from the Actions tab
  workflow_dispatch:
    inputs:
      run_spd_eval:
        description: "Run SPD aggregation evaluation (non-core)"
        required: false
        default: "false"
        type: choice
        options: ["false", "true"]
      db_run_id:
        description: "Optional: force canonical DB from Actions run ID (/actions/runs/<id>) containing artifact 'pythia-resolver-db'"
        required: false
        default: ""
        type: string
      db_artifact_name:
        description: "Artifact name to download from db_run_id (default: pythia-resolver-db). Use pythia-resolver-db-reset for bootstrap runs."
        required: false
        default: "pythia-resolver-db"
        type: string

# --- Job Definition ---
jobs:
  build-report:
    runs-on: ubuntu-latest
    env:
      RESOLVER_DB_URL: duckdb:///${{ github.workspace }}/data/resolver.duckdb
      PYTHIA_DB_URL: duckdb:///${{ github.workspace }}/data/resolver.duckdb
      PYTHIA_LLM_CONCURRENCY: "18"
      HS_MAX_WORKERS: "6"
      FORECASTER_RESEARCH_MAX_WORKERS: "6"
      FORECASTER_SPD_MAX_WORKERS: "6"
      SIGNATURE_REQUIRED_TABLES: questions,hs_triage,question_research,forecasts_ensemble,llm_calls,facts_resolved,facts_deltas,acled_monthly_fatalities
      SIGNATURE_OPTIONAL_TABLES: scenarios
      CANONICAL_DB_READY: "false"
      CANONICAL_DB_RUN_ID: ""
      CANONICAL_DB_SOURCE: ""
      CANONICAL_CREATED_AT: ""
      SIGNATURE_OK: "false"
      DB_SOURCE: ""

    steps:
      # Step 1: Check out the repository's code
      - name: Check out code
        uses: actions/checkout@v4

      - name: Install gh CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y gh

      - name: Authenticate gh
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh auth status

      # Step 2: Set up Python
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Diagnostics – workspace layout and key files
      - name: Diagnostics — workspace layout and key files
        run: |
          echo "=== Diagnostics: workspace layout ==="
          echo "PWD: $(pwd)"
          echo ""
          echo "Top-level listing:"
          ls -la
          echo ""
          echo "horizon_scanner directory listing:"
          if [ -d horizon_scanner ]; then
            ls -la horizon_scanner
          else
            echo "✘ MISSING: horizon_scanner directory"
          fi
          echo ""
          echo "Checking for Horizon Scanner files:"
          for f in \
            "horizon_scanner/horizon_scanner.py" \
            "horizon_scanner/db_writer.py" \
            "horizon_scanner/hs_country_list.txt" \
            "horizon_scanner/hs_prompt.py" \
            "python_library_requirements.txt"
          do
            if [ -f "$f" ]; then
              echo "✔ Found $f"
            else
              echo "✘ MISSING: $f"
            fi
          done
          echo ""
          if [ -f python_library_requirements.txt ]; then
            echo "=== python_library_requirements.txt (root) ==="
            cat python_library_requirements.txt
          else
            echo "WARNING: python_library_requirements.txt is missing in $(pwd); dependency install will fail."
          fi

      # Step 4: Install the necessary Python libraries
      - name: Install dependencies
        run: |
          set -e
          echo "Upgrading pip..."
          python -m pip install --upgrade pip
          echo ""
          echo "Installing dependencies from python_library_requirements.txt..."
          if [ ! -f python_library_requirements.txt ]; then
            echo "ERROR: python_library_requirements.txt not found in $(pwd)."
            exit 1
          fi
          pip install -r python_library_requirements.txt

      - name: Download canonical resolver DB with signature guardrail
        id: canonical_download
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p data diagnostics
          rm -f diagnostics/db_signature_before.json
          rm -rf data/pythia-resolver-db

          OVERRIDE_RUN_ID="${{ github.event.inputs.db_run_id }}"
          DB_ARTIFACT_NAME_INPUT="${{ github.event.inputs.db_artifact_name }}"
          DB_ARTIFACT_NAME="${DB_ARTIFACT_NAME_INPUT:-pythia-resolver-db}"

          download_artifact_with_retries() {
            local run_id="$1"
            local artifact_name="$2"
            local out_dir="$3"
            local max_attempts="${4:-6}"

            local attempt=1
            while [ "$attempt" -le "$max_attempts" ]; do
              echo "Downloading artifact '${artifact_name}' from run ${run_id} (attempt ${attempt}/${max_attempts})"

              # avoid partial/old contents
              rm -rf "${out_dir}/pythia-resolver-db" || true

              if gh run download "$run_id" -n "$artifact_name" --dir "$out_dir"; then
                echo "Download OK."
                return 0
              fi

              rc=$?
              sleep_seconds=$(( 5 * attempt * attempt ))  # 5, 20, 45, 80, 125...
              echo "Download failed (exit=${rc}). Sleeping ${sleep_seconds}s before retry..."
              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
            done

            echo "ERROR: Failed to download artifact '${artifact_name}' from run ${run_id} after ${max_attempts} attempts."
            return 1
          }

          if [ -n "$OVERRIDE_RUN_ID" ]; then
            if ! echo "$OVERRIDE_RUN_ID" | grep -Eq '^[0-9]+$'; then
              echo "ERROR: db_run_id must be a numeric Actions run id from /actions/runs/<id>. Got: '$OVERRIDE_RUN_ID'"
              exit 1
            fi

            DB_RUN_ID="$OVERRIDE_RUN_ID"
            DB_SOURCE="override"
            DB_CREATED_AT="manual-override"
          fi

          if [ -z "${DB_ARTIFACT_NAME:-}" ]; then
            DB_ARTIFACT_NAME="pythia-resolver-db"
          fi

          if [ -z "${DB_RUN_ID:-}" ]; then
            CANDIDATES_FILE=$(mktemp)
            gh run list \
              --workflow "Horizon Scanner Triage" \
              --branch main \
              --status success \
              --json databaseId,createdAt \
              --limit 20 | jq -r '.[] | "\(.createdAt),\(.databaseId),Horizon Scanner Triage"' >>"${CANDIDATES_FILE}" || true
            gh run list \
              --workflow "Resolver — Initial Backfill" \
              --branch main \
              --status success \
              --json databaseId,createdAt \
              --limit 20 | jq -r '.[] | "\(.createdAt),\(.databaseId),Resolver — Initial Backfill"' >>"${CANDIDATES_FILE}" || true

            if [ ! -s "${CANDIDATES_FILE}" ]; then
              echo "ERROR: No successful Horizon Scanner or Resolver — Initial Backfill runs found; refusing to start without canonical DB."
              exit 1
            fi

            sort -r "${CANDIDATES_FILE}" > "${CANDIDATES_FILE}.sorted"

            FOUND=0

            while IFS=',' read -r CREATED RUN_ID LABEL; do
              DB_RUN_ID="$RUN_ID"
              DB_CREATED_AT="$CREATED"
              if [ "$LABEL" = "Horizon Scanner Triage" ]; then
                DB_SOURCE="pipeline"
              else
                DB_SOURCE="backfill"
              fi

              DB_ARTIFACT_NAME="pythia-resolver-db"

              rm -rf data/pythia-resolver-db
              rm -f data/resolver.duckdb

              if ! download_artifact_with_retries "$DB_RUN_ID" "$DB_ARTIFACT_NAME" data 6; then
                echo "Download failed for run ${DB_RUN_ID}; trying next candidate."
                continue
              fi

              if [ -f data/pythia-resolver-db/resolver.duckdb ]; then
                SRC="data/pythia-resolver-db/resolver.duckdb"
              else
                SRC=$(find data -maxdepth 4 -type f -name "resolver.duckdb" | head -n 1 || true)
              fi

              if [ -z "${SRC}" ]; then
                echo "resolver.duckdb not found in downloaded artifact from ${DB_RUN_ID}; trying next candidate."
                continue
              fi

              if [ "$SRC" != "data/resolver.duckdb" ]; then
                cp "$SRC" data/resolver.duckdb
              fi

              if python -m scripts.ci.db_signature write \
                  --db data/resolver.duckdb \
                  --required "${SIGNATURE_REQUIRED_TABLES}" \
                  --optional "${SIGNATURE_OPTIONAL_TABLES}" \
                  --out diagnostics/db_signature_before.json; then
                FOUND=1
                break
              else
                rc=$?
                echo "Candidate rejected (exit ${rc}); missing required tables or invalid signature. Trying next candidate."
                rm -f diagnostics/db_signature_before.json
              fi
            done < "${CANDIDATES_FILE}.sorted"

            if [ "${FOUND}" -ne 1 ]; then
              echo "ERROR: No valid canonical DB found after checking Horizon Scanner and Resolver — Initial Backfill runs."
              exit 1
            fi
          fi

          if [ -z "${DB_RUN_ID:-}" ]; then
            echo "ERROR: No successful Horizon Scanner or Resolver — Initial Backfill runs found; refusing to start without canonical DB."
            exit 1
          fi

          if [ -n "$OVERRIDE_RUN_ID" ]; then
            rm -rf data/pythia-resolver-db
            rm -f data/resolver.duckdb
            if ! download_artifact_with_retries "$DB_RUN_ID" "${DB_ARTIFACT_NAME:-pythia-resolver-db}" data 6; then
              exit 1
            fi
          fi

          if [ -f data/pythia-resolver-db/resolver.duckdb ]; then
            SRC="data/pythia-resolver-db/resolver.duckdb"
          else
            SRC=$(find data -maxdepth 4 -type f -name "resolver.duckdb" | head -n 1 || true)
          fi

          if [ -z "${SRC}" ]; then
            echo "ERROR: resolver.duckdb not found in downloaded artifact from ${DB_RUN_ID}."
            exit 1
          fi

          if [ "$SRC" != "data/resolver.duckdb" ]; then
            cp "$SRC" data/resolver.duckdb
          fi

          # Bootstrap-only: if we're starting from a fresh reset DB, ensure Pythia schema tables exist before signature check.
          if [ "${DB_ARTIFACT_NAME:-pythia-resolver-db}" = "pythia-resolver-db-reset" ]; then
            echo "Bootstrap DB detected (${DB_ARTIFACT_NAME}); ensuring Pythia schema tables exist before signature check."
            PYTHIA_DB_URL="${RESOLVER_DB_URL}" python -c "from pythia.db.schema import ensure_schema; ensure_schema(); print('ensure_schema(): OK')"
          fi

          python -m scripts.ci.db_signature write \
            --db data/resolver.duckdb \
            --required "${SIGNATURE_REQUIRED_TABLES}" \
            --optional "${SIGNATURE_OPTIONAL_TABLES}" \
            --out diagnostics/db_signature_before.json

          if [ ! -f diagnostics/db_signature_before.json ]; then
            echo "ERROR: Signature missing after validating run ${DB_RUN_ID}."
            exit 1
          fi

          {
            echo "CANONICAL_DB_READY=true"
            echo "CANONICAL_DB_SOURCE=${DB_SOURCE:-unknown}"
            echo "CANONICAL_DB_RUN_ID=${DB_RUN_ID}"
            echo "CANONICAL_CREATED_AT=${DB_CREATED_AT:-manual}"
            echo "DB_RUN_ID=${DB_RUN_ID}"
            echo "DB_SOURCE=${DB_SOURCE:-unknown}"
            echo "DB_CREATED_AT=${DB_CREATED_AT:-manual}"
            echo "DB_ARTIFACT_NAME=${DB_ARTIFACT_NAME:-pythia-resolver-db}"
          } >> "$GITHUB_ENV"
          CANONICAL_DB_SOURCE="${DB_SOURCE:-unknown}"
          CANONICAL_DB_RUN_ID="${DB_RUN_ID}"
          CANONICAL_CREATED_AT="${DB_CREATED_AT:-manual}"
          echo "Baseline signature written to diagnostics/db_signature_before.json"

          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f diagnostics/db_signature_before.json ]; then
            {
              echo "### Canonical DB signature (before HS run)"
              echo "- Source workflow: ${CANONICAL_DB_SOURCE}"
              echo "- Run ID: ${CANONICAL_DB_RUN_ID}"
              echo "- Created: ${CANONICAL_CREATED_AT}"
              echo "- Signature file: diagnostics/db_signature_before.json"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

          echo "DB selection: source=${DB_SOURCE:-unknown} run_id=${DB_RUN_ID} created=${DB_CREATED_AT:-manual} artifact=${DB_ARTIFACT_NAME:-pythia-resolver-db}"

      # Step 5: Diagnostics – confirm GEMINI_API_KEY is present
      - name: Diagnostics — GEMINI_API_KEY presence
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          if [ -z "${GEMINI_API_KEY:-}" ]; then
            echo "ERROR: GEMINI_API_KEY is NOT set."
            echo "Set it under: Settings > Secrets and variables > Actions."
            exit 1
          else
            echo "GEMINI_API_KEY is set (value hidden)."
          fi

      # Step 6: Run the main Python script (as a module)
      - name: Run Horizon Scanner
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          set -e
          echo "=== Running horizon_scanner.horizon_scanner as a module ==="
          python -m horizon_scanner.horizon_scanner
          echo "=== horizon_scanner.horizon_scanner completed ==="

      - name: Create questions from latest HS triage run
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -e
          echo "Creating questions from hs_triage into resolver.duckdb..."
          python -m scripts.create_questions_from_triage --db "${PYTHIA_DB_URL}"

      # Step 7: Run Forecaster on newly written HS questions (Pythia mode)
      - name: Diagnostics — Forecaster provider activation & ensemble composition
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
          PYTHIA_DEBUG_MODELS: "1"
          PYTHIA_LLM_PROFILE: prod

          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          MODEL_COSTS_JSON: ${{ secrets.MODEL_COSTS_JSON }}
        run: |
          set -e
          mkdir -p diagnostics
          python scripts/print_forecaster_ensemble.py | tee diagnostics/forecaster_ensemble.txt
          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f diagnostics/forecaster_ensemble.txt ]; then
            cat diagnostics/forecaster_ensemble.txt >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Run Forecaster (Pythia mode on HS questions)
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
          PYTHIA_LLM_PROFILE: prod
          PYTHIA_SPD_V2_WRITE_BOTH: "1"

          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          MODEL_COSTS_JSON: ${{ secrets.MODEL_COSTS_JSON }}
        run: |
          set -e
          echo "=== Running Forecaster in Pythia mode on active HS questions ==="
          # No limit: forecast all questions from the current HS epoch.
          # Use a batch size to keep per-batch concurrency manageable for large epochs.
          python -m forecaster.cli \
            --mode pythia \
            --limit 0 \
            --batch-size 100 \
            --purpose hs_pipeline
          echo "=== Forecaster (Pythia mode) completed ==="

      - name: Verify Forecaster wrote both aggregation methods
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -e
          python -m scripts.ci.verify_forecaster_aggregations --db "${PYTHIA_DB_URL}"

      - name: Evaluate SPD aggregations (Brier + log score)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_spd_eval == 'true'
        continue-on-error: true
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          set -e
          mkdir -p debug/eval
          python scripts/evaluate_spd_aggregations.py \
            --db "${PYTHIA_DB_URL}" \
            --model-names ensemble_mean_v2,ensemble_bayesmc_v2 \
            --out-dir debug/eval
          ls -la debug/eval

      # Step 8: Diagnostics – DuckDB validation
      - name: Diagnostics — DuckDB
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -e
          echo "=== Post-run diagnostics ==="
          echo "Top-level listing after run:"
          ls -la
          echo ""

          DB_PATH="data/resolver.duckdb"
          if [ -f "$DB_PATH" ]; then
            echo "DuckDB database found at $DB_PATH"
          else
            echo "WARNING: DuckDB database $DB_PATH not found."
          fi

          python scripts/post_run_diagnostics.py --db "${PYTHIA_DB_URL}"

      - name: Pythia v2 run summary
        run: |
          set -e
          python -m scripts.dump_pythia_v2_run_summary
        env:
          PYTHIA_DB_URL: ${{ env.RESOLVER_DB_URL }}

      - name: Dump unified Pythia v2 debug bundle
        run: python -m scripts.dump_pythia_debug_bundle --db "${{ env.RESOLVER_DB_URL }}"

      - name: Diagnostics — LLM latency p50/p95 by phase/provider/model
        run: |
          set -e
          mkdir -p diagnostics
          python -m scripts.ci.llm_latency_summary --db "${RESOLVER_DB_URL}" | tee diagnostics/llm_latency_summary.md
          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f diagnostics/llm_latency_summary.md ]; then
            cat diagnostics/llm_latency_summary.md >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload unified debug bundle artifact
        uses: actions/upload-artifact@v4
        with:
          name: pythia-debug-bundle
          path: |
            debug/pytia_debug_bundle__*.md
            diagnostics/llm_latency_summary.md

      - name: Upload evaluation artifacts
        if: always() && github.event_name == 'workflow_dispatch' && github.event.inputs.run_spd_eval == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pythia-eval
          path: debug/eval/*
          if-no-files-found: warn

      - name: Compare DB signature after HS run
        id: signature_after
        if: env.CANONICAL_DB_READY == 'true'
        env:
          REQUIRED_TABLES: ${{ env.SIGNATURE_REQUIRED_TABLES }}
          OPTIONAL_TABLES: ${{ env.SIGNATURE_OPTIONAL_TABLES }}
        run: |
          set -euo pipefail
          python -m scripts.ci.db_signature compare \
            --before diagnostics/db_signature_before.json \
            --after-db data/resolver.duckdb \
            --required "${REQUIRED_TABLES}" \
            --optional "${OPTIONAL_TABLES}" \
            --out diagnostics/db_signature_after.json

          echo "SIGNATURE_OK=true" >> "$GITHUB_ENV"
          echo "signature_ok=true" >> "$GITHUB_OUTPUT"

          if [ -n "${GITHUB_STEP_SUMMARY:-}" ]; then
            {
              echo "### Canonical DB signature (after HS run)"
              echo "- Source workflow: ${CANONICAL_DB_SOURCE}"
              echo "- Run ID: ${CANONICAL_DB_RUN_ID}"
              echo "- Signature file: diagnostics/db_signature_after.json"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload canonical resolver DB
        if: success() && env.CANONICAL_DB_READY == 'true' && steps.signature_after.outputs.signature_ok == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pythia-resolver-db
          path: data/resolver.duckdb
