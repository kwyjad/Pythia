# Pythia
# Copyright (c) 2025 Kevin Wyjad
# Licensed under the Pythia Non-Commercial Public License v1.0.
# See the LICENSE file in the project root for details.

---
# This workflow runs the Pythia Horizon Scanner triage stage.

name: Horizon Scanner Triage

concurrency:
  group: pythia-resolver-db
  cancel-in-progress: false

# --- Triggers ---
on:
  # Manual run from the Actions tab
  workflow_dispatch:
    inputs:
      run_spd_eval:
        description: "Run SPD aggregation evaluation (non-core)"
        required: false
        default: "false"
        type: choice
        options: ["false", "true"]

# --- Job Definition ---
jobs:
  build-report:
    runs-on: ubuntu-latest
    env:
      RESOLVER_DB_URL: duckdb:///${{ github.workspace }}/data/resolver.duckdb
      SIGNATURE_REQUIRED_TABLES: questions,hs_triage,question_research,forecasts_ensemble,llm_calls,facts_resolved,facts_deltas,acled_monthly_fatalities
      SIGNATURE_OPTIONAL_TABLES: scenarios
      CANONICAL_DB_READY: "false"
      CANONICAL_DB_RUN_ID: ""
      CANONICAL_DB_SOURCE: ""
      CANONICAL_CREATED_AT: ""
      SIGNATURE_OK: "false"

    steps:
      # Step 1: Check out the repository's code
      - name: Check out code
        uses: actions/checkout@v4

      - name: Install gh CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y gh

      - name: Authenticate gh
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh auth status

      # Step 2: Set up Python
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Diagnostics – workspace layout and key files
      - name: Diagnostics — workspace layout and key files
        run: |
          echo "=== Diagnostics: workspace layout ==="
          echo "PWD: $(pwd)"
          echo ""
          echo "Top-level listing:"
          ls -la
          echo ""
          echo "horizon_scanner directory listing:"
          if [ -d horizon_scanner ]; then
            ls -la horizon_scanner
          else
            echo "✘ MISSING: horizon_scanner directory"
          fi
          echo ""
          echo "Checking for Horizon Scanner files:"
          for f in \
            "horizon_scanner/horizon_scanner.py" \
            "horizon_scanner/db_writer.py" \
            "horizon_scanner/hs_country_list.txt" \
            "horizon_scanner/hs_prompt.py" \
            "python_library_requirements.txt"
          do
            if [ -f "$f" ]; then
              echo "✔ Found $f"
            else
              echo "✘ MISSING: $f"
            fi
          done
          echo ""
          if [ -f python_library_requirements.txt ]; then
            echo "=== python_library_requirements.txt (root) ==="
            cat python_library_requirements.txt
          else
            echo "WARNING: python_library_requirements.txt is missing in $(pwd); dependency install will fail."
          fi

      # Step 4: Install the necessary Python libraries
      - name: Install dependencies
        run: |
          set -e
          echo "Upgrading pip..."
          python -m pip install --upgrade pip
          echo ""
          echo "Installing dependencies from python_library_requirements.txt..."
          if [ ! -f python_library_requirements.txt ]; then
            echo "ERROR: python_library_requirements.txt not found in $(pwd)."
            exit 1
          fi
          pip install -r python_library_requirements.txt

      - name: Download canonical resolver DB with signature guardrail
        id: canonical_download
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p data diagnostics
          rm -f diagnostics/db_signature_before.json
          rm -rf data/pythia-resolver-db

          CANDIDATES_FILE=$(mktemp)
          gh run list \
            --workflow "Horizon Scanner Triage" \
            --branch main \
            --status success \
            --json databaseId,createdAt \
            --limit 1 | jq -r '.[] | "\(.createdAt),\(.databaseId),Horizon Scanner Triage"' >>"${CANDIDATES_FILE}" || true
          gh run list \
            --workflow "Resolver — Initial Backfill" \
            --branch main \
            --status success \
            --json databaseId,createdAt \
            --limit 1 | jq -r '.[] | "\(.createdAt),\(.databaseId),Resolver — Initial Backfill"' >>"${CANDIDATES_FILE}" || true

          if [ ! -s "${CANDIDATES_FILE}" ]; then
            echo "ERROR: No successful Horizon Scanner or Resolver — Initial Backfill runs found; refusing to start without canonical DB."
            exit 1
          fi

          sort -r "${CANDIDATES_FILE}" > "${CANDIDATES_FILE}.sorted"
          while IFS=',' read -r CREATED RUN_ID LABEL; do
            echo "Attempting canonical DB download from ${LABEL} (run ${RUN_ID}, created ${CREATED})"
            rm -rf data/pythia-resolver-db
            rm -f data/resolver.duckdb
            if ! gh run download "$RUN_ID" -n pythia-resolver-db --dir data; then
              echo "Download failed for run ${RUN_ID}; trying next candidate."
              continue
            fi

            if [ -f data/pythia-resolver-db/resolver.duckdb ]; then
              SRC="data/pythia-resolver-db/resolver.duckdb"
            else
              SRC=$(find data -maxdepth 4 -type f -name "resolver.duckdb" | head -n 1 || true)
            fi

            if [ -z "${SRC}" ]; then
              echo "resolver.duckdb not found in downloaded artifact from ${RUN_ID}; trying next candidate."
              continue
            fi

            if [ "$SRC" != "data/resolver.duckdb" ]; then
              cp "$SRC" data/resolver.duckdb
            fi

            python - <<'PY'
            import json
            import os
            import sys
            import duckdb

            db_path = "data/resolver.duckdb"
            required = [t.strip() for t in os.environ.get("SIGNATURE_REQUIRED_TABLES", "").split(",") if t.strip()]
            optional = [t.strip() for t in os.environ.get("SIGNATURE_OPTIONAL_TABLES", "").split(",") if t.strip()]

            if not os.path.isfile(db_path):
                sys.exit("resolver.duckdb missing after download")

            con = duckdb.connect(db_path)
            try:
                tables = sorted(r[0] for r in con.execute("SHOW TABLES").fetchall())
                counts = {}
                for table in required + optional:
                    if table in tables:
                        counts[table] = int(con.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0])
                    else:
                        counts[table] = None
            finally:
                con.close()

            missing = [t for t in required if counts.get(t) is None]
            if missing:
                sys.exit(f"Missing required tables: {missing}")

            signature = {
                "db_path": os.path.abspath(db_path),
                "size_bytes": os.path.getsize(db_path),
                "tables": tables,
                "required_counts": {t: counts.get(t) for t in required},
                "optional_counts": {t: counts.get(t) for t in optional},
            }

            os.makedirs("diagnostics", exist_ok=True)
            out_path = "diagnostics/db_signature_before.json"
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(signature, f, indent=2, sort_keys=True)

            print(f"DB size (bytes): {signature['size_bytes']}")
            print("Tables:", ", ".join(signature["tables"]))
            print("Required row counts:")
            for table in required:
                print(f"  - {table}: {signature['required_counts'][table]}")
            print("Optional row counts:")
            for table in optional:
                print(f"  - {table}: {signature['optional_counts'][table]}")
            PY

            if [ -f diagnostics/db_signature_before.json ]; then
              echo "CANONICAL_DB_READY=true" >> "$GITHUB_ENV"
              echo "CANONICAL_DB_SOURCE=${LABEL}" >> "$GITHUB_ENV"
              echo "CANONICAL_DB_RUN_ID=${RUN_ID}" >> "$GITHUB_ENV"
              echo "CANONICAL_CREATED_AT=${CREATED}" >> "$GITHUB_ENV"
              echo "Baseline signature written to diagnostics/db_signature_before.json"
              break
            else
              echo "Signature missing after validating run ${RUN_ID}; trying next candidate."
            fi
          done < "${CANDIDATES_FILE}.sorted"

          if [ "${CANONICAL_DB_READY:-false}" != "true" ]; then
            echo "ERROR: Could not find a valid canonical resolver.duckdb with required tables; aborting."
            exit 1
          fi

          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f diagnostics/db_signature_before.json ]; then
            {
              echo "### Canonical DB signature (before HS run)"
              echo "- Source workflow: ${CANONICAL_DB_SOURCE}"
              echo "- Run ID: ${CANONICAL_DB_RUN_ID}"
              echo "- Created: ${CANONICAL_CREATED_AT}"
              echo "- Signature file: diagnostics/db_signature_before.json"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      # Step 5: Diagnostics – confirm GEMINI_API_KEY is present
      - name: Diagnostics — GEMINI_API_KEY presence
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          if [ -z "${GEMINI_API_KEY:-}" ]; then
            echo "ERROR: GEMINI_API_KEY is NOT set."
            echo "Set it under: Settings > Secrets and variables > Actions."
            exit 1
          else
            echo "GEMINI_API_KEY is set (value hidden)."
          fi

      # Step 6: Run the main Python script (as a module)
      - name: Run Horizon Scanner
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          set -e
          echo "=== Running horizon_scanner.horizon_scanner as a module ==="
          python -m horizon_scanner.horizon_scanner
          echo "=== horizon_scanner.horizon_scanner completed ==="

      - name: Create questions from latest HS triage run
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb
        run: |
          set -e
          echo "Creating questions from hs_triage into resolver.duckdb..."
          python -m scripts.create_questions_from_triage --db duckdb:///data/resolver.duckdb

      - name: Create demo questions in resolver.duckdb (disabled)
        run: |
          set -e
          echo "Demo questions disabled; skipping inserts."
          python -m scripts.create_demo_questions --db duckdb:///data/resolver.duckdb
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb

      # Step 7: Run Forecaster on newly written HS questions (Pythia mode)
      - name: Run Forecaster (Pythia mode on HS questions)
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb
          PYTHIA_LLM_PROFILE: prod
          PYTHIA_SPD_V2_WRITE_BOTH: "1"

          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          MODEL_COSTS_JSON: ${{ secrets.MODEL_COSTS_JSON }}
        run: |
          set -e
          echo "=== Running Forecaster in Pythia mode on active HS questions ==="
          # No limit: forecast all questions from the current HS epoch.
          # Use a batch size to keep per-batch concurrency manageable for large epochs.
          python -m forecaster.cli \
            --mode pythia \
            --limit 0 \
            --batch-size 100 \
            --purpose hs_pipeline
          echo "=== Forecaster (Pythia mode) completed ==="

      - name: Verify Forecaster wrote both aggregation methods
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb
        run: |
          set -e
          python - <<'PY'
          import os

          from resolver.db import duckdb_io

          db_url = os.environ.get("PYTHIA_DB_URL", "duckdb:///data/resolver.duckdb")

          con = duckdb_io.get_db(db_url)
          try:
              # show latest run_id
              run_id = con.execute("SELECT max(run_id) FROM forecasts_ensemble").fetchone()[0]
              print("latest_run_id:", run_id)

              rows = con.execute(
                  """
                  SELECT model_name, count(*) as n
                  FROM forecasts_ensemble
                  WHERE run_id = ?
                  GROUP BY 1
                  ORDER BY n DESC
                  """,
                  [run_id],
              ).fetchall()

              print("model_name counts:", rows)

              # Fail if we don't see BOTH expected model names
              names = {r[0] for r in rows}
              needed = {"ensemble_mean_v2", "ensemble_bayesmc_v2"}
              missing = needed - names
              if missing:
                  raise SystemExit(
                      f"Missing expected model_names in forecasts_ensemble: {missing}"
                  )

              pairs = con.execute(
                  """
                  SELECT question_id,
                         SUM(CASE WHEN model_name='ensemble_mean_v2' THEN 1 ELSE 0 END) AS n_mean,
                         SUM(CASE WHEN model_name='ensemble_bayesmc_v2' THEN 1 ELSE 0 END) AS n_bmc
                  FROM forecasts_ensemble
                  WHERE run_id = ?
                  GROUP BY 1
                  HAVING n_mean = 0 OR n_bmc = 0
                  LIMIT 20
                  """,
                  [run_id],
              ).fetchall()

              if pairs:
                  raise SystemExit(
                      f"Some questions missing one aggregation method (showing up to 20): {pairs}"
                  )

              print("OK: both aggregation methods present.")
          finally:
              duckdb_io.close_db(con)
          PY

      - name: Evaluate SPD aggregations (Brier + log score)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_spd_eval == 'true'
        continue-on-error: true
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb
          PYTHONPATH: ${{ github.workspace }}
        run: |
          set -e
          mkdir -p debug/eval
          python scripts/evaluate_spd_aggregations.py \
            --db duckdb:///data/resolver.duckdb \
            --model-names ensemble_mean_v2,ensemble_bayesmc_v2 \
            --out-dir debug/eval
          ls -la debug/eval

      # Step 8: Diagnostics – DuckDB validation
      - name: Diagnostics — DuckDB
        run: |
          set -e
          echo "=== Post-run diagnostics ==="
          echo "Top-level listing after run:"
          ls -la
          echo ""

          DB_PATH="data/resolver.duckdb"
          if [ -f "$DB_PATH" ]; then
            echo "DuckDB database found at $DB_PATH"
          else
            echo "WARNING: DuckDB database $DB_PATH not found."
          fi

          python scripts/post_run_diagnostics.py --db duckdb:///data/resolver.duckdb

      - name: Pythia v2 run summary
        run: |
          set -e
          python -m scripts.dump_pythia_v2_run_summary
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb

      - name: Dump unified Pythia v2 debug bundle
        run: python -m scripts.dump_pythia_debug_bundle --db duckdb:///data/resolver.duckdb

      - name: Upload unified debug bundle artifact
        uses: actions/upload-artifact@v4
        with:
          name: pythia-debug-bundle
          path: debug/pytia_debug_bundle__*.md

      - name: Upload evaluation artifacts
        if: always() && github.event_name == 'workflow_dispatch' && github.event.inputs.run_spd_eval == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pythia-eval
          path: debug/eval/*
          if-no-files-found: warn

      - name: Compare DB signature after HS run
        id: signature_after
        if: env.CANONICAL_DB_READY == 'true'
        env:
          REQUIRED_TABLES: ${{ env.SIGNATURE_REQUIRED_TABLES }}
          OPTIONAL_TABLES: ${{ env.SIGNATURE_OPTIONAL_TABLES }}
        run: |
          set -euo pipefail
          python - <<'PY'
            import json
            import os
            import sys
            import duckdb

            required = [t.strip() for t in os.environ.get("REQUIRED_TABLES", "").split(",") if t.strip()]
            optional = [t.strip() for t in os.environ.get("OPTIONAL_TABLES", "").split(",") if t.strip()]
            before_path = "diagnostics/db_signature_before.json"
            db_path = "data/resolver.duckdb"

            if not os.path.isfile(before_path):
                sys.exit(f"Baseline signature not found: {before_path}")

            if not os.path.isfile(db_path):
                sys.exit(f"Database not found at {db_path}")

            with open(before_path, "r", encoding="utf-8") as f:
                before = json.load(f)

            con = duckdb.connect(db_path)
            try:
                tables = sorted(r[0] for r in con.execute("SHOW TABLES").fetchall())
                counts = {}
                for table in required + optional:
                    if table in tables:
                        counts[table] = int(con.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0])
                    else:
                        counts[table] = None
            finally:
                con.close()

            missing = [t for t in required if counts.get(t) is None]
            if missing:
                sys.exit(f"Missing required tables after HS run: {missing}")

            regressing = []
            for table in required:
                before_count = before.get("required_counts", {}).get(table)
                after_count = counts.get(table)
                if before_count is not None and after_count is not None and after_count < before_count:
                    regressing.append((table, before_count, after_count))

            if regressing:
                lines = ", ".join(f"{t}: {after} < {before}" for t, before, after in regressing)
                sys.exit(f"Required tables regressed: {lines}")

            signature = {
                "db_path": os.path.abspath(db_path),
                "size_bytes": os.path.getsize(db_path),
                "tables": tables,
                "required_counts": {t: counts.get(t) for t in required},
                "optional_counts": {t: counts.get(t) for t in optional},
            }

            os.makedirs("diagnostics", exist_ok=True)
            after_path = "diagnostics/db_signature_after.json"
            with open(after_path, "w", encoding="utf-8") as f:
                json.dump(signature, f, indent=2, sort_keys=True)

            print(f"DB size (bytes): {signature['size_bytes']}")
            print("Required row counts (after):")
            for table in required:
                print(f"  - {table}: {signature['required_counts'][table]}")
            print("Optional row counts (after):")
            for table in optional:
                print(f"  - {table}: {signature['optional_counts'][table]}")
          PY

          echo "SIGNATURE_OK=true" >> "$GITHUB_ENV"
          echo "signature_ok=true" >> "$GITHUB_OUTPUT"

          if [ -n "${GITHUB_STEP_SUMMARY:-}" ]; then
            {
              echo "### Canonical DB signature (after HS run)"
              echo "- Source workflow: ${CANONICAL_DB_SOURCE}"
              echo "- Run ID: ${CANONICAL_DB_RUN_ID}"
              echo "- Signature file: diagnostics/db_signature_after.json"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload canonical resolver DB
        if: success() && env.CANONICAL_DB_READY == 'true' && steps.signature_after.outputs.signature_ok == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pythia-resolver-db
          path: data/resolver.duckdb
