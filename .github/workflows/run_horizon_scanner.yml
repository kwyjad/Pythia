# Pythia
# Copyright (c) 2025 Kevin Wyjad
# Licensed under the Pythia Non-Commercial Public License v1.0.
# See the LICENSE file in the project root for details.

---
# This workflow runs the Pythia Horizon Scanner triage stage.

name: Horizon Scanner Triage

concurrency:
  group: pythia-resolver-db
  cancel-in-progress: false

# --- Triggers ---
on:
  # Manual run from the Actions tab
  workflow_dispatch:
    inputs:
      run_spd_eval:
        description: "Run SPD aggregation evaluation (non-core)"
        required: false
        default: "false"
        type: choice
        options: ["false", "true"]

# --- Job Definition ---
jobs:
  build-report:
    runs-on: ubuntu-latest
    env:
      RESOLVER_DB_URL: duckdb:///${{ github.workspace }}/data/resolver.duckdb

    steps:
      # Step 1: Check out the repository's code
      - name: Check out code
        uses: actions/checkout@v4

      - name: Install gh CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y gh

      - name: Authenticate gh
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh auth status

      - name: Select resolver DB artifact
        id: select_resolver_db
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PIPELINE_RUN_ID=$(gh run list \
            --workflow "${{ github.workflow }}" \
            --branch main \
            --status success \
            --json databaseId \
            --limit 1 \
            --jq '.[0].databaseId' || true)

          BACKFILL_RUN_ID=$(gh run list \
            --workflow "Resolver — Initial Backfill" \
            --branch main \
            --status success \
            --json databaseId \
            --limit 1 \
            --jq '.[0].databaseId' || true)

          if [ -n "$PIPELINE_RUN_ID" ]; then
            echo "Using DB from latest pipeline run: $PIPELINE_RUN_ID"
            echo "DB_RUN_ID=$PIPELINE_RUN_ID" >> "$GITHUB_ENV"
          elif [ -n "$BACKFILL_RUN_ID" ]; then
            echo "No pipeline DB found; falling back to backfill run: $BACKFILL_RUN_ID"
            echo "DB_RUN_ID=$BACKFILL_RUN_ID" >> "$GITHUB_ENV"
          else
            echo "No existing DB artifacts found; HS will run on a fresh DB."
          fi

      - name: Download canonical resolver DB
        if: env.DB_RUN_ID != ''
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p data

          echo "Downloading pythia-resolver-db from run $DB_RUN_ID ..."
          gh run download "$DB_RUN_ID" -n pythia-resolver-db --dir data

          echo "Downloaded artifacts:"
          ls -R data

          # Canonical layout: data/pythia-resolver-db/resolver.duckdb
          if [ -f data/pythia-resolver-db/resolver.duckdb ]; then
            SRC="data/pythia-resolver-db/resolver.duckdb"
          else
            SRC=$(find data -maxdepth 4 -type f -name "resolver.duckdb" | head -n 1 || true)
          fi

          if [ -n "$SRC" ]; then
            echo "Found resolver DB at $SRC"
            # Only copy if SRC != canonical path
            if [ "$SRC" != "data/resolver.duckdb" ]; then
              cp "$SRC" data/resolver.duckdb
            else
              echo "DB already at canonical path data/resolver.duckdb; no copy needed."
            fi
          fi

          if [ ! -f data/resolver.duckdb ]; then
            echo "WARNING: resolver.duckdb not found in pythia-resolver-db; HS will run on a fresh DB."
          else
            echo "Using canonical DB at data/resolver.duckdb"
          fi

          if [ -n "${DB_RUN_ID:-}" ] && [ ! -f data/resolver.duckdb ]; then
            echo "ERROR: Expected to download resolver.duckdb from artifact run $DB_RUN_ID but did not find it."
            exit 1
          fi

      # Step 2: Set up Python
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Diagnostics – workspace layout and key files
      - name: Diagnostics — workspace layout and key files
        run: |
          echo "=== Diagnostics: workspace layout ==="
          echo "PWD: $(pwd)"
          echo ""
          echo "Top-level listing:"
          ls -la
          echo ""
          echo "horizon_scanner directory listing:"
          if [ -d horizon_scanner ]; then
            ls -la horizon_scanner
          else
            echo "✘ MISSING: horizon_scanner directory"
          fi
          echo ""
          echo "Checking for Horizon Scanner files:"
          for f in \
            "horizon_scanner/horizon_scanner.py" \
            "horizon_scanner/db_writer.py" \
            "horizon_scanner/hs_country_list.txt" \
            "horizon_scanner/hs_prompt.py" \
            "python_library_requirements.txt"
          do
            if [ -f "$f" ]; then
              echo "✔ Found $f"
            else
              echo "✘ MISSING: $f"
            fi
          done
          echo ""
          if [ -f python_library_requirements.txt ]; then
            echo "=== python_library_requirements.txt (root) ==="
            cat python_library_requirements.txt
          else
            echo "WARNING: python_library_requirements.txt is missing in $(pwd); dependency install will fail."
          fi

      # Step 4: Install the necessary Python libraries
      - name: Install dependencies
        run: |
          set -e
          echo "Upgrading pip..."
          python -m pip install --upgrade pip
          echo ""
          echo "Installing dependencies from python_library_requirements.txt..."
          if [ ! -f python_library_requirements.txt ]; then
            echo "ERROR: python_library_requirements.txt not found in $(pwd)."
            exit 1
          fi
          pip install -r python_library_requirements.txt

      # Step 5: Diagnostics – confirm GEMINI_API_KEY is present
      - name: Diagnostics — GEMINI_API_KEY presence
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          if [ -z "${GEMINI_API_KEY:-}" ]; then
            echo "ERROR: GEMINI_API_KEY is NOT set."
            echo "Set it under: Settings > Secrets and variables > Actions."
            exit 1
          else
            echo "GEMINI_API_KEY is set (value hidden)."
          fi

      # Step 6: Run the main Python script (as a module)
      - name: Run Horizon Scanner
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          set -e
          echo "=== Running horizon_scanner.horizon_scanner as a module ==="
          python -m horizon_scanner.horizon_scanner
          echo "=== horizon_scanner.horizon_scanner completed ==="

      - name: Create questions from latest HS triage run
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb
        run: |
          set -e
          echo "Creating questions from hs_triage into resolver.duckdb..."
          python -m scripts.create_questions_from_triage --db duckdb:///data/resolver.duckdb

      - name: Create demo questions in resolver.duckdb (disabled)
        run: |
          set -e
          echo "Demo questions disabled; skipping inserts."
          python -m scripts.create_demo_questions --db duckdb:///data/resolver.duckdb
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb

      # Step 7: Run Forecaster on newly written HS questions (Pythia mode)
      - name: Run Forecaster (Pythia mode on HS questions)
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb
          PYTHIA_LLM_PROFILE: prod
          PYTHIA_SPD_V2_WRITE_BOTH: "1"

          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          MODEL_COSTS_JSON: ${{ secrets.MODEL_COSTS_JSON }}
        run: |
          set -e
          echo "=== Running Forecaster in Pythia mode on active HS questions ==="
          # No limit: forecast all questions from the current HS epoch.
          # Use a batch size to keep per-batch concurrency manageable for large epochs.
          python -m forecaster.cli \
            --mode pythia \
            --limit 0 \
            --batch-size 100 \
            --purpose hs_pipeline
          echo "=== Forecaster (Pythia mode) completed ==="

      - name: Verify Forecaster wrote both aggregation methods
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb
        run: |
          set -e
          python - <<'PY'
          import os

          from resolver.db import duckdb_io

          db_url = os.environ.get("PYTHIA_DB_URL", "duckdb:///data/resolver.duckdb")

          con = duckdb_io.get_db(db_url)
          try:
              # show latest run_id
              run_id = con.execute("SELECT max(run_id) FROM forecasts_ensemble").fetchone()[0]
              print("latest_run_id:", run_id)

              rows = con.execute(
                  """
                  SELECT model_name, count(*) as n
                  FROM forecasts_ensemble
                  WHERE run_id = ?
                  GROUP BY 1
                  ORDER BY n DESC
                  """,
                  [run_id],
              ).fetchall()

              print("model_name counts:", rows)

              # Fail if we don't see BOTH expected model names
              names = {r[0] for r in rows}
              needed = {"ensemble_mean_v2", "ensemble_bayesmc_v2"}
              missing = needed - names
              if missing:
                  raise SystemExit(
                      f"Missing expected model_names in forecasts_ensemble: {missing}"
                  )

              pairs = con.execute(
                  """
                  SELECT question_id,
                         SUM(CASE WHEN model_name='ensemble_mean_v2' THEN 1 ELSE 0 END) AS n_mean,
                         SUM(CASE WHEN model_name='ensemble_bayesmc_v2' THEN 1 ELSE 0 END) AS n_bmc
                  FROM forecasts_ensemble
                  WHERE run_id = ?
                  GROUP BY 1
                  HAVING n_mean = 0 OR n_bmc = 0
                  LIMIT 20
                  """,
                  [run_id],
              ).fetchall()

              if pairs:
                  raise SystemExit(
                      f"Some questions missing one aggregation method (showing up to 20): {pairs}"
                  )

              print("OK: both aggregation methods present.")
          finally:
              duckdb_io.close_db(con)
          PY

      - name: Evaluate SPD aggregations (Brier + log score)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_spd_eval == 'true'
        continue-on-error: true
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb
          PYTHONPATH: ${{ github.workspace }}
        run: |
          set -e
          mkdir -p debug/eval
          python scripts/evaluate_spd_aggregations.py \
            --db duckdb:///data/resolver.duckdb \
            --model-names ensemble_mean_v2,ensemble_bayesmc_v2 \
            --out-dir debug/eval
          ls -la debug/eval

      # Step 8: Diagnostics – DuckDB validation
      - name: Diagnostics — DuckDB
        run: |
          set -e
          echo "=== Post-run diagnostics ==="
          echo "Top-level listing after run:"
          ls -la
          echo ""

          DB_PATH="data/resolver.duckdb"
          if [ -f "$DB_PATH" ]; then
            echo "DuckDB database found at $DB_PATH"
          else
            echo "WARNING: DuckDB database $DB_PATH not found."
          fi

          python scripts/post_run_diagnostics.py --db duckdb:///data/resolver.duckdb

      - name: Pythia v2 run summary
        run: |
          set -e
          python -m scripts.dump_pythia_v2_run_summary
        env:
          PYTHIA_DB_URL: duckdb:///data/resolver.duckdb

      - name: Dump unified Pythia v2 debug bundle
        run: python -m scripts.dump_pythia_debug_bundle --db duckdb:///data/resolver.duckdb

      - name: Upload unified debug bundle artifact
        uses: actions/upload-artifact@v4
        with:
          name: pythia-debug-bundle
          path: debug/pytia_debug_bundle__*.md

      - name: Upload evaluation artifacts
        if: always() && github.event_name == 'workflow_dispatch' && github.event.inputs.run_spd_eval == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pythia-eval
          path: debug/eval/*
          if-no-files-found: warn

      - name: Upload canonical resolver DB
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: pythia-resolver-db
          path: data/resolver.duckdb
