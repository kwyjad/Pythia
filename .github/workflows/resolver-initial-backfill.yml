---
name: Resolver — Initial Backfill

on:
  workflow_dispatch:
    inputs:
      months_back:
        description: "Number of months to include (default 12)"
        required: false
        default: "12"
      only_connector:
        description: "Optional connector name for --only (e.g. dtm_client)"
        required: false
        default: ""
      log_level:
        description: "Log level (DEBUG, INFO, WARNING, ERROR)"
        required: false
        default: "INFO"
        type: choice
        options:
          - DEBUG
          - INFO
          - WARNING
          - ERROR
      empty_policy:
        description: "What to do when a connector writes 0 rows? (allow|warn|fail)"
        required: false
        default: "allow"
        type: choice
        options:
          - allow
          - warn
          - fail
      no_date_filter:
        description: "DTM: disable the date window filter (sanity check)"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
      fake_on_timeout:
        description: "Write clearly-marked fake DTM rows if network times out"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
      force_fake:
        description: "Force fake DTM rows (skip network)"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"

env:
  PYTHONUNBUFFERED: "1"
  BACKFILL_DB_PATH: data/resolver_backfill.duckdb
  BACKFILL_INGEST_ARTIFACT: resolver-backfill-ingest-${{ github.run_id }}-${{ github.run_attempt }}
  BACKFILL_ACLED_ARTIFACT: resolver-backfill-acled-${{ github.run_id }}-${{ github.run_attempt }}
  BACKFILL_SNAPSHOT_ARTIFACT: resolver-backfill-snapshots-${{ github.run_id }}-${{ github.run_attempt }}
  BACKFILL_CONTEXT_ARTIFACT: resolver-backfill-context-${{ github.run_id }}-${{ github.run_attempt }}

jobs:
  ingest:
    name: Ingest connectors
    runs-on: ubuntu-latest
    outputs:
      months: ${{ steps.window.outputs.months }}
      start_iso: ${{ steps.window.outputs.start_iso }}
      end_iso: ${{ steps.window.outputs.end_iso }}
      month_count: ${{ steps.window.outputs.month_count }}
    env:
      RESOLVER_DB_URL: ${{ secrets.RESOLVER_DUCKDB_URL != '' && secrets.RESOLVER_DUCKDB_URL || format('duckdb:///{0}/data/resolver_backfill.duckdb', github.workspace) }}
      ACLED_REFRESH_TOKEN: ${{ secrets.ACLED_REFRESH_TOKEN }}
      ACLED_USERNAME: ${{ secrets.ACLED_USERNAME }}
      ACLED_PASSWORD: ${{ secrets.ACLED_PASSWORD }}
      ACLED_ACCESS_KEY: ${{ secrets.ACLED_ACCESS_KEY }}
      DTM_API_KEY: ${{ secrets.DTM_API_KEY }}
      EMDAT_NETWORK: "1"
      EMDAT_API_KEY: ${{ secrets.EMDAT_API_KEY }}
      EMDAT_SOURCE: "api"  # explicit override (safety)
      IDMC_API_TOKEN: ${{ secrets.IDMC_API_TOKEN }}
      IDMC_REQ_PER_SEC: 0.3
      IDMC_MAX_CONCURRENCY: 1
      IDMC_ALLOW_HDX_FALLBACK: "1"
      IDMC_HDX_PACKAGE_ID: preliminary-internal-displacement-updates
      ONLY_CONNECTOR: ${{ inputs.only_connector }}
      CONNECTOR_LIST: dtm_client,emdat_client
      RESOLVER_OUTPUT_DIR: resolver/staging
      LOG_LEVEL: ${{ inputs.log_level }}
      EMPTY_POLICY: ${{ inputs.empty_policy }}
      NO_DATE_FILTER: ${{ inputs.no_date_filter }}
      DTM_CONFIG_PATH: resolver/config/dtm.yml
      IDMC_HELIX_CLIENT_ID: ${{ secrets.IDMC_HELIX_CLIENT_ID }}
      IDMC_HELIX_ENV: RELEASE
      RESOLVER_EXPORT_ENABLE_FLOW: "1"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -c constraints-ci.txt -e '.[db,connectors]'

      - name: Compute backfill window
        id: window
        env:
          MONTHS_INPUT: ${{ inputs.months_back }}
          TIMEZONE: Europe/Istanbul
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m scripts.ci.compute_backfill_window

      - name: Clean staging & diagnostics
        run: |
          set -euo pipefail
          rm -rf resolver/staging || true
          rm -rf diagnostics/ingestion || true

      - name: Probe DTM reachability
        shell: bash
        run: |
          set -euo pipefail
          python -m scripts.ci.probe_dtm_reachability || true

      - name: Probe IDMC reachability
        shell: bash
        run: |
          set -euo pipefail
          python -m scripts.ci.probe_idmc_reachability || true

      - name: Run IDMC (HELIX single-shot)
        if: ${{ inputs.only_connector == '' || inputs.only_connector == 'idmc' }}
        env:
          IDMC_HELIX_CLIENT_ID: ${{ secrets.IDMC_HELIX_CLIENT_ID }}
          IDMC_HELIX_ENV: RELEASE
          RESOLVER_OUTPUT_DIR: resolver/staging
          RESOLVER_EXPORT_ENABLE_FLOW: "1"
          LOG_LEVEL: ${{ inputs.log_level }}
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m resolver.ingestion.idmc.cli \
            --network-mode helix \
            --series flow \
            --enable-export \
            --write-outputs \
            --start "${{ steps.window.outputs.start_iso }}" \
            --end   "${{ steps.window.outputs.end_iso }}" \
            --debug

      - name: Run connectors (with per-connector log tee)
        env:
          RESOLVER_START_ISO: ${{ steps.window.outputs.start_iso }}
          RESOLVER_END_ISO: ${{ steps.window.outputs.end_iso }}
          BACKFILL_START_ISO: ${{ steps.window.outputs.start_iso }}
          BACKFILL_END_ISO: ${{ steps.window.outputs.end_iso }}
          ONLY_CONNECTOR: ${{ inputs.only_connector }}
          LOG_LEVEL: ${{ inputs.log_level }}
          EMPTY_POLICY: ${{ inputs.empty_policy }}
          IDMC_HTTP_VERIFY: "true"
          IDMC_HTTP_TIMEOUT_CONNECT: "5"
          IDMC_HTTP_TIMEOUT_READ: "20"
        shell: bash
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p diagnostics/ingestion/{logs,raw,metrics,samples,dtm}
          mkdir -p "$(dirname "${BACKFILL_DB_PATH}")"
          DTM_ARGS=""
          if [ "${EMPTY_POLICY:-allow}" = "fail" ]; then
            DTM_ARGS="--strict-empty"
          fi
          if [ "${NO_DATE_FILTER:-false}" = "true" ]; then
            if [ -n "${DTM_ARGS}" ]; then
              DTM_ARGS="${DTM_ARGS} --no-date-filter"
            else
              DTM_ARGS="--no-date-filter"
            fi
          fi
          if [ "${EMPTY_POLICY:-allow}" = "allow" ]; then
            if [ -n "${DTM_ARGS}" ]; then
              DTM_ARGS="${DTM_ARGS} --soft-timeouts"
            else
              DTM_ARGS="--soft-timeouts"
            fi
          fi
          CMD_ARGS=(--extra-args "dtm_client=${DTM_ARGS}")
          if [ "${NO_DATE_FILTER:-false}" = "true" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_NO_DATE_FILTER=1")
          fi
          if [ "${EMPTY_POLICY:-allow}" = "allow" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_SOFT_TIMEOUTS=1")
          fi
          if [ "${{ inputs.fake_on_timeout }}" = "true" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_FAKE_ON_TIMEOUT=1")
          fi
          if [ "${{ inputs.force_fake }}" = "true" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_FORCE_FAKE=1")
          fi
          # While diagnosing IDMC runs, you can append --debug via:
          # CMD_ARGS+=(--extra-args "idmc=--debug")
          CONNECTOR_EXIT=0
          python -m scripts.ci.run_connectors "${CMD_ARGS[@]}" || CONNECTOR_EXIT=$?
          exit ${CONNECTOR_EXIT}

      - name: UNHCR ODP → DuckDB (odp_timeseries_raw)
        if: ${{ false }} # Disabled: connector not in current backfill set (DTM, IDMC, EM-DAT, ACLED only).
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m resolver.cli.odp_json_to_duckdb \
            --db "${BACKFILL_DB_PATH}" \
            --config "resolver/ingestion/config/unhcr_odp.yml" \
            --normalizers "resolver/ingestion/config/odp_normalizers.yml" \
            --log-level "${LOG_LEVEL:-INFO}"
        continue-on-error: true

      - name: Ensure diagnostics dirs exist (post-run)
        if: always()
        run: |
          mkdir -p diagnostics/ingestion/{logs,raw,metrics,samples,dtm,export_preview}
          : > diagnostics/ingestion/raw/KEEP.txt
          : > diagnostics/ingestion/metrics/KEEP.txt
          : > diagnostics/ingestion/samples/KEEP.txt
          : > diagnostics/ingestion/export_preview/KEEP.txt
          : > diagnostics/ingestion/dtm/KEEP.txt

      - name: Prepare diagnostics dir
        run: mkdir -p diagnostics/ingestion

      - name: Summarize connector diagnostics
        if: always()
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m scripts.ci.summarize_connectors \
            --report diagnostics/ingestion/connectors_report.jsonl \
            --out diagnostics/ingestion/summary.md \
            --github-step-summary

      - name: Prime ingestion artifact placeholders
        if: always()
        run: |
          mkdir -p diagnostics/ingestion/{raw,metrics,samples,dtm,export_preview}
          : > diagnostics/ingestion/raw/KEEP.txt
          : > diagnostics/ingestion/metrics/KEEP.txt
          : > diagnostics/ingestion/samples/KEEP.txt
          : > diagnostics/ingestion/dtm/KEEP.txt
          : > diagnostics/ingestion/export_preview/KEEP.txt

      - name: Upload connector logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: connector-logs
          path: diagnostics/ingestion/logs/
          if-no-files-found: warn

      - name: Upload connector diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: connector-diagnostics
          path: |
            diagnostics/ingestion/
          retention-days: 30

      - name: Upload DTM raw diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-raw
          path: diagnostics/ingestion/raw/
          if-no-files-found: warn

      - name: Upload DTM metrics diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-metrics
          path: diagnostics/ingestion/metrics/
          if-no-files-found: warn

      - name: Upload DTM samples diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-samples
          path: diagnostics/ingestion/samples/
          if-no-files-found: warn

      - name: Upload ingestion outputs
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_INGEST_ARTIFACT }}
          if-no-files-found: warn
          path: |
            data/
            resolver/staging/
            resolver/logs/ingestion/

      - name: Build single flat artifacts zip
        if: always()
        run: |
          python scripts/ci/backfill_helpers.py --out diagnostics/all-artifacts-flat.zip

      - name: Upload flat artifacts bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: all-artifacts-flat
          path: diagnostics/all-artifacts-flat.zip
          if-no-files-found: warn

  acled-backfill:
    name: ACLED monthly fatalities → DuckDB
    runs-on: ubuntu-latest
    needs: ingest
    env:
      ACLED_USERNAME: ${{ secrets.ACLED_USERNAME }}
      ACLED_PASSWORD: ${{ secrets.ACLED_PASSWORD }}
      ACLED_REFRESH_TOKEN: ${{ secrets.ACLED_REFRESH_TOKEN }}
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -c constraints-ci.txt -e '.[db,connectors]'

      - name: Download ingestion bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_INGEST_ARTIFACT }}
          path: .

      - name: Compute ACLED backfill window
        env:
          MONTHS_INPUT: ${{ inputs.months_back }}
          TIMEZONE: Europe/Istanbul
        run: |
          set -euo pipefail
          python -m scripts.ci.compute_backfill_window

      - name: Run ACLED → DuckDB backfill
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p diagnostics/acled
          python -m resolver.cli.acled_to_duckdb \
            --start "${BACKFILL_START_ISO}" \
            --end "${BACKFILL_END_ISO}" \
            --db "${BACKFILL_DB_PATH}" \
            --log-level "${LOG_LEVEL}" \
            2>&1 | tee diagnostics/acled/acled_to_duckdb.log

      - name: Summarize ACLED DuckDB table
        run: |
          set -euo pipefail
          mkdir -p diagnostics/acled
          python scripts/ci/duckdb_summary.py --db "${BACKFILL_DB_PATH}" --tables "acled_monthly_fatalities" | tee diagnostics/acled/duckdb_summary.md
          if [ -n "${GITHUB_STEP_SUMMARY:-}" ]; then
            cat diagnostics/acled/duckdb_summary.md >> "${GITHUB_STEP_SUMMARY}"
          fi

      - name: Upload ACLED artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_ACLED_ARTIFACT }}
          path: |
            ${{ env.BACKFILL_DB_PATH }}
            diagnostics/acled/
          if-no-files-found: error

  derive-freeze:
    name: Derive & freeze snapshots
    runs-on: ubuntu-latest
    needs:
      - ingest
      - acled-backfill
    env:
      RESOLVER_DB_URL: ${{ secrets.RESOLVER_DUCKDB_URL != '' && secrets.RESOLVER_DUCKDB_URL || format('duckdb:///{0}/data/resolver_backfill.duckdb', github.workspace) }}
      RESOLVER_DUCKDB_URL: ${{ secrets.RESOLVER_DUCKDB_URL != '' && secrets.RESOLVER_DUCKDB_URL || format('duckdb:///{0}/data/resolver_backfill.duckdb', github.workspace) }}
      BACKFILL_MONTHS_CSV: ${{ needs.ingest.outputs.months }}
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -e .[db]

      - name: Download ingestion bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_INGEST_ARTIFACT }}
          path: .

      - name: Download ACLED DuckDB artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_ACLED_ARTIFACT }}
          path: acled-artifacts

      - name: Stage ACLED artifacts
        run: |
          set -euo pipefail
          if [ -f "acled-artifacts/${{ env.BACKFILL_DB_PATH }}" ]; then
            mkdir -p "$(dirname "${{ env.BACKFILL_DB_PATH }}")"
            cp "acled-artifacts/${{ env.BACKFILL_DB_PATH }}" "${{ env.BACKFILL_DB_PATH }}"
          fi
          if [ -d acled-artifacts/diagnostics/acled ]; then
            mkdir -p diagnostics/ingestion/acled
            cp -R acled-artifacts/diagnostics/acled/. diagnostics/ingestion/acled/
          fi

      - name: Derive freeze month env
        run: |
          set -euo pipefail
          python - <<'PY'
          import os

          months = os.environ.get("BACKFILL_MONTHS_CSV", "")
          parts = [part.strip() for part in months.split(",") if part.strip()]
          if not parts:
              raise SystemExit("Unable to derive BACKFILL_MONTH from BACKFILL_MONTHS_CSV")

          latest = parts[-1]
          github_env = os.environ.get("GITHUB_ENV")
          if not github_env:
              raise SystemExit("GITHUB_ENV is not set; cannot persist BACKFILL_MONTH")

          with open(github_env, "a", encoding="utf-8") as handle:
              handle.write(f"BACKFILL_MONTH={latest}\n")
          PY

      - name: Export canonical facts
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p resolver/exports/backfill
          mkdir -p diagnostics/ingestion
          python -m scripts.ci.append_stage_to_summary \
            --section "Derive-freeze stage: Export canonical facts (start)" \
            --status start || true
          if python -m resolver.tools.export_facts \
            --in resolver/staging \
            --out resolver/exports/backfill \
            --config resolver/tools/export_config.yml \
            --write-db 1 \
            --db "${{ env.RESOLVER_DB_URL }}" \
            --append-summary diagnostics/ingestion/summary.md; then
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Export canonical facts (end)" \
              --status success || true
          else
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Export canonical facts (failed)" \
              --status failed || true
            exit 1
          fi

      - name: Load IDMC facts into DuckDB (auxiliary)
        env:
          RESOLVER_WRITE_DB: "1"
          RESOLVER_WRITE_TO_DUCKDB: "1"
          RESOLVER_EXPORT_ENABLE_IDMC: "1"
          WRITE_TO_DUCKDB: "1"
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          if [ -d "resolver/staging/idmc" ]; then
            mkdir -p diagnostics/ingestion/export_preview
            python -m resolver.cli.idmc_to_duckdb \
                --staging-dir resolver/staging/idmc \
                --out diagnostics/ingestion/export_preview \
                --db "${RESOLVER_DB_URL}" \
                --write-db \
                --append-summary diagnostics/ingestion/summary.md
          else
            echo "ℹ️ No resolver/staging/idmc directory; skipping auxiliary IDMC→DB step."
          fi

      - name: Skip EM-DAT freeze for ACLED-only backfill
        if: ${{ inputs.only_connector == 'acled_client' }}
        run: |
          echo "Skipping EM-DAT freeze snapshot + DuckDB verification for ACLED-only backfill."

      - name: Derive & Freeze (write DuckDB)
        if: ${{ inputs.only_connector != 'acled_client' }}
        env:
          RESOLVER_WRITE_DB: "1"
          WRITE_DB: "1"
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          facts_path="diagnostics/ingestion/export_preview/facts.csv"
          if [ ! -f "${facts_path}" ]; then
            echo "facts.csv not found at ${facts_path}" >&2
            ls -la diagnostics/ingestion/export_preview || true
            exit 1
          fi
          python -m scripts.ci.append_stage_to_summary \
            --section "Derive-freeze stage: Freeze snapshot (bulk) (start)" \
            --status start || true
          if python -m resolver.tools.freeze_snapshot \
            --facts diagnostics/ingestion/export_preview/facts.csv \
            --month "${BACKFILL_MONTH}" \
            --write-db=1 \
            --db-url "${{ env.RESOLVER_DUCKDB_URL }}"; then
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Freeze snapshot (bulk) (end)" \
              --status success || true
          else
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Freeze snapshot (bulk) (failed)" \
              --status failed || true
            exit 1
          fi

      - name: Verify DuckDB counts
        if: ${{ inputs.only_connector != 'acled_client' }}
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          DB_PATH="${RESOLVER_DB_URL#duckdb:///}"
          python -m scripts.ci.append_stage_to_summary \
            --section "Derive-freeze stage: Verify DuckDB counts (start)" \
            --status start || true
          if python scripts/ci/verify_duckdb_counts.py "${DB_PATH}" \
            --allow-missing \
            --tables acled_monthly_fatalities facts facts_resolved facts_deltas; then
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Verify DuckDB counts (end)" \
              --status success || true
          else
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Verify DuckDB counts (failed)" \
              --status failed || true
            exit 1
          fi

      - name: Derive and freeze monthly snapshots
        if: ${{ false }} # Temporarily disabled: snapshot building will move to resolver-snapshot-from-db workflow.
        env:
          MONTHS: ${{ env.BACKFILL_MONTHS_CSV }}
        run: |
          echo "::warning::Freeze snapshot stage is disabled in resolver-initial-backfill; use resolver-snapshot-from-db instead."
          python -m scripts.ci.append_stage_to_summary \
            --section "Derive-freeze stage: Freeze snapshots (start)" \
            --status skipped || true

      - name: Summarize DuckDB contents (post-freeze)
        run: |
          set -euo pipefail
          mkdir -p diagnostics/ingestion
          summary_file="${GITHUB_STEP_SUMMARY:-}"
          python scripts/ci/duckdb_summary.py \
            --db "${{ env.BACKFILL_DB_PATH }}" \
            --tables "acled_monthly_fatalities,facts,facts_resolved,facts_deltas" \
            | tee diagnostics/ingestion/duckdb_summary.md
          if [ -n "$summary_file" ]; then
            cat diagnostics/ingestion/duckdb_summary.md >> "$summary_file"
          fi
          if [ -f diagnostics/ingestion/summary.md ]; then
            {
              echo ""
              echo ""
              cat diagnostics/ingestion/duckdb_summary.md
            } >> diagnostics/ingestion/summary.md
          fi

      - name: Upload snapshots and database
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_SNAPSHOT_ARTIFACT }}
          if-no-files-found: warn
          path: |
            resolver/snapshots/
            resolver/exports/backfill/
            ${{ env.BACKFILL_DB_PATH }}

  context:
    name: Build LLM context bundle
    runs-on: ubuntu-latest
    needs:
      - ingest
      - derive-freeze
    env:
      RESOLVER_DB_URL: ${{ secrets.RESOLVER_DUCKDB_URL != '' && secrets.RESOLVER_DUCKDB_URL || format('duckdb:///{0}/data/resolver_backfill.duckdb', github.workspace) }}
      CONTEXT_MONTHS: ${{ needs.ingest.outputs.month_count }}
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -e .[db]

      - name: Download snapshots bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_SNAPSHOT_ARTIFACT }}
          path: .

      - name: Build context bundle
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m scripts.ci.append_stage_to_summary \
            --section "LLM context stage: build bundle (start)" \
            --status start || true
          if python scripts/ci/build_llm_context.py; then
            python -m scripts.ci.append_stage_to_summary \
              --section "LLM context stage: build bundle (end)" \
              --status success || true
          else
            python -m scripts.ci.append_stage_to_summary \
              --section "LLM context stage: build bundle (failed)" \
              --status failed || true
            exit 1
          fi

      - name: Upload context artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_CONTEXT_ARTIFACT }}
          if-no-files-found: warn
          path: context/

  artifacts:
    name: Publish combined artifacts
    runs-on: ubuntu-latest
    needs:
      - derive-freeze
      - context
    env:
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
    steps:
      - name: Download snapshots
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_SNAPSHOT_ARTIFACT }}
          path: .

      - name: Download context bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_CONTEXT_ARTIFACT }}
          path: .

      - name: Upload combined bundle
        uses: actions/upload-artifact@v4
        with:
          name: resolver-initial-backfill-${{ github.run_id }}-${{ github.run_attempt }}
          if-no-files-found: error
          path: |
            resolver/snapshots/
            context/
            ${{ env.BACKFILL_DB_PATH }}
