# Pythia
# Copyright (c) 2025 Kevin Wyjad
# Licensed under the Pythia Non-Commercial Public License v1.0.
# See the LICENSE file in the project root for details.

---
name: Resolver — Initial Backfill

"on":
  workflow_dispatch:
    inputs:
      months_back:
        description: "Number of months to backfill"
        required: true
        default: "3"
      reset_db:
        description: "Delete existing resolver.duckdb and rebuild from scratch?"
        required: false
        default: "false"
      only_connector:
        description: "Optional connector name for --only (e.g. dtm_client)"
        required: false
        default: ""
      log_level:
        description: "Log level (DEBUG, INFO, WARNING, ERROR)"
        required: false
        default: "INFO"
        type: choice
        options:
          - DEBUG
          - INFO
          - WARNING
          - ERROR
      empty_policy:
        description: "What to do when a connector writes 0 rows? (allow|warn|fail)"
        required: false
        default: "allow"
        type: choice
        options:
          - allow
          - warn
          - fail
      no_date_filter:
        description: "DTM: disable the date window filter (sanity check)"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
      fake_on_timeout:
        description: "Write clearly-marked fake DTM rows if network times out"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
      force_fake:
        description: "Force fake DTM rows (skip network)"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
  schedule:
    # Run at 03:00 UTC on the 15th of each month
    - cron: "0 3 15 * *"

env:
  PYTHONUNBUFFERED: "1"
  BACKFILL_DB_PATH: data/resolver.duckdb
  SIGNATURE_REQUIRED_TABLES: questions,hs_triage,question_research,forecasts_ensemble,llm_calls,facts_resolved,facts_deltas,acled_monthly_fatalities
  SIGNATURE_OPTIONAL_TABLES: scenarios
  BACKFILL_INGEST_ARTIFACT: resolver-backfill-ingest-${{ github.run_id }}-${{ github.run_attempt }}
  BACKFILL_ACLED_ARTIFACT: resolver-backfill-acled-${{ github.run_id }}-${{ github.run_attempt }}
  BACKFILL_SNAPSHOT_ARTIFACT: resolver-backfill-snapshots-${{ github.run_id }}-${{ github.run_attempt }}
  BACKFILL_CONTEXT_ARTIFACT: resolver-backfill-context-${{ github.run_id }}-${{ github.run_attempt }}

jobs:
  ingest:
    name: Ingest connectors
    runs-on: ubuntu-latest
    outputs:
      months: ${{ steps.window.outputs.months }}
      start_iso: ${{ steps.window.outputs.start_iso }}
      end_iso: ${{ steps.window.outputs.end_iso }}
      month_count: ${{ steps.window.outputs.month_count }}
      canonical_ready: ${{ steps.canonical_status.outputs.canonical_ready }}
      canonical_run_id: ${{ steps.canonical_status.outputs.canonical_run_id }}
      canonical_source: ${{ steps.canonical_status.outputs.canonical_source }}
      canonical_created_at: ${{ steps.canonical_status.outputs.canonical_created_at }}
    env:
      RESOLVER_DB_URL: ${{ secrets.RESOLVER_DUCKDB_URL != '' && secrets.RESOLVER_DUCKDB_URL || format('duckdb:///{0}/data/resolver.duckdb', github.workspace) }}
      ACLED_REFRESH_TOKEN: ${{ secrets.ACLED_REFRESH_TOKEN }}
      ACLED_USERNAME: ${{ secrets.ACLED_USERNAME }}
      ACLED_PASSWORD: ${{ secrets.ACLED_PASSWORD }}
      ACLED_ACCESS_KEY: ${{ secrets.ACLED_ACCESS_KEY }}
      DTM_API_KEY: ${{ secrets.DTM_API_KEY }}
      EMDAT_NETWORK: "1"
      EMDAT_API_KEY: ${{ secrets.EMDAT_API_KEY }}
      EMDAT_SOURCE: "api"  # explicit override (safety)
      IDMC_API_TOKEN: ${{ secrets.IDMC_API_TOKEN }}
      IDMC_REQ_PER_SEC: 0.3
      IDMC_MAX_CONCURRENCY: 1
      IDMC_ALLOW_HDX_FALLBACK: "1"
      IDMC_HDX_PACKAGE_ID: preliminary-internal-displacement-updates
      ONLY_CONNECTOR: ${{ inputs.only_connector }}
      CONNECTOR_LIST: dtm_client,emdat_client
      RESOLVER_OUTPUT_DIR: resolver/staging
      LOG_LEVEL: ${{ inputs.log_level }}
      EMPTY_POLICY: ${{ inputs.empty_policy }}
      NO_DATE_FILTER: ${{ inputs.no_date_filter }}
      DTM_CONFIG_PATH: resolver/config/dtm.yml
      IDMC_HELIX_CLIENT_ID: ${{ secrets.IDMC_HELIX_CLIENT_ID }}
      IDMC_HELIX_ENV: RELEASE
      RESOLVER_EXPORT_ENABLE_FLOW: "1"
      CANONICAL_DB_READY: "false"
      CANONICAL_DB_RUN_ID: ""
      CANONICAL_DB_SOURCE: ""
      CANONICAL_CREATED_AT: ""
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -c constraints-ci.txt -e '.[db,connectors]'

      - name: Compute backfill window
        id: window
        env:
          MONTHS_INPUT: ${{ inputs.months_back || '3' }}
          TIMEZONE: Europe/Istanbul
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m scripts.ci.compute_backfill_window

      - name: Download canonical resolver DB for incremental runs
        id: canonical_download
        if: github.event_name != 'workflow_dispatch' || inputs.reset_db != 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p data diagnostics
          rm -f diagnostics/db_signature_before.json
          rm -rf data/pythia-resolver-db

          CANDIDATES_FILE=$(mktemp)
          gh run list \
            --workflow "Horizon Scanner Triage" \
            --branch main \
            --status success \
            --json databaseId,createdAt \
            --limit 20 | jq -r '.[] | "\(.createdAt),\(.databaseId),Horizon Scanner Triage"' >>"${CANDIDATES_FILE}" || true
          gh run list \
            --workflow "Resolver — Initial Backfill" \
            --branch main \
            --status success \
            --json databaseId,createdAt \
            --limit 20 | jq -r '.[] | "\(.createdAt),\(.databaseId),Resolver — Initial Backfill"' >>"${CANDIDATES_FILE}" || true

          if [ ! -s "${CANDIDATES_FILE}" ]; then
            echo "ERROR: No prior successful Horizon Scanner or Resolver — Initial Backfill runs found; cannot build canonical DB."
            exit 1
          fi

          sort -r "${CANDIDATES_FILE}" > "${CANDIDATES_FILE}.sorted"

          FOUND=0
          FOUND_SOURCE=""
          FOUND_RUN_ID=""
          FOUND_CREATED_AT=""

          while IFS=',' read -r CREATED RUN_ID LABEL; do
            echo "Attempting canonical DB download from ${LABEL} (run ${RUN_ID}, created ${CREATED})"
            rm -rf data/pythia-resolver-db
            rm -f data/resolver.duckdb
            if ! gh run download "$RUN_ID" -n pythia-resolver-db --dir data; then
              echo "Download failed for run ${RUN_ID}; trying next candidate."
              continue
            fi

            if [ -f data/pythia-resolver-db/resolver.duckdb ]; then
              SRC="data/pythia-resolver-db/resolver.duckdb"
            else
              SRC=$(find data -maxdepth 4 -type f -name "resolver.duckdb" | head -n 1 || true)
            fi

            if [ -z "${SRC}" ]; then
              echo "resolver.duckdb not found in downloaded artifact from ${RUN_ID}; trying next candidate."
              continue
            fi

            if [ "$SRC" != "data/resolver.duckdb" ]; then
              cp "$SRC" data/resolver.duckdb
            fi

            # Validate candidate DB signature; if it fails, try next candidate instead of aborting
            if python -m scripts.ci.db_signature write \
                --db data/resolver.duckdb \
                --required "${SIGNATURE_REQUIRED_TABLES}" \
                --optional "${SIGNATURE_OPTIONAL_TABLES}" \
                --out diagnostics/db_signature_before.json; then
              echo "Candidate signature OK."
            else
              rc=$?
              echo "Candidate rejected (exit ${rc}); missing required tables or invalid signature. Trying next candidate."
              rm -f diagnostics/db_signature_before.json
              continue
            fi

            if [ -f diagnostics/db_signature_before.json ]; then
              FOUND=1
              FOUND_SOURCE="${LABEL}"
              FOUND_RUN_ID="${RUN_ID}"
              FOUND_CREATED_AT="${CREATED}"
              echo "Baseline signature written to diagnostics/db_signature_before.json"
              break
            else
              echo "Signature missing after validating run ${RUN_ID}; trying next candidate."
            fi
          done < "${CANDIDATES_FILE}.sorted"
          if [ "${FOUND}" -ne 1 ]; then
            echo "ERROR: Could not find a valid canonical resolver.duckdb with required tables; run run_horizon_scanner.yml once to seed HS tables, or rerun with reset_db=true to bootstrap."
            exit 1
          fi

          {
            echo "CANONICAL_DB_READY=true"
            echo "CANONICAL_DB_SOURCE=${FOUND_SOURCE}"
            echo "CANONICAL_DB_RUN_ID=${FOUND_RUN_ID}"
            echo "CANONICAL_CREATED_AT=${FOUND_CREATED_AT}"
          } >> "$GITHUB_ENV"

          if [ -n "${GITHUB_STEP_SUMMARY:-}" ] && [ -f diagnostics/db_signature_before.json ]; then
            {
              echo "### Canonical DB signature (before run)"
              echo "- Source workflow: ${FOUND_SOURCE}"
              echo "- Run ID: ${FOUND_RUN_ID}"
              echo "- Created: ${FOUND_CREATED_AT}"
              echo "- Signature file: diagnostics/db_signature_before.json"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Remove any existing resolver.duckdb (reset mode only)
        if: github.event_name == 'workflow_dispatch' && inputs.reset_db == 'true'
        run: |
          set -euo pipefail
          rm -f data/resolver.duckdb

      - name: Record canonical DB selection
        id: canonical_status
        if: always()
        run: |
          {
            echo "canonical_ready=${CANONICAL_DB_READY:-false}"
            echo "canonical_run_id=${CANONICAL_DB_RUN_ID:-}"
            echo "canonical_source=${CANONICAL_DB_SOURCE:-}"
            echo "canonical_created_at=${CANONICAL_CREATED_AT:-}"
          } >> "$GITHUB_OUTPUT"

      - name: Clean staging & diagnostics
        run: |
          set -euo pipefail
          rm -rf resolver/staging || true
          rm -rf diagnostics/ingestion || true

      - name: Probe DTM reachability
        shell: bash
        run: |
          set -euo pipefail
          python -m scripts.ci.probe_dtm_reachability || true

      - name: Probe IDMC reachability
        shell: bash
        run: |
          set -euo pipefail
          python -m scripts.ci.probe_idmc_reachability || true

      - name: Run IDMC (HELIX single-shot)
        if: ${{ inputs.only_connector == '' || inputs.only_connector == 'idmc' }}
        env:
          IDMC_HELIX_CLIENT_ID: ${{ secrets.IDMC_HELIX_CLIENT_ID }}
          IDMC_HELIX_ENV: RELEASE
          RESOLVER_OUTPUT_DIR: resolver/staging
          RESOLVER_EXPORT_ENABLE_FLOW: "1"
          LOG_LEVEL: ${{ inputs.log_level }}
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m resolver.ingestion.idmc.cli \
            --network-mode helix \
            --series flow \
            --enable-export \
            --write-outputs \
            --start "${{ steps.window.outputs.start_iso }}" \
            --end   "${{ steps.window.outputs.end_iso }}" \
            --debug

      - name: Run connectors (with per-connector log tee)
        env:
          RESOLVER_START_ISO: ${{ steps.window.outputs.start_iso }}
          RESOLVER_END_ISO: ${{ steps.window.outputs.end_iso }}
          BACKFILL_START_ISO: ${{ steps.window.outputs.start_iso }}
          BACKFILL_END_ISO: ${{ steps.window.outputs.end_iso }}
          ONLY_CONNECTOR: ${{ inputs.only_connector }}
          LOG_LEVEL: ${{ inputs.log_level }}
          EMPTY_POLICY: ${{ inputs.empty_policy }}
          IDMC_HTTP_VERIFY: "true"
          IDMC_HTTP_TIMEOUT_CONNECT: "5"
          IDMC_HTTP_TIMEOUT_READ: "20"
          RESOLVER_SKIP_DTM: "1"
        shell: bash
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p diagnostics/ingestion/{logs,raw,metrics,samples,dtm}
          mkdir -p "$(dirname "${BACKFILL_DB_PATH}")"
          DTM_ARGS=""
          if [ "${EMPTY_POLICY:-allow}" = "fail" ]; then
            DTM_ARGS="--strict-empty"
          fi
          if [ "${NO_DATE_FILTER:-false}" = "true" ]; then
            if [ -n "${DTM_ARGS}" ]; then
              DTM_ARGS="${DTM_ARGS} --no-date-filter"
            else
              DTM_ARGS="--no-date-filter"
            fi
          fi
          if [ "${EMPTY_POLICY:-allow}" = "allow" ]; then
            if [ -n "${DTM_ARGS}" ]; then
              DTM_ARGS="${DTM_ARGS} --soft-timeouts"
            else
              DTM_ARGS="--soft-timeouts"
            fi
          fi
          CMD_ARGS=(--extra-args "dtm_client=${DTM_ARGS}")
          if [ "${NO_DATE_FILTER:-false}" = "true" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_NO_DATE_FILTER=1")
          fi
          if [ "${EMPTY_POLICY:-allow}" = "allow" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_SOFT_TIMEOUTS=1")
          fi
          if [ "${{ inputs.fake_on_timeout }}" = "true" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_FAKE_ON_TIMEOUT=1")
          fi
          if [ "${{ inputs.force_fake }}" = "true" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_FORCE_FAKE=1")
          fi
          # While diagnosing IDMC runs, you can append --debug via:
          # CMD_ARGS+=(--extra-args "idmc=--debug")
          CONNECTOR_EXIT=0
          python -m scripts.ci.run_connectors "${CMD_ARGS[@]}" || CONNECTOR_EXIT=$?
          exit ${CONNECTOR_EXIT}

      - name: UNHCR ODP → DuckDB (odp_timeseries_raw)
        if: ${{ false }} # Disabled: connector not in current backfill set (DTM, IDMC, EM-DAT, ACLED only).
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m resolver.cli.odp_json_to_duckdb \
            --db "${BACKFILL_DB_PATH}" \
            --config "resolver/ingestion/config/unhcr_odp.yml" \
            --normalizers "resolver/ingestion/config/odp_normalizers.yml" \
            --log-level "${LOG_LEVEL:-INFO}"
        continue-on-error: true

      - name: Ensure diagnostics dirs exist (post-run)
        if: always()
        run: |
          mkdir -p diagnostics/ingestion/{logs,raw,metrics,samples,dtm,export_preview}
          : > diagnostics/ingestion/raw/KEEP.txt
          : > diagnostics/ingestion/metrics/KEEP.txt
          : > diagnostics/ingestion/samples/KEEP.txt
          : > diagnostics/ingestion/export_preview/KEEP.txt
          : > diagnostics/ingestion/dtm/KEEP.txt

      - name: Prepare diagnostics dir
        run: mkdir -p diagnostics/ingestion

      - name: Summarize connector diagnostics
        if: always()
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m scripts.ci.summarize_connectors \
            --report diagnostics/ingestion/connectors_report.jsonl \
            --out diagnostics/ingestion/summary.md \
            --github-step-summary

      - name: Prime ingestion artifact placeholders
        if: always()
        run: |
          mkdir -p diagnostics/ingestion/{raw,metrics,samples,dtm,export_preview}
          : > diagnostics/ingestion/raw/KEEP.txt
          : > diagnostics/ingestion/metrics/KEEP.txt
          : > diagnostics/ingestion/samples/KEEP.txt
          : > diagnostics/ingestion/dtm/KEEP.txt
          : > diagnostics/ingestion/export_preview/KEEP.txt

      - name: Upload connector logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: connector-logs
          path: diagnostics/ingestion/logs/
          if-no-files-found: warn

      - name: Upload connector diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: connector-diagnostics
          path: |
            diagnostics/ingestion/
          retention-days: 30

      - name: Upload DTM raw diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-raw
          path: diagnostics/ingestion/raw/
          if-no-files-found: warn

      - name: Upload DTM metrics diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-metrics
          path: diagnostics/ingestion/metrics/
          if-no-files-found: warn

      - name: Upload DTM samples diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-samples
          path: diagnostics/ingestion/samples/
          if-no-files-found: warn

      - name: Upload ingestion outputs
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_INGEST_ARTIFACT }}
          if-no-files-found: warn
          path: |
            data/
            diagnostics/db_signature_before.json
            resolver/staging/
            resolver/logs/ingestion/

      - name: Build single flat artifacts zip
        if: always()
        run: |
          python scripts/ci/backfill_helpers.py --out diagnostics/all-artifacts-flat.zip

      - name: Upload flat artifacts bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: all-artifacts-flat
          path: diagnostics/all-artifacts-flat.zip
          if-no-files-found: warn

  acled-backfill:
    name: ACLED monthly fatalities → DuckDB
    runs-on: ubuntu-latest
    needs: ingest
    env:
      ACLED_USERNAME: ${{ secrets.ACLED_USERNAME }}
      ACLED_PASSWORD: ${{ secrets.ACLED_PASSWORD }}
      ACLED_REFRESH_TOKEN: ${{ secrets.ACLED_REFRESH_TOKEN }}
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -c constraints-ci.txt -e '.[db,connectors]'

      - name: Download ingestion bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_INGEST_ARTIFACT }}
          path: .

      - name: Compute ACLED backfill window
        env:
          MONTHS_INPUT: ${{ inputs.months_back }}
          TIMEZONE: Europe/Istanbul
        run: |
          set -euo pipefail
          python -m scripts.ci.compute_backfill_window

      - name: Run ACLED → DuckDB backfill
        if: ${{ inputs.only_connector == '' || inputs.only_connector == 'acled' }}
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p diagnostics/acled
          python -m resolver.cli.acled_to_duckdb \
            --start "${BACKFILL_START_ISO}" \
            --end "${BACKFILL_END_ISO}" \
            --db "${BACKFILL_DB_PATH}" \
            --log-level "${LOG_LEVEL}" \
            2>&1 | tee diagnostics/acled/acled_to_duckdb.log

      - name: Summarize ACLED DuckDB table
        run: |
          set -euo pipefail
          mkdir -p diagnostics/acled
          python scripts/ci/duckdb_summary.py --db "${BACKFILL_DB_PATH}" --tables "acled_monthly_fatalities" | tee diagnostics/acled/duckdb_summary.md
          if [ -n "${GITHUB_STEP_SUMMARY:-}" ]; then
            cat diagnostics/acled/duckdb_summary.md >> "${GITHUB_STEP_SUMMARY}"
          fi

      - name: Upload ACLED artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_ACLED_ARTIFACT }}
          path: |
            ${{ env.BACKFILL_DB_PATH }}
            diagnostics/acled/
          if-no-files-found: error

  export-duckdb:
    name: Export canonical facts to DuckDB
    runs-on: ubuntu-latest
    needs:
      - ingest
      - acled-backfill
    env:
      RESOLVER_DB_URL: ${{ secrets.RESOLVER_DUCKDB_URL != '' && secrets.RESOLVER_DUCKDB_URL || format('duckdb:///{0}/data/resolver.duckdb', github.workspace) }}
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
      EMDAT_API_KEY: ${{ secrets.EMDAT_API_KEY }}
      SIGNATURE_OK: "false"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -e .[db]

      - name: Download ingestion bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_INGEST_ARTIFACT }}
          path: .

      - name: Download ACLED DuckDB artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_ACLED_ARTIFACT }}
          path: acled-artifacts

      - name: Stage ACLED artifacts
        run: |
          set -euo pipefail
          if [ -f "acled-artifacts/${{ env.BACKFILL_DB_PATH }}" ]; then
            mkdir -p "$(dirname "${{ env.BACKFILL_DB_PATH }}")"
            cp "acled-artifacts/${{ env.BACKFILL_DB_PATH }}" "${{ env.BACKFILL_DB_PATH }}"
          fi
          if [ -d acled-artifacts/diagnostics/acled ]; then
            mkdir -p diagnostics/ingestion/acled
            cp -R acled-artifacts/diagnostics/acled/. diagnostics/ingestion/acled/
          fi

      - name: Restore baseline DB signature
        run: |
          set -euo pipefail
          if [ -f diagnostics/db_signature_before.json ]; then
            echo "Baseline signature already present at diagnostics/db_signature_before.json"
          else
            CANDIDATE=$(find . -maxdepth 4 -name db_signature_before.json | head -n 1 || true)
            if [ -n "$CANDIDATE" ]; then
              mkdir -p diagnostics
              cp "$CANDIDATE" diagnostics/db_signature_before.json
            fi
          fi

          if [ "${{ needs.ingest.outputs.canonical_ready }}" = "true" ] && [ ! -f diagnostics/db_signature_before.json ]; then
            echo "ERROR: Missing baseline signature diagnostics/db_signature_before.json for canonical comparison."
            exit 1
          fi

      - name: Run EM-DAT PA backfill into resolver.duckdb
        if: ${{ always() && env.EMDAT_API_KEY != '' }}
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -euo pipefail

          DB_URL="${RESOLVER_DB_URL:-duckdb:///${GITHUB_WORKSPACE}/data/resolver.duckdb}"

          END_ISO="$(date -u +%Y-%m-%d)"
          END_YEAR="${END_ISO%%-*}"
          START_YEAR=$((END_YEAR - 5))

          echo "Running EM-DAT PA backfill from ${START_YEAR} to ${END_YEAR} into ${DB_URL}"
          python -m resolver.cli.emdat_to_duckdb \
            --from "${START_YEAR}" \
            --to "${END_YEAR}" \
            --db "${DB_URL}" \
            --network

      - name: Export canonical facts
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p resolver/exports/backfill
          mkdir -p diagnostics/ingestion
          python -m scripts.ci.append_stage_to_summary \
            --section "Derive-freeze stage: Export canonical facts (start)" \
            --status start || true
          if python -m resolver.tools.export_facts \
            --in resolver/staging \
            --out resolver/exports/backfill \
            --config resolver/tools/export_config.yml \
            --write-db 1 \
            --db "${{ env.RESOLVER_DB_URL }}" \
            --append-summary diagnostics/ingestion/summary.md; then
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Export canonical facts (end)" \
              --status success || true
          else
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Export canonical facts (failed)" \
              --status failed || true
            exit 1
          fi

      - name: Load IDMC facts into DuckDB (auxiliary)
        env:
          RESOLVER_WRITE_DB: "1"
          RESOLVER_WRITE_TO_DUCKDB: "1"
          RESOLVER_EXPORT_ENABLE_IDMC: "1"
          WRITE_TO_DUCKDB: "1"
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          if [ -d "resolver/staging/idmc" ]; then
            mkdir -p diagnostics/ingestion/export_preview
            python -m resolver.cli.idmc_to_duckdb \
                --staging-dir resolver/staging/idmc \
                --out diagnostics/ingestion/export_preview \
                --db "${RESOLVER_DB_URL}" \
                --write-db \
                --append-summary diagnostics/ingestion/summary.md
          else
            echo "ℹ️ No resolver/staging/idmc directory; skipping auxiliary IDMC→DB step."
          fi

      - name: Verify DuckDB counts
        env:
          RESOLVER_DB_URL: ${{ env.RESOLVER_DB_URL }}
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          DB_PATH="${RESOLVER_DB_URL#duckdb:///}"
          python -m scripts.ci.append_stage_to_summary \
            --section "Derive-freeze stage: Verify DuckDB counts (start)" \
            --status start || true
          if python scripts/ci/verify_duckdb_counts.py "${DB_PATH}" \
            --allow-missing \
            --tables acled_monthly_fatalities facts facts_resolved facts_deltas; then
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Verify DuckDB counts (end)" \
              --status success || true
          else
            python -m scripts.ci.append_stage_to_summary \
              --section "Derive-freeze stage: Verify DuckDB counts (failed)" \
              --status failed || true
            exit 1
          fi

      - name: Summarize DuckDB contents (post-freeze)
        run: |
          set -euo pipefail
          mkdir -p diagnostics/ingestion
          summary_file="${GITHUB_STEP_SUMMARY:-}"
          python scripts/ci/duckdb_summary.py \
            --db "${{ env.BACKFILL_DB_PATH }}" \
            --tables "acled_monthly_fatalities,facts,facts_resolved,facts_deltas" \
            | tee diagnostics/ingestion/duckdb_summary.md
          if [ -n "$summary_file" ]; then
            cat diagnostics/ingestion/duckdb_summary.md >> "$summary_file"
          fi
          if [ -f diagnostics/ingestion/summary.md ]; then
            {
              echo ""
              echo ""
              cat diagnostics/ingestion/duckdb_summary.md
            } >> diagnostics/ingestion/summary.md
          fi

      - name: Upload resolver_backfill.duckdb artifact
        if: ${{ success() }}
        uses: actions/upload-artifact@v4
        with:
          name: resolver-backfill-db
          path: ${{ env.BACKFILL_DB_PATH }}

      - name: Upload exports and database
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_SNAPSHOT_ARTIFACT }}
          if-no-files-found: warn
          path: |
            resolver/exports/backfill/
            ${{ env.BACKFILL_DB_PATH }}
            diagnostics/db_signature_before.json
            diagnostics/db_signature_after.json

      - name: Simple DuckDB snapshot (facts_resolved/facts_deltas)
        run: |
          set -euo pipefail
          DB_PATH="${BACKFILL_DB_PATH}"
          echo "[simple_snapshot] Running simple snapshot extract against ${DB_PATH}"
          python -m resolver.cli.simple_snapshot --db "${DB_PATH}"

      - name: Compute DB signature after backfill
        id: signature_after
        if: ${{ needs.ingest.outputs.canonical_ready == 'true' }}
        env:
          REQUIRED_TABLES: ${{ env.SIGNATURE_REQUIRED_TABLES }}
          OPTIONAL_TABLES: ${{ env.SIGNATURE_OPTIONAL_TABLES }}
        run: |
          set -euo pipefail
          python -m scripts.ci.db_signature compare \
            --before diagnostics/db_signature_before.json \
            --after-db "${BACKFILL_DB_PATH}" \
            --required "${REQUIRED_TABLES}" \
            --optional "${OPTIONAL_TABLES}" \
            --out diagnostics/db_signature_after.json

          echo "SIGNATURE_OK=true" >> "$GITHUB_ENV"
          echo "signature_ok=true" >> "$GITHUB_OUTPUT"

          if [ -n "${GITHUB_STEP_SUMMARY:-}" ]; then
            {
              echo "### Canonical DB signature (after run)"
              echo "- Source workflow: ${{ needs.ingest.outputs.canonical_source }}"
              echo "- Run ID: ${{ needs.ingest.outputs.canonical_run_id }}"
              echo "- Signature file: diagnostics/db_signature_after.json"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload canonical resolver DB
        if: ${{ success() && needs.ingest.outputs.canonical_ready == 'true' && steps.signature_after.outputs.signature_ok == 'true' && (github.event_name != 'workflow_dispatch' || inputs.reset_db != 'true') }}
        uses: actions/upload-artifact@v4
        with:
          name: pythia-resolver-db
          path: data/resolver.duckdb

      - name: Upload reset resolver DB snapshot
        if: success() && github.event_name == 'workflow_dispatch' && inputs.reset_db == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pythia-resolver-db-reset
          path: data/resolver.duckdb
