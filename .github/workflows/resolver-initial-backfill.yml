---
name: Resolver â€” Initial Backfill

on:
  workflow_dispatch:
    inputs:
      months_back:
        description: "Number of months to include (default 12)"
        required: false
        default: "12"
      only_connector:
        description: "Optional connector name for --only (e.g. dtm_client)"
        required: false
        default: ""
      log_level:
        description: "Log level (DEBUG, INFO, WARNING, ERROR)"
        required: false
        default: "INFO"
        type: choice
        options:
          - DEBUG
          - INFO
          - WARNING
          - ERROR
      empty_policy:
        description: "What to do when a connector writes 0 rows? (allow|warn|fail)"
        required: false
        default: "allow"
        type: choice
        options:
          - allow
          - warn
          - fail
      no_date_filter:
        description: "DTM: disable the date window filter (sanity check)"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"

env:
  PYTHONUNBUFFERED: "1"
  BACKFILL_DB_PATH: data/resolver_backfill.duckdb
  BACKFILL_INGEST_ARTIFACT: resolver-backfill-ingest-${{ github.run_id }}-${{ github.run_attempt }}
  BACKFILL_SNAPSHOT_ARTIFACT: resolver-backfill-snapshots-${{ github.run_id }}-${{ github.run_attempt }}
  BACKFILL_CONTEXT_ARTIFACT: resolver-backfill-context-${{ github.run_id }}-${{ github.run_attempt }}

jobs:
  ingest:
    name: Ingest connectors
    runs-on: ubuntu-latest
    outputs:
      months: ${{ steps.window.outputs.months }}
      start_iso: ${{ steps.window.outputs.start_iso }}
      end_iso: ${{ steps.window.outputs.end_iso }}
      month_count: ${{ steps.window.outputs.month_count }}
    env:
      RESOLVER_DB_URL: duckdb:///${{ github.workspace }}/data/resolver_backfill.duckdb
      ACLED_REFRESH_TOKEN: ${{ secrets.ACLED_REFRESH_TOKEN }}
      ACLED_USERNAME: ${{ secrets.ACLED_USERNAME }}
      ACLED_PASSWORD: ${{ secrets.ACLED_PASSWORD }}
      DTM_API_KEY: ${{ secrets.DTM_API_KEY }}
      RELIEFWEB_APPNAME: ${{ secrets.RELIEFWEB_APPNAME }}
      RELIEFWEB_USER_AGENT: ${{ secrets.RELIEFWEB_USER_AGENT }}
      ONLY_CONNECTOR: ${{ inputs.only_connector }}
      RESOLVER_OUTPUT_DIR: resolver/staging
      LOG_LEVEL: ${{ inputs.log_level }}
      EMPTY_POLICY: ${{ inputs.empty_policy }}
      NO_DATE_FILTER: ${{ inputs.no_date_filter }}
      DTM_CONFIG_PATH: resolver/config/dtm.yml
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -c constraints-ci.txt -e '.[db,connectors]'

      - name: Compute backfill window
        id: window
        env:
          MONTHS_INPUT: ${{ inputs.months_back }}
          TIMEZONE: Europe/Istanbul
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m scripts.ci.compute_backfill_window

      - name: Probe DTM reachability
        shell: bash
        run: |
          set -euo pipefail
          python -m scripts.ci.probe_dtm_reachability || true

      - name: Run connectors (with per-connector log tee)
        env:
          RESOLVER_START_ISO: ${{ steps.window.outputs.start_iso }}
          RESOLVER_END_ISO: ${{ steps.window.outputs.end_iso }}
          BACKFILL_START_ISO: ${{ steps.window.outputs.start_iso }}
          BACKFILL_END_ISO: ${{ steps.window.outputs.end_iso }}
          ONLY_CONNECTOR: ${{ inputs.only_connector }}
          LOG_LEVEL: ${{ inputs.log_level }}
          EMPTY_POLICY: ${{ inputs.empty_policy }}
          # CONNECTOR_LIST: "dtm_client,reliefweb_client,ifrc_go_client"
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p diagnostics/ingestion/{logs,raw,metrics,samples,dtm}
          mkdir -p "$(dirname "${BACKFILL_DB_PATH}")"
          DTM_ARGS=""
          if [ "${EMPTY_POLICY:-allow}" = "fail" ]; then
            DTM_ARGS="--strict-empty"
          fi
          if [ "${NO_DATE_FILTER:-false}" = "true" ]; then
            if [ -n "${DTM_ARGS}" ]; then
              DTM_ARGS="${DTM_ARGS} --no-date-filter"
            else
              DTM_ARGS="--no-date-filter"
            fi
          fi
          if [ "${EMPTY_POLICY:-allow}" = "allow" ]; then
            if [ -n "${DTM_ARGS}" ]; then
              DTM_ARGS="${DTM_ARGS} --soft-timeouts"
            else
              DTM_ARGS="--soft-timeouts"
            fi
          fi
          CMD_ARGS=(--extra-args "dtm_client=${DTM_ARGS}")
          if [ "${NO_DATE_FILTER:-false}" = "true" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_NO_DATE_FILTER=1")
          fi
          if [ "${EMPTY_POLICY:-allow}" = "allow" ]; then
            CMD_ARGS+=(--extra-env "dtm_client=DTM_SOFT_TIMEOUTS=1")
          fi
          CONNECTOR_EXIT=0
          python -m scripts.ci.run_connectors "${CMD_ARGS[@]}" || CONNECTOR_EXIT=$?
          exit ${CONNECTOR_EXIT}

      - name: Ensure diagnostics dirs exist (post-run)
        if: always()
        run: |
          mkdir -p diagnostics/ingestion/{logs,raw,metrics,samples,dtm}

      - name: Prepare diagnostics dir
        run: mkdir -p diagnostics/ingestion

      - name: Summarize connector diagnostics
        if: always()
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python -m scripts.ci.summarize_connectors \
            --report diagnostics/ingestion/connectors_report.jsonl \
            --out diagnostics/ingestion/summary.md \
            --github-step-summary

      - name: Prime ingestion artifact placeholders
        if: always()
        run: |
          mkdir -p diagnostics/ingestion/{raw,metrics,samples,dtm}
          echo "placeholder" > diagnostics/ingestion/raw/KEEP.txt
          echo "placeholder" > diagnostics/ingestion/metrics/KEEP.txt
          echo "placeholder" > diagnostics/ingestion/samples/KEEP.txt
          echo "placeholder" > diagnostics/ingestion/dtm/KEEP.txt

      - name: Upload connector logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: connector-logs
          path: diagnostics/ingestion/logs/
          if-no-files-found: warn

      - name: Upload connector diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: connector-diagnostics
          path: |
            diagnostics/ingestion/
          retention-days: 30

      - name: Upload DTM raw diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-raw
          path: diagnostics/ingestion/raw/
          if-no-files-found: warn

      - name: Upload DTM metrics diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-metrics
          path: diagnostics/ingestion/metrics/
          if-no-files-found: warn

      - name: Upload DTM samples diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dtm-diagnostics-samples
          path: diagnostics/ingestion/samples/
          if-no-files-found: warn

      - name: Upload ingestion outputs
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_INGEST_ARTIFACT }}
          if-no-files-found: warn
          path: |
            data/
            resolver/staging/
            resolver/logs/ingestion/

  derive-freeze:
    name: Derive & freeze snapshots
    runs-on: ubuntu-latest
    needs: ingest
    env:
      RESOLVER_DB_URL: duckdb:///${{ github.workspace }}/data/resolver_backfill.duckdb
      BACKFILL_MONTHS_CSV: ${{ needs.ingest.outputs.months }}
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -e .[db]

      - name: Download ingestion bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_INGEST_ARTIFACT }}
          path: .

      - name: Export canonical facts
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p resolver/exports/backfill
          python -m resolver.tools.export_facts \
            --in resolver/staging \
            --out resolver/exports/backfill \
            --config resolver/tools/export_config.yml

      - name: Derive and freeze monthly snapshots
        env:
          MONTHS: ${{ env.BACKFILL_MONTHS_CSV }}
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          python - <<'PY'
          import calendar
          import os
          import subprocess
          import sys
          from pathlib import Path

          import pandas as pd

          months_env = os.environ.get("MONTHS", "")
          months = [m.strip() for m in months_env.split(",") if m.strip()]
          base_csv = Path("resolver/exports/backfill/facts.csv")
          if not base_csv.exists():
              raise SystemExit(f"facts.csv missing at {base_csv}")

          backfill_db = os.environ.get("BACKFILL_DB_PATH", "data/resolver_backfill.duckdb")
          for ym in months:
              print(f"::group::Deriving {ym}")
              month_dir = Path("resolver/exports/backfill") / ym
              month_dir.mkdir(parents=True, exist_ok=True)

              df = pd.read_csv(base_csv)
              if "as_of_date" in df.columns:
                  df["as_of_date"] = pd.to_datetime(df["as_of_date"], errors="coerce")
                  df = df[df["as_of_date"].dt.strftime("%Y-%m") == ym]
              elif "ym" in df.columns:
                  df = df[df["ym"].astype(str) == ym]

              if df.empty:
                  df = df.head(0)
                  df.to_csv(month_dir / "facts.csv", index=False)
                  df.to_parquet(month_dir / "facts.parquet", index=False)
                  print(f"No facts rows for {ym}; skipping snapshot")
                  print("::endgroup::")
                  continue

              df.to_csv(month_dir / "facts.csv", index=False)
              df.to_parquet(month_dir / "facts.parquet", index=False)

              year, month = map(int, ym.split("-"))
              last_day = calendar.monthrange(year, month)[1]
              cutoff = f"{year:04d}-{month:02d}-{last_day:02d}"

              subprocess.run(
                  [
                      sys.executable,
                      "-m",
                      "resolver.tools.precedence_engine",
                      "--facts",
                      str(month_dir / "facts.csv"),
                      "--cutoff",
                      cutoff,
                      "--outdir",
                      str(month_dir),
                  ],
                  check=True,
              )

              resolved_csv = month_dir / "resolved.csv"
              if not resolved_csv.exists():
                  print(f"resolved.csv missing for {ym}; skipping")
                  print("::endgroup::")
                  continue

              subprocess.run(
                  [
                      sys.executable,
                      "-m",
                      "resolver.tools.make_deltas",
                      "--resolved",
                      str(resolved_csv),
                      "--out",
                      str(month_dir / "deltas.csv"),
                  ],
                  check=True,
              )

              subprocess.run(
                  [
                      sys.executable,
                      "-m",
                      "resolver.tools.freeze_snapshot",
                      "--facts",
                      str(month_dir / "facts.csv"),
                      "--deltas",
                      str(month_dir / "deltas.csv"),
                      "--resolved",
                      str(resolved_csv),
                      "--month",
                      ym,
                      "--outdir",
                      "resolver/snapshots",
                      "--write-db",
                      "1",
                      "--db-url",
                      f"duckdb:///{backfill_db}",
                  ],
                  check=True,
              )

              print("::endgroup::")
          PY

      - name: Upload snapshots and database
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_SNAPSHOT_ARTIFACT }}
          if-no-files-found: warn
          path: |
            resolver/snapshots/
            resolver/exports/backfill/
            ${{ env.BACKFILL_DB_PATH }}

  context:
    name: Build LLM context bundle
    runs-on: ubuntu-latest
    needs:
      - ingest
      - derive-freeze
    env:
      RESOLVER_DB_URL: duckdb:///${{ github.workspace }}/data/resolver_backfill.duckdb
      CONTEXT_MONTHS: ${{ needs.ingest.outputs.month_count }}
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip wheel setuptools
          pip install -e .[db]

      - name: Download snapshots bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_SNAPSHOT_ARTIFACT }}
          path: .

      - name: Build context bundle
        run: |
          set -euo pipefail
          if [ "${LOG_LEVEL:-INFO}" = "DEBUG" ]; then
            set -x
            export PYTHONDEVMODE=1
            export PYTHONWARNINGS=default
          fi
          mkdir -p context
          python -m resolver.tools.build_llm_context --months "${CONTEXT_MONTHS:-12}" --outdir context

      - name: Upload context artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BACKFILL_CONTEXT_ARTIFACT }}
          if-no-files-found: warn
          path: context/

  artifacts:
    name: Publish combined artifacts
    runs-on: ubuntu-latest
    needs:
      - derive-freeze
      - context
    env:
      LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
    steps:
      - name: Download snapshots
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_SNAPSHOT_ARTIFACT }}
          path: .

      - name: Download context bundle
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BACKFILL_CONTEXT_ARTIFACT }}
          path: .

      - name: Upload combined bundle
        uses: actions/upload-artifact@v4
        with:
          name: resolver-initial-backfill-${{ github.run_id }}-${{ github.run_attempt }}
          if-no-files-found: error
          path: |
            resolver/snapshots/
            context/
            ${{ env.BACKFILL_DB_PATH }}
