name: Publish Latest Data (Release)

on:
  workflow_run:
    workflows:
      - "Horizon Scanner Triage"
      - "Resolver — Initial Backfill"
      - "Pythia — Compute Calibration Weights & Advice"
    types: [completed]

permissions:
  contents: write   # needed to upload release assets
  actions: read     # needed to download artifacts from workflow_run

concurrency:
  group: pythia-publish-latest-data
  cancel-in-progress: false

jobs:
  publish:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest

    env:
      RELEASE_TAG: pythia-data-latest
      DB_ARTIFACT_NAME: pythia-resolver-db
      GH_REPO: ${{ github.repository }}
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - name: Wait for DB artifact to be visible (retry)
        id: wait_artifact
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          RUN_ID: ${{ github.event.workflow_run.id }}
          REPO: ${{ github.repository }}
          ARTIFACT_NAME: ${{ env.DB_ARTIFACT_NAME }}
        run: |
          set -euo pipefail
          echo "Triggering workflow run id: ${RUN_ID}"
          echo "Looking for artifact: ${ARTIFACT_NAME}"

          for attempt in $(seq 1 12); do
            echo "Attempt ${attempt}/12: listing artifacts for run ${RUN_ID}"
            JSON="$(gh api -H "Accept: application/vnd.github+json" "/repos/${REPO}/actions/runs/${RUN_ID}/artifacts")"

            # Print what GitHub thinks exists (for debugging)
            echo "${JSON}" | jq -r '.artifacts[]? | "\(.name)\tID=\(.id)\tEXPIRED=\(.expired)\tSIZE=\(.size_in_bytes)"' || true

            ART_ID="$(echo "${JSON}" | jq -r --arg NAME "${ARTIFACT_NAME}" '.artifacts[]? | select(.name==$NAME) | .id' | head -n 1)"
            ART_URL="$(echo "${JSON}" | jq -r --arg NAME "${ARTIFACT_NAME}" '.artifacts[]? | select(.name==$NAME) | .archive_download_url' | head -n 1)"
            if [ -n "${ART_ID}" ] && [ "${ART_ID}" != "null" ] && [ -n "${ART_URL}" ] && [ "${ART_URL}" != "null" ]; then
              echo "Found artifact id: ${ART_ID}"
              echo "Found archive_download_url: ${ART_URL}"
              echo "artifact_id=${ART_ID}" >> "$GITHUB_OUTPUT"
              echo "archive_download_url=${ART_URL}" >> "$GITHUB_OUTPUT"
              exit 0
            fi

            sleep 10
          done

          echo "::error::Artifact '${ARTIFACT_NAME}' not found for run ${RUN_ID} after retries."
          exit 1

      - name: Download DB artifact zip (curl + archive_download_url)
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARCHIVE_URL: ${{ steps.wait_artifact.outputs.archive_download_url }}
        run: |
          set -euo pipefail
          mkdir -p downloaded_artifact
          echo "Downloading artifact zip from archive_download_url..."
          echo "${ARCHIVE_URL}"

          # Download zip; follow redirects (GitHub returns a redirect to blob storage)
          curl -sSL -L \
            -H "Authorization: Bearer ${GITHUB_TOKEN}" \
            -H "Accept: application/vnd.github+json" \
            "${ARCHIVE_URL}" \
            -o artifact.zip
          if [ ! -s artifact.zip ]; then
            echo "::error::artifact.zip is empty; download failed."
            exit 1
          fi

          # Verify it’s actually a zip (catches HTML/JSON error bodies)
          if ! unzip -tq artifact.zip >/dev/null 2>&1; then
            echo "::error::artifact.zip is not a valid zip. Showing first 40 lines:"
            head -n 40 artifact.zip || true
            exit 1
          fi

          unzip -q artifact.zip -d downloaded_artifact
          echo "Downloaded artifact contents:"
          find downloaded_artifact -maxdepth 4 -type f -print || true

      - name: Locate resolver.duckdb
        id: locate_db
        run: |
          set -euo pipefail
          echo "Downloaded artifact contents:"
          find downloaded_artifact -maxdepth 4 -type f -print || true

          DB_PATH="$(find downloaded_artifact -type f -name '*.duckdb' | head -n 1 || true)"
          if [ -z "${DB_PATH}" ]; then
            echo "ERROR: No .duckdb found inside artifact '${DB_ARTIFACT_NAME}'."
            exit 1
          fi

          echo "Found DB at: ${DB_PATH}"
          mkdir -p data_out
          cp -f "${DB_PATH}" data_out/resolver.duckdb

          echo "db_path=data_out/resolver.duckdb" >> "$GITHUB_OUTPUT"

      - name: Compute DB sha256
        id: sha
        run: |
          set -euo pipefail
          SHA="$(sha256sum data_out/resolver.duckdb | awk '{print $1}')"
          echo "sha256=${SHA}" >> "$GITHUB_OUTPUT"

      - name: Build manifest.json (best-effort hs_run_id)
        run: |
          set -euo pipefail

          python -m pip install --upgrade pip >/dev/null
          pip install duckdb >/dev/null

          python - <<'PY'
          import json, os, datetime
          import duckdb

          db_path = "data_out/resolver.duckdb"
          event = {
            "created_utc": datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z",
            "source_workflow": os.environ.get("GITHUB_WORKFLOW", ""),
            "source_run_id": os.environ.get("GITHUB_RUN_ID", ""),
            "source_created_at": os.environ.get("GITHUB_EVENT_WORKFLOW_RUN_CREATED_AT", ""),
            "head_sha": os.environ.get("GITHUB_EVENT_WORKFLOW_RUN_HEAD_SHA", ""),
            "release_tag": os.environ.get("RELEASE_TAG", ""),
            "db_asset": "resolver.duckdb",
            "db_sha256": os.environ.get("DB_SHA256", ""),
            "latest_hs_run_id": None,
            "latest_hs_created_at": None,
          }

          # Try to read latest HS run from DB if table exists
          try:
            con = duckdb.connect(db_path, read_only=True)
            try:
              # hs_runs is expected in your pipeline DB
              row = con.execute(
                "SELECT run_id, created_at FROM hs_runs ORDER BY created_at DESC LIMIT 1"
              ).fetchone()
              if row:
                event["latest_hs_run_id"] = row[0]
                event["latest_hs_created_at"] = str(row[1])
            finally:
              con.close()
          except Exception:
            # best-effort only; leave nulls
            pass

          with open("data_out/manifest.json", "w", encoding="utf-8") as f:
            json.dump(event, f, indent=2, sort_keys=True)

          print("Wrote data_out/manifest.json")
          PY
        env:
          DB_SHA256: ${{ steps.sha.outputs.sha256 }}
          GITHUB_EVENT_WORKFLOW_RUN_CREATED_AT: ${{ github.event.workflow_run.created_at }}
          GITHUB_EVENT_WORKFLOW_RUN_HEAD_SHA: ${{ github.event.workflow_run.head_sha }}

      - name: Ensure release exists (create if missing)
        run: |
          set -euo pipefail
          echo "Repo: ${GH_REPO}"
          echo "Release tag: ${RELEASE_TAG}"

          if gh release view "${RELEASE_TAG}" -R "${GH_REPO}" >/dev/null 2>&1; then
            echo "Release ${RELEASE_TAG} exists."
          else
            echo "Creating release ${RELEASE_TAG}..."
            gh release create "${RELEASE_TAG}" \
              -R "${GH_REPO}" \
              --title "Pythia – Latest Data" \
              --notes "Automated latest-data release."
          fi

      - name: Upload resolver.duckdb + manifest.json to release (overwrite)
        run: |
          set -euo pipefail
          ls -lh data_out || true
          gh release upload "${RELEASE_TAG}" \
            -R "${GH_REPO}" \
            data_out/resolver.duckdb \
            data_out/manifest.json \
            --clobber
